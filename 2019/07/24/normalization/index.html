<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="cheer up"><meta name="baidu-site-verification" content="9pSIuwCbvi"><meta name="google-site-verification" content="YzcCTjF6VoVlNAtL37_S4vFjzFwYTAFZzD51Il2IGKY"><title>normalization | WenHuiZhou</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">normalization</h1><a id="logo" href="/.">WenHuiZhou</a><p class="description">perper（打起精神！）</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">normalization</h1><div class="post-meta">Jul 24, 2019<span> | </span><span class="category"><a href="/categories/super-resolution/">super resolution</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" href="/2019/07/24/normalization/#vcomment"><span class="valine-comment-count" data-xid="/2019/07/24/normalization/"></span><span> 条评论</span></a><div class="post-content"><p>Normalization 正则化在wikipedia上的解释是，使得某个东西更加正规和正常化的一个过程。深度学习中，正则化使用十分广泛，通常网络通过修改loss，添加参数的正则项，对参数的分布进行控制；或是在数据预处理阶段，对数据进行正则化操作。正则化操作通常指的是将数据大小范围缩放到[0,1]之间。<br><a id="more"></a></p>
<h3 id="对数据集的正则化操作"><a href="#对数据集的正则化操作" class="headerlink" title="对数据集的正则化操作"></a>对数据集的正则化操作</h3><blockquote>
<p>Normalization is useful when your data has varying scales and the algorithm you are using does not make assumptions about the distribution of your data, such as <strong>k-nearest neighbors and artificial neural networks.</strong></p>
<p>正则化使用场景是数据特征范围差异大，且数据的分布未知。</p>
</blockquote>
<p>对于一般的数据集来说，我们不需要对其进行正则化操作。但如果数据集不同特征的数据范围相差过大时，我们需要对其进行正则化操作。因为数据范围大的数据，其波动对精度的影响很大，而数据范围小的特征，数据波动的影响不会有这么大，这样造成了结果精度无法提升。因此需要对数据进行正则化操作。使得数据局限在一个固定的范围内。</p>
<h3 id="正则项"><a href="#正则项" class="headerlink" title="正则项"></a>正则项</h3><p>我们知道，当一个网络与数据过度拟合，这个网络能够很好的反应训练数据，但是它的泛化性能也会大大下降。为了避免这种过拟合现象，做法通常有：</p>
<ol>
<li>削减特征的数量（难以确定哪些特征是需要丢弃的）</li>
<li>减少特征的参数，控制参数的分布，即使用正则项方法</li>
</ol>
<p>正则项的目的是为了对参数进行控制，包括：</p>
<ol>
<li>实现参数的稀疏化，即某些参数为0。参数的稀疏化能够自动对数据的特征进行筛选，过滤掉一些不需要的特征，同时起到简化模型的作用，避免过拟合。</li>
<li>最小化正则项能够尽量保持参数较小，参数小的好处在于计算方便，且在网络求导的过程中，产生的导数通常比较小，结果比较稳定。</li>
</ol>
<h4 id="范数-（norm）"><a href="#范数-（norm）" class="headerlink" title="范数 （norm）"></a>范数 （norm）</h4><p>在线性代数领域中，范数是一个函数，它为向量空间中的每个向量分配严格正长度或大小 。</p>
<p><strong>L0 范数：指向量空间中非0向量的个数</strong></p>
<p><strong>无穷范数：指所有向量中欧式距离的最大值作为无穷范数</strong></p>
<h4 id="参数正则项"><a href="#参数正则项" class="headerlink" title="参数正则项"></a>参数正则项</h4><p><strong>L0正则项：模型参数中，不为0的参数的个数</strong></p>
<p>​    L0正则化通过最小化不为0的参数的个数，以达到参数稀疏化的目的，使得模型自动选择特征。在使用时，由于L0正则项是一个NP hard问题，L1是L0的最优凸优化，因此通常用L1来代替L0。</p>
<p><strong>L1正则项：各个模型参数的绝对值之和</strong></p>
<p>​    最小化L1正则项能够将模型的参数变小，沿着0的方向靠近，降低网络的模型复杂度。添加L1正则项后方程如下：<br>$$<br>L = L_0 + \frac{\lambda}{n}\sum_{w}|W|<br>$$<br><strong>L2 正则项：各个参数的平方和再开根号。</strong></p>
<p>​    最小化L2正则项可以使得参数变小接近于0，当参数不会变成0（可以看下面的图来理解），因此L2将选择更多的特征，权重比较小，避免过拟合。方程如下：<br>$$<br>C=C_{0}+\frac{\lambda}{2 n} \sum_{w} w^{2}<br>$$<br><strong>lasso回归与岭参数</strong></p>
<p>L1正则化又称为losso回归，将L1正则项作为loss的惩罚函数。L2正则项又称为岭参数。同样可以将L2正则项作为公式的约束项。可以画图如下,其中等值线为原始的Loss，L1为正方形（绝对值），L2为一个圈（平方根）。可以看出来，图中的交点满足条件的点，因此可以看出L1正则项可以得到更多的稀疏解。</p>
<p><img src="../images/SR/L1L2_7_24.png" alt=""></p>
<h3 id="标准化操作（standardization）"><a href="#标准化操作（standardization）" class="headerlink" title="标准化操作（standardization）"></a>标准化操作（standardization）</h3><blockquote>
<p>Standardization is useful when your data has varying scales and the algorithm you are using does make assumptions about your data having a Gaussian distribution, such as <strong>linear regression, logistic regression and linear discriminant analysis.</strong></p>
<p>标准化使用场景是数据特征范围差异大，假设数据服从高斯分布。</p>
</blockquote>
<p>将数据标准化是指将数据rescale，使得数据的 $mean = 0,\sigma = 1$。数据的标准化操作如下：<br>$$<br>z=\frac{x-\mu}{\sigma}<br>$$<br>标准化操作对于很多机器学习的算法，在网络训练上有着很重要的作用。例如对于梯度下降法来说，处于中心（mean = 0）范围的数据，中心权重的参数更新将会加快。对于一些loss而言（MSE），利用欧式距离作为网络优化的目标，因此标准化操作是很重要的。</p>
<h3 id="Batch-Normalization（批量标准化）"><a href="#Batch-Normalization（批量标准化）" class="headerlink" title="Batch Normalization（批量标准化）"></a>Batch Normalization（批量标准化）</h3><p>其步骤如下，对一个batch中的数据进行标准化后，并学习$r,\beta$ 两个参数，对得到标准化后的值进行一个偏移，得到最终的结果：</p>
<p><img src="../images/SR/BN_7_24.png" alt=""></p>
<p><u>当进来一个batch的时候，具体的做法是，在数据输入到下一层神经元激活函数之前，计算整个batch的mean，variance，偏移后最终得到下一层的输入。</u></p>
<p><strong>为什么要加入Batch Normalization层？</strong></p>
<p>由于深层网络的输入，经过多层神经网络层的作用后发生偏移（ReLu激活函数输出均大于0，因此整体输出的mean将往大于0的方向偏移）。导致网络训练难以收敛，落入梯度饱和区导致梯度消失等问题。BN层重新通过将数据拉回N(0,1)的正态分布上，是的输入值落入激活函数梯度敏感的区域，避免梯度消失，加速网络的训练。（输入变小也有助于降低模型计算复杂度）。</p>
<p>但是仅仅做到这一步还不行，由于我们引入非线性的激活函数，使得网络能够学到一些非线性的性质。我们通过BN将输出拉回到N(0,1)分布上，削弱了激活函数的非线性部分的作用。因此BN通过学习两个参数$\gamma, \beta$ 来对输出做一个scale和shit操作。恢复学习到的非线性部分知识。最终得到的$y_i$ 在正态分布和非线性性质中做了一个trade off。</p>
<p><strong>Batch Normalization的作用</strong></p>
<ol>
<li>batch normalization极大的提升了网络训练的速度</li>
<li>每次BN都将网络的输出控制在一个范围内，近似于符合正态分布，能够起到正则项的作用</li>
<li>对参数的初始化要求降低，调参变得简单</li>
</ol>
<h4 id="layer-normalization"><a href="#layer-normalization" class="headerlink" title="layer normalization"></a>layer normalization</h4><p><img src="../images/SR/layer_normal_7_24.png" alt=""></p>
<p>layer normalization 正则化的方向是沿着feature的方向对CHW归一化，batch normalization 正则化的方向是以sample为单位，对NHW做归一化。</p>
</div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>WenHui Zhou</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2019/07/24/normalization/">https://wenhui-zhou.github.io/2019/07/24/normalization/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>| 本博客所有文章除特别声明外，均采用 <a href="&quot;http://creativecommons.org/licenses/by-nc-sa/3.0/cn/&quot;" rel="&quot;external" nofollow&quot;="" target="&quot;_blank&quot;">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！ </li></ul></div><br><div class="tags"><a href="/tags/深度学习/">深度学习</a></div><div class="post-nav"><a class="pre" href="/2019/08/16/深度学习代码的框架/">深度学习代码的框架</a><a class="next" href="/2019/07/23/image-upsample-downsample-method/">image upsample-downsample method</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'civq9nKD49NpRALooR9Llqmf-gzGzoHsz',
  appKey:'JggO9HaSi1Lfx17nt16oDfsI',
  placeholder:'Shall we talk',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Mac/">Mac</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tensorflow/">Tensorflow</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/super-resolution/">super resolution</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tool/">tool</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/xigua/">xigua</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/建站/">建站</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/手撕系列/">手撕系列</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/模型评价/">模型评价</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/比赛/">比赛</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法扫盲/">算法扫盲</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程/">编程</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/论文复现/">论文复现</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/论文阅读/">论文阅读</a><span class="category-list-count">20</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目/">项目</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目总结/">项目总结</a><span class="category-list-count">4</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/项目总结/论文阅读/">论文阅读</a><span class="category-list-count">3</span></li></ul></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/tip/" style="font-size: 15px;">tip</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/tags/dialog/" style="font-size: 15px;">dialog</a> <a href="/tags/leetcode/" style="font-size: 15px;">leetcode</a> <a href="/tags/netStation/" style="font-size: 15px;">netStation</a> <a href="/tags/tool/" style="font-size: 15px;">tool</a> <a href="/tags/interview/" style="font-size: 15px;">interview</a> <a href="/tags/c/" style="font-size: 15px;">c++</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/tips/" style="font-size: 15px;">tips</a> <a href="/tags/—-leetcode/" style="font-size: 15px;">— leetcode</a> <a href="/tags/论文阅读/" style="font-size: 15px;">论文阅读</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/08/27/并查集，python示例/">并查集，python示例</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/25/哈希表-python示例/">哈希表，python示例</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/22/堆排序，python实现/">堆排序，python实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/16/深度学习代码的框架/">深度学习代码的框架</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/24/normalization/">normalization</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/23/image-upsample-downsample-method/">image upsample-downsample method</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/23/Deep-Learning-for-image-Super-resolution-a-Survey/">Deep Learning for image Super-resolution: a Survey</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/23/一些提升效率的方法/">一些提升效率的方法</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/21/xigua-支持向量机/">xigua-支持向量机</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/20/xigua-神经网络/">xigua-神经网络</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/WenHui-Zhou" title="GITHUB" target="_blank">GITHUB</a><ul></ul><a href="http://www.google.com/" title="GOOGLE" target="_blank">GOOGLE</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">WenHuiZhou.</a> 访问人数:<span id="busuanzi_value_site_uv"></span> 
访问量:<span id="busuanzi_value_site_pv"></span></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script> </div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>