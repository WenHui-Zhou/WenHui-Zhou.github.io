<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="cheer up"><meta name="baidu-site-verification" content="9pSIuwCbvi"><meta name="google-site-verification" content="YzcCTjF6VoVlNAtL37_S4vFjzFwYTAFZzD51Il2IGKY"><title>深度学习代码的框架 | WenHuiZhou</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">深度学习代码的框架</h1><a id="logo" href="/.">WenHuiZhou</a><p class="description">perper（打起精神！）</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">深度学习代码的框架</h1><div class="post-meta">Aug 16, 2019<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" href="/2019/08/16/深度学习代码的框架/#vcomment"><span class="valine-comment-count" data-xid="/2019/08/16/深度学习代码的框架/"></span><span> 条评论</span></a><div class="post-content"><p>以pytorch为例，梳理一下深度学习中，数据的读取，神经网络的搭建，NMS，以及各个指标的计算流程。</p>
<a id="more"></a>
<h3 id="main-函数，程序入口，以及代码配置"><a href="#main-函数，程序入口，以及代码配置" class="headerlink" title="main 函数，程序入口，以及代码配置"></a>main 函数，程序入口，以及代码配置</h3><p>通常main函数中，通过实现argparse功能包，从函数的外部接受参数的传入，对数据，网络等进行一些基本的配置。argparse的使用方法：<a href="https://docs.python.org/zh-cn/3/library/argparse.html" target="_blank" rel="noopener">https://docs.python.org/zh-cn/3/library/argparse.html</a></p>
<p>main函数中一些常用的配置项：</p>
<ul>
<li>数据集的格式：coco，csv，pascal voc等等</li>
<li>数据的路径，包括训练集，测试集的路径等等</li>
<li>网络的一些细节配置，如深度，backbone 类型</li>
<li>一些功能的开关设置，如数据的增强等</li>
<li>训练过程中，一些变量的设置，比如epoch的设置，batch_size的设置等等</li>
</ul>
<h3 id="数据读取部分"><a href="#数据读取部分" class="headerlink" title="数据读取部分"></a>数据读取部分</h3><p>数据读取部分的操作包括数据集文件的读取，对图片进行数据的增强，继承dataloader实现数据的批量读取。</p>
<h4 id="数据文件的读取"><a href="#数据文件的读取" class="headerlink" title="数据文件的读取"></a>数据文件的读取</h4><p>这部分读取任务主要包括读取annotation文件，以及class_id文件，这里以csv格式的数据集文件为例。</p>
<p>首先实现一个CSVDataset类，继承至torch.utils.data.Dataset类。该类必须实现<code>__len__</code>,<code>__getitem__</code>两个方法。</p>
<p>在CSVDataset方法的<code>__init__</code>中，进行数据集文件的读取，最终将得到：</p>
<ul>
<li>self.classes</li>
<li>self.image_names : list 包含所有的数据集图片路径</li>
<li>self.image_data: dict[image_name] = [ {x1,y1,x2,y2,class_name},…]</li>
</ul>
<p><code>__getitem__</code>函数中需要实现的方法有根据下标来得到image，以及其对应的标注。最终返回的格式为：</p>
<p><code>sample = {&#39;img&#39;: img, &#39;annot&#39;: annot}</code>。在返回之前，如果有数据增强部分，还需要进行数据的增强。</p>
<h4 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h4><p>数据增强的方法有很多种，常用的图片的翻转，切割，resize，归一化等等。数据增强利用一张图片，得到它的许多副本，有效的增大数据集。数据增强能够起效果的一个本质因素在于，卷积操作对位移，视角，图片大小，光照等因素具有不变性。数据增强有线下增强和线上增强两种方式，后一种方式在dataloader提取数据的时候，才对数据进行增强。</p>
<p>数据增强的方法通常可以写成一个类，通过pytorch中的<code>transforms.Compose([Augumenter(),Resizer()])</code> 来对所有的增强方法进行整合。</p>
<p><strong>Normalizer</strong></p>
<p>实现一个Normalizer类，覆盖其中的<code>__call__</code>方法，对每张图片做一个正则化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Normalizer</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.mean = np.array([[[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]]])</span><br><span class="line">        self.std = np.array([[[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]]])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, sample)</span>:</span></span><br><span class="line"></span><br><span class="line">        image, annots = sample[<span class="string">'img'</span>], sample[<span class="string">'annot'</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'img'</span>:((image.astype(np.float32)-self.mean)/self.std), <span class="string">'annot'</span>: annots&#125;</span><br></pre></td></tr></table></figure>
<p><strong>argument</strong></p>
<p>实现对图片的翻转，需要注意对标注也要进行处理。</p>
<p><strong>Resizer</strong></p>
<p>该方法意图将图片的大小限制在一定范围以内。因此在缩放的时候，需要找到最大的缩放比例,同时保证图片能够被32整除。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Resizer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Convert ndarrays in sample to Tensors."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, sample, min_side=<span class="number">608</span>, max_side=<span class="number">1024</span>)</span>:</span> <span class="comment">#将图片resize到608，1024以下的大小</span></span><br><span class="line">        image, annots = sample[<span class="string">'img'</span>], sample[<span class="string">'annot'</span>]       <span class="comment"># 不能超过这个尺寸（有一边等于这个尺寸）</span></span><br><span class="line"></span><br><span class="line">        rows, cols, cns = image.shape</span><br><span class="line"></span><br><span class="line">        smallest_side = min(rows, cols)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># rescale the image so the smallest side is min_side</span></span><br><span class="line">        scale = min_side / smallest_side</span><br><span class="line"></span><br><span class="line">        <span class="comment"># check if the largest side is now greater than max_side, which can happen</span></span><br><span class="line">        <span class="comment"># when images have a large aspect ratio</span></span><br><span class="line">        largest_side = max(rows, cols)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> largest_side * scale &gt; max_side:</span><br><span class="line">            scale = max_side / largest_side</span><br><span class="line"></span><br><span class="line">        <span class="comment"># resize the image with the computed scale</span></span><br><span class="line">        image = skimage.transform.resize(image, (int(round(rows*scale)), int(round((cols*scale)))))</span><br><span class="line">        rows, cols, cns = image.shape</span><br><span class="line"></span><br><span class="line">        pad_w = <span class="number">32</span> - rows%<span class="number">32</span></span><br><span class="line">        pad_h = <span class="number">32</span> - cols%<span class="number">32</span></span><br><span class="line"></span><br><span class="line">        new_image = np.zeros((rows + pad_w, cols + pad_h, cns)).astype(np.float32)</span><br><span class="line">        new_image[:rows, :cols, :] = image.astype(np.float32) <span class="comment"># 两个边长需要保证被32整除，少掉的的那部分使用0来补全</span></span><br><span class="line"></span><br><span class="line">        annots[:, :<span class="number">4</span>] *= scale</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'img'</span>: torch.from_numpy(new_image), <span class="string">'annot'</span>: torch.from_numpy(annots), <span class="string">'scale'</span>: scale&#125;</span><br></pre></td></tr></table></figure>
<h3 id="数据调用-dataloader"><a href="#数据调用-dataloader" class="headerlink" title="数据调用 dataloader"></a>数据调用 dataloader</h3><p>pytorch通过实现dataloader方法来实现网络训练时，每次iteration的数据的输出。dataloader的逻辑是，每次从dataset中调用<code>__getitem__()</code>获取单个数据，然后组合成batch，在使用<code>collate_fn</code>参数对batch进行一些操作。</p>
<p><code>torch.utils.data.Dataloader</code><strong>中的参数</strong>：</p>
<blockquote>
<p><strong>dataset</strong>(<a href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Dataset" target="_blank" rel="noopener"><em>Dataset</em></a>) – dataset from which to load the data.</p>
<p><strong>batch_size</strong>(<a href="https://docs.python.org/3/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>, <em>optional</em>) – how many samples per batch to load (default: 1).</p>
<p><strong>shuffle</strong>(<a href="https://docs.python.org/3/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>, <em>optional</em>) – set to <code>True</code>to have the data reshuffled at every epoch (default: False).</p>
<p><strong>sampler</strong>(<a href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Sampler" target="_blank" rel="noopener"><em>Sampler</em></a>, <em>optional</em>) – defines the strategy to draw samples from the dataset. If specified, <code>shuffle</code>must be False.</p>
<p><strong>batch_sampler</strong>(<a href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Sampler" target="_blank" rel="noopener"><em>Sampler</em></a>, <em>optional</em>) – like sampler, but returns a batch of indices at a time. Mutually exclusive with batch_size, shuffle, sampler, and drop_last.</p>
<p><strong>num_workers</strong>(<a href="https://docs.python.org/3/library/functions.html#int" target="_blank" rel="noopener"><em>int</em></a>, <em>optional</em>) – how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)</p>
<p><strong>collate_fn</strong>(<em>callable<strong>, </strong>optional</em>) – merges a list of samples to form a mini-batch.</p>
<p><strong>pin_memory</strong>(<a href="https://docs.python.org/3/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>, <em>optional</em>) – If <code>True</code>, the data loader will copy tensors into CUDA pinned memory before returning them.</p>
<p><strong>drop_last</strong>(<a href="https://docs.python.org/3/library/functions.html#bool" target="_blank" rel="noopener"><em>bool</em></a>, <em>optional</em>) – set to <code>True</code>to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If <code>False</code>and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False)</p>
<p><strong>timeout</strong>(<em>numeric</em>, <em>optional</em>) – if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: 0)</p>
<p><strong>worker_init_fn</strong>(<em>callable</em>, <em>optional</em>) – If not None, this will be called on each worker subprocess with the worker id (an int in <code>[0, num_workers - 1]</code>) as input, after seeding and before data loading. (default: None)</p>
</blockquote>
<p>算法中使用如下参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataloader_train = DataLoader(dataset_train, num_workers=3, collate_fn=collater, batch_sampler=sampler)</span><br></pre></td></tr></table></figure>
<p>其中<code>dataset_train</code>为<code>Dataset</code>类的对象，如上实现数据问价读取的部分。<code>num_workers</code>设置了这个类的线程数。<code>batch_sampler</code> 设置了每次从数据集中返回一个batch的sample的策略。<code>collate_fn</code> 将一系列的样本融合成一个小的mini-batch。</p>
<p><strong>首先是batch_sampler:</strong></p>
<p>继承至采样器类，需要实现其中的<code>__len__</code>方法，<code>__iter__</code>方法。该参数的作用是将数据集做成许多group组成的一个list。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AspectRatioBasedSampler</span><span class="params">(Sampler)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data_source, batch_size, drop_last)</span>:</span></span><br><span class="line">        self.data_source = data_source</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.drop_last = drop_last</span><br><span class="line">        self.groups = self.group_images()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        random.shuffle(self.groups)</span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> self.groups:</span><br><span class="line">            <span class="keyword">yield</span> group</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.drop_last:</span><br><span class="line">            <span class="keyword">return</span> len(self.data_source) // self.batch_size</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (len(self.data_source) + self.batch_size - <span class="number">1</span>) // self.batch_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">group_images</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># determine the order of the images</span></span><br><span class="line">        order = list(range(len(self.data_source)))</span><br><span class="line">        order.sort(key=<span class="keyword">lambda</span> x: self.data_source.image_aspect_ratio(x))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># divide into groups, one group = one batch</span></span><br><span class="line">        <span class="keyword">return</span> [[order[x % len(order)] <span class="keyword">for</span> x <span class="keyword">in</span> range(i, i + self.batch_size)] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(order), self.batch_size)]</span><br></pre></td></tr></table></figure>
<p>如上，这个方法将数据分别存入group中，然后组成一个groups的list。通过一个<code>__iter__()</code>方法，迭代的方式将数据输出。每次输出一个batch大小的数据。</p>
<p><strong>collate_fn参数：</strong></p>
<p>该参数接受来自batch_sampler的数据，对数据进行进一步的处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collater</span><span class="params">(data)</span>:</span></span><br><span class="line">    imgs = [s[<span class="string">'img'</span>] <span class="keyword">for</span> s <span class="keyword">in</span> data]</span><br><span class="line">    annots = [s[<span class="string">'annot'</span>] <span class="keyword">for</span> s <span class="keyword">in</span> data]</span><br><span class="line">    scales = [s[<span class="string">'scale'</span>] <span class="keyword">for</span> s <span class="keyword">in</span> data]     </span><br><span class="line">    widths = [int(s.shape[<span class="number">0</span>]) <span class="keyword">for</span> s <span class="keyword">in</span> imgs]</span><br><span class="line">    heights = [int(s.shape[<span class="number">1</span>]) <span class="keyword">for</span> s <span class="keyword">in</span> imgs]</span><br><span class="line">    batch_size = len(imgs)</span><br><span class="line">    max_width = np.array(widths).max()</span><br><span class="line">    max_height = np.array(heights).max()</span><br><span class="line">    padded_imgs = torch.zeros(batch_size, max_width, max_height, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">        img = imgs[i]</span><br><span class="line">        padded_imgs[i, :int(img.shape[<span class="number">0</span>]), :int(img.shape[<span class="number">1</span>]), :] = img</span><br><span class="line">    max_num_annots = max(annot.shape[<span class="number">0</span>] <span class="keyword">for</span> annot <span class="keyword">in</span> annots)</span><br><span class="line">    <span class="keyword">if</span> max_num_annots &gt; <span class="number">0</span>:</span><br><span class="line">        annot_padded = torch.ones((len(annots), max_num_annots, <span class="number">5</span>)) * <span class="number">-1</span></span><br><span class="line">        <span class="keyword">if</span> max_num_annots &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">for</span> idx, annot <span class="keyword">in</span> enumerate(annots):</span><br><span class="line">                <span class="comment">#print(annot.shape)</span></span><br><span class="line">                <span class="keyword">if</span> annot.shape[<span class="number">0</span>] &gt; <span class="number">0</span>:</span><br><span class="line">                    annot_padded[idx, :annot.shape[<span class="number">0</span>], :] = annot</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        annot_padded = torch.ones((len(annots), <span class="number">1</span>, <span class="number">5</span>)) * <span class="number">-1</span></span><br><span class="line">    padded_imgs = padded_imgs.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'img'</span>: padded_imgs, <span class="string">'annot'</span>: annot_padded, <span class="string">'scale'</span>: scales&#125;</span><br></pre></td></tr></table></figure>
<p>上面的操作，将同一个batch中的图片的大小统一同样的大小。annotation的维度也统一到同样大小的维度。然后进行RGB通道的变换之后，放回一个dict。</p>
<p>上面这些步骤就完成了数据的loader，通过for循环从其中取得元素。</p>
<h3 id="retinanet网络结构"><a href="#retinanet网络结构" class="headerlink" title="retinanet网络结构"></a>retinanet网络结构</h3><p>下面从数据流动的角度分析一下retinanet的各个结构的组成。</p>
<p>retinanet的特征提取部分，使用的是resnet，resnet有多种深度的选择，分别有18，34，50，101，152五种深度。常用的网络深度为50，101:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet50</span><span class="params">(num_classes, pretrained=False, **kwargs)</span>:</span></span><br><span class="line">    <span class="string">"""Constructs a ResNet-50 model.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        pretrained (bool): If True, returns a model pre-trained on ImageNet</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    model = ResNet(num_classes, Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], **kwargs)</span><br><span class="line">    <span class="keyword">if</span> pretrained:</span><br><span class="line">        model.load_state_dict(model_zoo.load_url(model_urls[<span class="string">'resnet50'</span>], model_dir=<span class="string">'.'</span>), strict=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>让我们一行一行来看，第一个调用了ResNet()类，创建了一个ResNet对象。ResNet继承至<code>nn.Module</code>,需要实现函数<code>__init__</code>以及<code>forward()</code>两个方法，通常将可学习的参数放到构造函数<code>__init__()</code>中，在<code>forward</code>中实现网络数据的流动，即可实现网络的自动求导机制。</p>
<p><strong>ResNet</strong></p>
<p>resnet首次提出残差的思想，传统的卷积网络或者全连接网络在信息传递的时候或多或少会存在信息丢失，损耗等问题，同时还有导致梯度消失或者梯度爆炸，导致很深的网络无法训练。ResNet通过学习残差的方式，在一定程度上解决了<strong>网络退化和梯度消失</strong>的问题。ResNet通过大量叠加残差块的方式，加深网络的深度的同时，保证了网络的梯度不消失。ResNet有着两种不同的残差单元。分别是basicBlock 和 bottleneck结构。深层次网络使用bottleneck结构，每次经过残差结构之前都对数据进行一次降维，大大降低了网络的参数量。</p>
<p><img src="/images/res_unit.png" alt=""></p>
<p>bottleneck的结构feature经过第一个1x1的卷积层，将特征的维度压缩，对压缩后的特征进行3x3的卷积，然后经过1x1卷积层，将特征的维度放大到原来的大小。</p>
<p>bottleneck的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bottleneck</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inplanes, planes, stride=<span class="number">1</span>, downsample=None)</span>:</span></span><br><span class="line">        super(Bottleneck, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=<span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.conv2 = nn.Conv2d(planes, planes, kernel_size=<span class="number">3</span>, stride=stride,</span><br><span class="line">                               padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.conv3 = nn.Conv2d(planes, planes * <span class="number">4</span>, kernel_size=<span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(planes * <span class="number">4</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="keyword">True</span>)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        residual = x</span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        </span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        </span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.bn3(out)</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            residual = self.downsample(x)</span><br><span class="line">        out += residual</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>pytorch中常用的搭建网络的函数如下：</p>
<p>Conv2d卷积：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="keyword">True</span>)</span><br><span class="line">参数：</span><br><span class="line">in_channels(int) – 输入信号的通道</span><br><span class="line">out_channels(int) – 卷积产生的通道</span><br><span class="line">kerner_size(int <span class="keyword">or</span> tuple) - 卷积核的尺寸</span><br><span class="line">stride(int <span class="keyword">or</span> tuple, optional) - 卷积步长</span><br><span class="line">padding(int <span class="keyword">or</span> tuple, optional) - 输入的每一条边补充<span class="number">0</span>的层数</span><br><span class="line">dilation(int <span class="keyword">or</span> tuple, optional) – 卷积核元素之间的间距</span><br><span class="line">groups(int, optional) – 从输入通道到输出通道的阻塞连接数</span><br><span class="line">bias(bool, optional) - 如果bias=<span class="keyword">True</span>，添加偏置</span><br><span class="line">输入：</span><br><span class="line">input: (N,C_in,H_in,W_in) </span><br><span class="line">输出：</span><br><span class="line">output: (N,C_out,H_out,W_out)</span><br><span class="line">计算公式：Fout = (Fin + <span class="number">2</span>*padding-kernel)/stride + <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>batchNorm2d：</p>
<p>在训练时，该层计算每次输入的均值与方差，并进行移动平均。移动平均默认的动量值为0.1。</p>
<p>在验证时，训练求得的均值/方差将用于标准化验证数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">BatchNorm2d(num_features, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=<span class="keyword">True</span>)</span><br><span class="line">参数：</span><br><span class="line">num_features： 来自期望输入的特征数，该期望输入的大小为<span class="string">'batch_size x num_features x height x width'</span></span><br><span class="line">eps： 为保证数值稳定性（分母不能趋近或取<span class="number">0</span>）,给分母加上的值。默认为<span class="number">1e-5</span>。</span><br><span class="line">momentum： 动态均值和动态方差所使用的动量。默认为<span class="number">0.1</span>。</span><br><span class="line">affine： 一个布尔值，当设为true，给该层添加可学习的仿射变换参数。</span><br><span class="line">输入：（N, C，H, W) - 输出：（N, C, H, W）</span><br><span class="line">值得至于的是，参数num_feature写channel数即可。</span><br></pre></td></tr></table></figure>
<p>ReLU：修正线性单元函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nn.ReLU(inplace=<span class="keyword">False</span>)</span><br><span class="line">参数：</span><br><span class="line">inplace：表示是否进行覆盖计算，节省内存</span><br><span class="line">不会引起数据维度的变化</span><br></pre></td></tr></table></figure>
<p>MaxPool2d 层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">nn.MaxPool2d(kernel_size, stride=<span class="keyword">None</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, return_indices=<span class="keyword">False</span>, ceil_mode=<span class="keyword">False</span>)</span><br><span class="line">参数：</span><br><span class="line">kernel_size(int <span class="keyword">or</span> tuple) - max pooling的窗口大小</span><br><span class="line">stride(int <span class="keyword">or</span> tuple, optional) - max pooling的窗口移动的步长。默认值是kernel_size</span><br><span class="line">padding(int <span class="keyword">or</span> tuple, optional) - 输入的每一条边补充<span class="number">0</span>的层数</span><br><span class="line">dilation(int <span class="keyword">or</span> tuple, optional) – 一个控制窗口中元素步幅的参数</span><br><span class="line">return_indices - 如果等于<span class="keyword">True</span>，会返回输出最大值的序号，对于上采样操作会有帮助</span><br><span class="line">ceil_mode - 如果等于<span class="keyword">True</span>，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作</span><br><span class="line">输入: (N,C,H_&#123;<span class="keyword">in</span>&#125;,W_in) </span><br><span class="line">输出: (N,C,H_out,W_out)</span><br><span class="line">计算公式：Fout = (Fin + <span class="number">2</span>*padding - kernel)/stride + <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>nn.Upsample 上采样操作对channel进行采样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nn.Upsample(size=<span class="keyword">None</span>, scale_factor=<span class="keyword">None</span>, mode=<span class="string">'nearest'</span>, align_corners=<span class="keyword">None</span>)</span><br><span class="line">给定上采样策略mode，上采样的大小：scale_factor</span><br></pre></td></tr></table></figure>
<p>nn.Sequential一个有序的容器，神经网络模块将按照在传入构造器的顺序依次被添加到计算图中执行，同时以神经网络模块为元素的有序字典也可以作为传入参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(self.inplanes, planes * block.expansion,</span><br><span class="line">                          kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="keyword">False</span>),</span><br><span class="line">                nn.BatchNorm2d(planes * block.expansion),</span><br><span class="line">            )</span><br></pre></td></tr></table></figure>
<p><strong>网络结构类继承至<code>nn.Module</code>,需要实现函数<code>__init__</code>以及<code>forward()</code>两个方法，通常在<strong>init</strong>中完成网络层的初始化工作，定义各类的网络层。在forward中完成网络层数据的流动。</strong></p>
<p>retinanet金字塔模型的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PyramidFeatures</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, C3_size, C4_size, C5_size, feature_size=<span class="number">256</span>)</span>:</span></span><br><span class="line">        super(PyramidFeatures, self).__init__()</span><br><span class="line">        <span class="comment"># upsample C5 to get P5 from the FPN paper</span></span><br><span class="line">        self.P5_1           = nn.Conv2d(C5_size, feature_size, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.P5_upsampled   = nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">'nearest'</span>)</span><br><span class="line">        self.P5_2           = nn.Conv2d(feature_size, feature_size, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># add P5 elementwise to C4</span></span><br><span class="line">        self.P4_1           = nn.Conv2d(C4_size, feature_size, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.P4_upsampled   = nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">'nearest'</span>)</span><br><span class="line">        self.P4_2           = nn.Conv2d(feature_size, feature_size, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># add P4 elementwise to C3</span></span><br><span class="line">        self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># "P6 is obtained via a 3x3 stride-2 conv on C5"</span></span><br><span class="line">        self.P6 = nn.Conv2d(C5_size, feature_size, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># "P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6"</span></span><br><span class="line">        self.P7_1 = nn.ReLU()</span><br><span class="line">        self.P7_2 = nn.Conv2d(feature_size, feature_size, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line"></span><br><span class="line">        C3, C4, C5 = inputs</span><br><span class="line"></span><br><span class="line">        P5_x = self.P5_1(C5)</span><br><span class="line">        P5_upsampled_x = self.P5_upsampled(P5_x)</span><br><span class="line">        P5_x = self.P5_2(P5_x)</span><br><span class="line">        </span><br><span class="line">        P4_x = self.P4_1(C4)</span><br><span class="line">        P4_x = P5_upsampled_x + P4_x</span><br><span class="line">        P4_upsampled_x = self.P4_upsampled(P4_x)</span><br><span class="line">        P4_x = self.P4_2(P4_x)</span><br><span class="line"></span><br><span class="line">        P3_x = self.P3_1(C3)</span><br><span class="line">        P3_x = P3_x + P4_upsampled_x</span><br><span class="line">        P3_x = self.P3_2(P3_x)</span><br><span class="line"></span><br><span class="line">        P6_x = self.P6(C5)</span><br><span class="line"></span><br><span class="line">        P7_x = self.P7_1(P6_x)</span><br><span class="line">        P7_x = self.P7_2(P7_x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> [P3_x, P4_x, P5_x, P6_x, P7_x]</span><br></pre></td></tr></table></figure>
<p>retinanet在金字塔之后，接了一个回归网络以及分类网络，分别对边框位置以及类别进行分类。</p>
<p><strong>回归网络</strong>简单的接了五个卷积层，保持feature的大小不变，每一个channel的维度最终降为num_anchors x 4，即每一个channel需要回归出num_anchors x 4 个坐标点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RegressionModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_features_in, num_anchors=<span class="number">9</span>, feature_size=<span class="number">256</span>)</span>:</span></span><br><span class="line">        super(RegressionModel, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.act1 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.act2 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.act3 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.act4 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        self.output = nn.Conv2d(feature_size, num_anchors*<span class="number">4</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.act1(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.act2(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.act3(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv4(out)</span><br><span class="line">        out = self.act4(out)</span><br><span class="line"></span><br><span class="line">        out = self.output(out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># out is B x C x W x H, with C = 4*num_anchors</span></span><br><span class="line">        out = out.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out.contiguous().view(out.shape[<span class="number">0</span>], <span class="number">-1</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>上诉最后一行值得注意一下view()函数相当于numpy中的reshape函数，但是要求数据必须在内存中是连续存储的。由于permute函数，改变了数据的分布（浅拷贝）。因此在使用view之前，需要执行contiguous函数使得数据内存连续分布。最终out的shape为[batch_size，w x h ，4]。上诉得到的out最终输入criterion中，计算loss。</p>
<p><strong>分类模型</strong>的网络结构和回归模型的结构相同，唯一不同的地方在于最终输出的channel的大小。分类模型输出的channel大小为anchor的数量乘以类别（num_anchor x num_classes）。即每一个框都要预测一个类别信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassificationModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_features_in, num_anchors=<span class="number">9</span>, num_classes=<span class="number">80</span>, prior=<span class="number">0.01</span>, feature_size=<span class="number">256</span>)</span>:</span></span><br><span class="line">        super(ClassificationModel, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.num_anchors = num_anchors</span><br><span class="line">        </span><br><span class="line">        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.act1 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.act2 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.act3 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.act4 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        self.output = nn.Conv2d(feature_size, num_anchors*num_classes, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.output_act = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.act1(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.act2(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.act3(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv4(out)</span><br><span class="line">        out = self.act4(out)</span><br><span class="line"></span><br><span class="line">        out = self.output(out)</span><br><span class="line">        out = self.output_act(out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># out is B x C x W x H, with C = n_classes + n_anchors</span></span><br><span class="line">        out1 = out.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        batch_size, width, height, channels = out1.shape</span><br><span class="line"></span><br><span class="line">        out2 = out1.view(batch_size, width, height, self.num_anchors, self.num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out2.contiguous().view(x.shape[<span class="number">0</span>], <span class="number">-1</span>, self.num_classes)</span><br></pre></td></tr></table></figure>
<p>最后一行首先将out的维度控制在anchor x num_classes，然后通过一个view将其变为[x.shape[0],W x H x anchor, num_classes]，每一个值表示一个框的类别，然后到criterion中去做预测。</p>
<p>Torch.cat 用法：<a href="https://blog.csdn.net/qq_39709535/article/details/80803003" target="_blank" rel="noopener">https://blog.csdn.net/qq_39709535/article/details/80803003</a></p>
<p>接下来需要生成anchor。</p>
</div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>WenHui Zhou</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2019/08/16/深度学习代码的框架/">https://wenhui-zhou.github.io/2019/08/16/深度学习代码的框架/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>| 本博客所有文章除特别声明外，均采用 <a href="&quot;http://creativecommons.org/licenses/by-nc-sa/3.0/cn/&quot;" rel="&quot;external" nofollow&quot;="" target="&quot;_blank&quot;">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！ </li></ul></div><br><div class="tags"><a href="/tags/深度学习/">深度学习</a></div><div class="post-nav"><a class="pre" href="/2019/08/22/堆排序，python实现/">堆排序，python实现</a><a class="next" href="/2019/07/24/normalization/">normalization</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'civq9nKD49NpRALooR9Llqmf-gzGzoHsz',
  appKey:'JggO9HaSi1Lfx17nt16oDfsI',
  placeholder:'Shall we talk',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Mac/">Mac</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tensorflow/">Tensorflow</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/super-resolution/">super resolution</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tool/">tool</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/xigua/">xigua</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/建站/">建站</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/手撕系列/">手撕系列</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/模型评价/">模型评价</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/比赛/">比赛</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法扫盲/">算法扫盲</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程/">编程</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/论文复现/">论文复现</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/论文阅读/">论文阅读</a><span class="category-list-count">20</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目/">项目</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目总结/">项目总结</a><span class="category-list-count">4</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/项目总结/论文阅读/">论文阅读</a><span class="category-list-count">3</span></li></ul></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/interview/" style="font-size: 15px;">interview</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/tags/c/" style="font-size: 15px;">c++</a> <a href="/tags/leetcode/" style="font-size: 15px;">leetcode</a> <a href="/tags/netStation/" style="font-size: 15px;">netStation</a> <a href="/tags/tool/" style="font-size: 15px;">tool</a> <a href="/tags/dialog/" style="font-size: 15px;">dialog</a> <a href="/tags/tip/" style="font-size: 15px;">tip</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/tips/" style="font-size: 15px;">tips</a> <a href="/tags/—-leetcode/" style="font-size: 15px;">— leetcode</a> <a href="/tags/论文阅读/" style="font-size: 15px;">论文阅读</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/08/25/哈希表-python示例/">哈希表,python示例</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/22/堆排序，python实现/">堆排序，python实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/16/深度学习代码的框架/">深度学习代码的框架</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/24/normalization/">normalization</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/23/image-upsample-downsample-method/">image upsample-downsample method</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/23/Deep-Learning-for-image-Super-resolution-a-Survey/">Deep Learning for image Super-resolution: a Survey</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/23/一些提升效率的方法/">一些提升效率的方法</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/21/xigua-支持向量机/">xigua-支持向量机</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/20/xigua-神经网络/">xigua-神经网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/19/19-7-2019-preview/">19/7/2019 preview</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/WenHui-Zhou" title="GITHUB" target="_blank">GITHUB</a><ul></ul><a href="http://www.google.com/" title="GOOGLE" target="_blank">GOOGLE</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">WenHuiZhou.</a> 访问人数:<span id="busuanzi_value_site_uv"></span> 
访问量:<span id="busuanzi_value_site_pv"></span></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script> </div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>