<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="cheer up"><meta name="baidu-site-verification" content="9pSIuwCbvi"><meta name="google-site-verification" content="YzcCTjF6VoVlNAtL37_S4vFjzFwYTAFZzD51Il2IGKY"><title>NLP实践 文本分类任务 | WenHuiZhou</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">NLP实践 文本分类任务</h1><a id="logo" href="/.">WenHuiZhou</a><p class="description">perper（打起精神！）</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">NLP实践 文本分类任务</h1><div class="post-meta">Dec 2, 2019<span> | </span><span class="category"><a href="/categories/NLP/">NLP</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" href="/2019/12/02/NLP实践-文本分类任务/#vcomment"><span class="valine-comment-count" data-xid="/2019/12/02/NLP实践-文本分类任务/"></span><span> 条评论</span></a><div class="post-content"><p>文本分类是NLP中的一个很经典的问题，通过这个问题可以熟悉NLP在处理这类问题的一个大致的思路，达到快速入门的目的。</p>
<a id="more"></a>
<blockquote>
<p>nlp中token，tokenize，tokenizer</p>
<p>token：令牌，表示关键字，变量名，标点，括号等标记符号</p>
<p>tokenize：令牌化，解析标记，将一个句子中关键字等令牌解析出来</p>
<p>tokenizer：令牌解析器，具有解析的功能的类</p>
</blockquote>
<h3 id="任务简介"><a href="#任务简介" class="headerlink" title="任务简介"></a>任务简介</h3><p>文本分类的数据集，数据格式如下：<br>$$<br>X = {(x^1 , y^1 ),(x^2,y^2) · · · , (x^N , y^N )}<br>$$<br>其中$ x_i $表示一组文本，$ y_i $可以为一组标签如词性，也可以是一个标签，即文本的类别。本文的最终目标就是希望达到：<br>$$<br>f(x_i) \to y_i<br>$$<br>下面来深入解决这个问题。 </p>
<h3 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h3><p>在机器学习算法中，样本实例一般是以连续变量或离散变量的形式存在的。因此我们在分类之前，需要将文字转化为特征向量。</p>
<p><strong>词袋模型</strong></p>
<p>一种简单的方法就是认为文本是由字，词组成的无序，多重集合，不考虑语法和词序。这就是在自然语言处理中常用的词袋模型。磁带模型可以看车以词为基本单位的向量空间模型。</p>
<p><strong>N元特征</strong></p>
<p>在实际场景中词序是十分重要的，可能影响句子含义的表达。因此我们需要在特征中保留单词的词序。 </p>
<p>N 元特征(N-gram 特征),顾名思义,就是由 N 个字或词组成的字符串,单元可以 是字或词。这里N是大于等于1的任意整数。如果N 为2,就叫做二元特征,如果N为 3,就叫做三元特征以此类推。</p>
<p>以中文句子“机器学习算法”为例,以字为基本单位的二元特征集合为:{机器,器 学,学习,习算,算法}。集合中每一项都是由二个相邻的字组成的的子串,长度为 2。</p>
<p> 有了 N 元特征集合,就可以利用词袋模型将文本表示为向量形式。随着 N 的增加, 可以抽取的特征就会越多,特征空间也会呈指数增加。这些高阶的特征出现的频率也会相对较低,对分类不但没有太多帮助,还会直接影响着后续处理的效率与复杂度。<strong>因此在一般的文本分类任务中,N 取 3 就足够了,并且同时也使用一元和二元特征,防止出现过拟合。</strong></p>
<h3 id="特征分类"><a href="#特征分类" class="headerlink" title="特征分类"></a>特征分类</h3><p>经过特征的抽取之后，一个模型就可以认为是k维特征空间上的一个点，需接下来就要寻找一些超平面来对空间进行划分，也就是对文本进行分类。</p>
<p><strong>二分类问题</strong></p>
<p>$$ \hat y =sign((f(z))) = sign(\theta^Tz+\theta_0) $$</p>
<p>sign为符号函数，取判别函数f(z)的正负号，为方便，简写判别函数为 </p>
<p>$$ f(z) ＝ \theta^Tz+\theta_0 = \sum_{i=1}^{k}\theta_iz_i + \theta_0 = \sum_{i=0}^{k} = \hat \theta^T \hat z $$ </p>
<p>其中$z_0=1$,$\hat\theta,\hat z$分别称为增广权重向量和增光特征向量。 $$ \hat z = \left( \begin{array} {ccc} 1  z_1 . . z_k \end{array} \right) = \left( \begin{array}{ccc} 1   z   \<br>\end{array} \right) $$</p>
<p>$$ \hat \theta = \left( \begin{array} {ccc} \theta_0  \theta_1 . . \theta_k \end{array} \right) = \left( \begin{array}{ccc} \theta_0   \theta   \<br>\end{array} \right) $$</p>
<p>后面的分类器描述中,我们都采用简化的表示方法,并直接用 $θ , z$ 来表示增广权重向量和增广特征向量</p>
<p><strong>多分类问题</strong></p>
<p> 对于 C 类分类问题,需要定义 C 个判别函数。但是这种表示一般适用于类别 y 为离散变量的情况。在自然语言处理的很多学习任务,类别 y 可以是更复杂的结构,比如多标签、层次化以及结构化等形式。为了更好的描述这些情况，可采用如下形式： $$ \hat y = \mathop{argmax}_yf(\phi(x,y),\theta)$$ 这里$\phi(x,y)$是包含了样本x和类别y混合信息的特征向量，$\theta=[\theta_1;\theta_2…;\theta_C]$，$\phi(x,y)$ 为特征表示，$f(\phi,\theta)$为模型，一般在文本分类中为线性模型（由于我们可以构建足够复杂的特征表示，在高维空间中总是线性可分的），argmax 为解码过程，即寻求y解的过程，对于分类问题，看得分取高分即可。机器学习要学的参数是$\theta$。<a href="https://perper.site/2019/12/02/NLP%E5%AE%9E%E8%B7%B5-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1/" target="_blank" rel="noopener"></a></p>
<h3 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h3><p>数据集：<a href="https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews" target="_blank" rel="noopener">https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews</a></p>
<p>数据集由train.tsv和test.tsv组成，tsv即数列的分隔符是tab，数据的格式如下：</p>
<p><img src="/images/nlp/text1.png" style="zoom:45%;"></p>
<p>每一行由短语id，句子id，句子，类型四列组成。句子类型分为五类：</p>
<ul>
<li>0 - negative</li>
<li>1 - somewhat negative</li>
<li>2 - neutral</li>
<li>3 - somewhat positive</li>
<li>4 - positive</li>
</ul>
<p><strong>数据处理</strong></p>
<p><code>nltk</code>：一个专门处理英文文本的库</p>
<p><code>beautifulsoup</code>：构建，解析分析树</p>
<p>nlp数据预处理的流程：</p>
<ul>
<li>bs4等工具去除标签，或手工的方式，去除数据中不需要的内容</li>
<li>拼写错误检查，通常使用 pyenchant工具包</li>
<li>词干提取（stemming），词形还原（lemmatization），这两种方法都是要找到词的原始形式。即对于一些复数，过去式等英文的还原。stemming方法更加激进一些，还原后的词不一定是原词的词干，通常使用nltk工具包</li>
</ul>
<p>下面是具体的实现，在拿到原始的数据之后，我们需要对数据进行一些处理，处理的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下面这个函数用来去除html标签</span></span><br><span class="line"><span class="comment"># 去除非文本内容</span></span><br><span class="line"><span class="comment"># tokenize句子，即分词</span></span><br><span class="line"><span class="comment"># lemmatize句子，即词性还原</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_sentences</span><span class="params">(df)</span>:</span></span><br><span class="line">    reviews = []</span><br><span class="line">    <span class="keyword">for</span> sent <span class="keyword">in</span> tqdm(df[<span class="string">'Phrase'</span>]):</span><br><span class="line">        review_text = BeautifulSoup(sent).get_text() <span class="comment"># 去除html</span></span><br><span class="line">        review_text = re.sub(<span class="string">'[^a-zA-Z]'</span>,<span class="string">' '</span>,review_text) <span class="comment"># 去除非文本部分</span></span><br><span class="line">        <span class="comment"># tokenize the sentences</span></span><br><span class="line">        words = word_tokenize(review_text.lower()) <span class="comment"># 令牌化</span></span><br><span class="line">        lemma_words = [lemmatizer.lemmatize(i) <span class="keyword">for</span> i <span class="keyword">in</span> words] <span class="comment"># 词形恢复</span></span><br><span class="line">        reviews.append(lemma_words)</span><br><span class="line">    <span class="keyword">return</span> reviews</span><br><span class="line"><span class="comment"># clear the data</span></span><br><span class="line">train_sentences = clean_sentences(train)</span><br><span class="line">test_sentences = clean_sentences(test)</span><br><span class="line">print(len(train_sentences))</span><br><span class="line">print(len(test_sentences))</span><br></pre></td></tr></table></figure>
<p>上述代码对这个数据集进行统一的处理，将文本中的html部分去掉，非文字部分去掉。将单词还原到原始的拼写方式上，最终将得到一个每个句子都转变为一行单词组成的list中。</p>
<p>标注数据为5个类别，通过keras.utils中的to_categorical方法将target转变为one-hot的形式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将标注转变为one-hot格式</span></span><br><span class="line">target = train.Sentiment.values</span><br><span class="line">y_target = to_categorical(target)</span><br><span class="line">num_classes = y_target.shape[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>使用sklearn对训练集进行划分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># split the data into train and val</span></span><br><span class="line">x_train,x_val,y_train,y_val = train_test_split(train_sentences,y_target,test_size = <span class="number">0.2</span>,stratify=y_target)</span><br></pre></td></tr></table></figure>
<p>划分完训练集和测试集只有，训练集中不同单词的个数可能减少了，我们希望得到一个当前训练集的一个词汇全集，并且记录一下最长的句子的长度。得到词汇全集是为了之后做token以及句子的序列化将会使用到。记录句子的最大长度是为了对句子长度进行对齐的时候们将会使用到。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 去除重复出现的词，unique_words里头是一个单词的全集</span></span><br><span class="line">unique_words = set()</span><br><span class="line">len_max = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> tqdm(x_train):</span><br><span class="line">    unique_words.update(sent)</span><br><span class="line">    <span class="keyword">if</span>(len_max &lt; len(sent)):</span><br><span class="line">        len_max = len(sent)</span><br><span class="line">print(len(list(unique_words)))</span><br><span class="line">print(len_max)</span><br></pre></td></tr></table></figure>
<p>下面利用keras的方法，对句子进行重新的token，这里的目的和最开始做token的目的不同，这里是为了得到句子的序列表示，得到序列表示以及词汇表之后，就可以使用embedding层，对句子进行word2vec变换了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对句子再次进行tokenizer操作</span></span><br><span class="line">tokenizer = Tokenizer(num_words = len(list(unique_words)))</span><br><span class="line">tokenizer.fit_on_texts(list(x_train)) <span class="comment"># 用数据初始化tokenizer</span></span><br><span class="line"><span class="comment"># tokenizer.word_count 返回一个字典，字典的key为词，val为出现的个数</span></span><br><span class="line"><span class="comment"># tokenizer.word_index 对词集合中每一个词编号,key为词，val为编号</span></span><br><span class="line"><span class="comment"># 将句子中的词替换成词的编号</span></span><br><span class="line">x_train = tokenizer.texts_to_sequences(x_train)</span><br><span class="line">x_val = tokenizer.texts_to_sequences(x_val)</span><br><span class="line">x_test = tokenizer.texts_to_sequences(test_sentences)</span><br><span class="line"><span class="comment"># 由于每个句子的长度不一样长，因此需要对齐，通过pad在短的句子开头补上0</span></span><br><span class="line">x_train = sequence.pad_sequences(x_train,maxlen=len_max)</span><br><span class="line">x_val = sequence.pad_sequences(x_val,maxlen=len_max)</span><br><span class="line">x_test = sequence.pad_sequences(x_test,maxlen=len_max)</span><br></pre></td></tr></table></figure>
<p>最终得到长度一致的一个句子序列化结果，上述代码即完成了所有的数据处理的操作。</p>
<h3 id="网络搭建"><a href="#网络搭建" class="headerlink" title="网络搭建"></a>网络搭建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置early stop</span></span><br><span class="line">early_stopping = EarlyStopping(min_delta=<span class="number">0.001</span>,mode =<span class="string">'max'</span>,monitor=<span class="string">'val_acc'</span>,patience = <span class="number">2</span>)</span><br><span class="line">callback = [early_stopping]</span><br><span class="line"><span class="comment"># keras搭建模型</span></span><br><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># embedding(input_dim(词汇表长度),output_dim(输出的vector的长度)，input_length(输入句子的长度))</span></span><br><span class="line"><span class="comment"># 等于输入了词汇表，句子的sequences，然后去学习word2vec的参数，得到表示句子的vector，长度通常设置成128或300</span></span><br><span class="line">model.add(Embedding(len(list(unique_words)),<span class="number">300</span>,input_length=len_max))<span class="comment"># embedding 起到word2vec的作用</span></span><br><span class="line"><span class="comment"># LSTM return_sequences=true表示输出全序列的输出，False只输出最后一个LSTM的输出</span></span><br><span class="line">model.add(LSTM(<span class="number">128</span>,dropout=<span class="number">0.5</span>,recurrent_dropout=<span class="number">0.5</span>,return_sequences=<span class="keyword">True</span>))</span><br><span class="line">model.add(LSTM(<span class="number">64</span>,dropout=<span class="number">0.5</span>,recurrent_dropout=<span class="number">0.5</span>,return_sequences=<span class="keyword">False</span>))</span><br><span class="line">model.add(Dense(<span class="number">100</span>,activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line">model.add(Dense(num_classes,activation = <span class="string">'softmax'</span>))</span><br><span class="line">model.compile(loss = <span class="string">'categorical_crossentropy'</span>,optimizer=Adam(lr = <span class="number">0.005</span>),metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<p>模型结果如下：</p>
<p><img src="/images/nlp/text2.png" style="zoom:43%;"></p>
<p>embedding 层用于skip-gram方法得到词向量，最后一层softmax，使用交叉熵计算误差。</p>
<p>下面开始模型的训练过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 往模型中加入数据</span></span><br><span class="line">history = model.fit(x_train,y_train,validation_data,(x_val,y_val),epochs=<span class="number">6</span>,batch_size = <span class="number">256</span>,verbose=<span class="number">1</span>,callbacks=callback)</span><br></pre></td></tr></table></figure>
<p>包含了验证集的验证：</p>
<p><img src="/images/nlp/text3.png" style="zoom:50%;"></p>
<p>绘制训练结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line">epoch_count = range(<span class="number">1</span>,len(history.history[<span class="string">'loss'</span>]) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epoch_count,history.history[<span class="string">'loss'</span>],<span class="string">'r--'</span>)</span><br><span class="line">plt.plot(epoch_count,history.history[<span class="string">'val_loss'</span>],<span class="string">'b--'</span>)</span><br><span class="line">plt.legend([<span class="string">'Training loss'</span>,<span class="string">'validation loss'</span>])</span><br><span class="line">plt.xlabel(<span class="string">'epoch'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'loss'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>生成测试集的预测结果，代码结束：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># submission</span></span><br><span class="line">y_pred = model.predict_classes(x_test)</span><br><span class="line">sub_file = pd.read_csv(os.path.join(root,<span class="string">'sampleSubmission.csv'</span>),sep=<span class="string">','</span>)</span><br><span class="line">sub_file.Sentiment = y_pred</span><br><span class="line">sub_file.to_csv(<span class="string">'submission.csv'</span>,index = <span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>看完代码最后来总结一些模型的结构：</p>
<p><img src="/images/nlp/text2.png" style="zoom:43%;"></p>
<p>通过数据的预处理部分，将文本中句子整理成一个【句子个数，句子最大长度】的一个矩阵，每一个位置上为一个单词。如果句子长度不够长的话，就在句子的前面用0来填充。</p>
<p>随后将每个句子，每个词进行word2vec，即embedding处理。<code>keras.layers.embedding</code>层完成词嵌入的工作。本质是一个全连接层，embedding是一个【总词汇数量，每个词编码长度】的一个权重矩阵，每一行表示一个词的向量，embeding层训练好词向量之后，将这些词向量赋予给句子中的词</p>
<p><img src="/images/nlp/text4.png" style="zoom:50%;"></p>
<p>每一个句子将得到一个【句子最大长度，词向量长度】的矩阵。</p>
<p><strong>RNN</strong></p>
<p>LSTM类似于cv领域的CNN，值得好好琢磨一下（虽然现在transform更加的流行）</p>
<p>传统的RNN如下：</p>
<p><img src="/images/nlp/text5.jpg" style="zoom:70%;"></p>
<p>可以看出隐层间的信息传递，$\hat{h}$与h以及x有关，每一个RNN输出的y仅与$\hat{h}$有关，通过softmax进行分类，通过训练网络得出W的参数。RNN与DNN相同，当网络比较深的时候，同样面临梯度消失的问题，当两个词相距比较远的时候，前一个词难以影响到后面的词（但是有时候这种长连接的词能够决定句子的含义）。</p>
<p><strong>LSTM</strong></p>
<p><img src="/images/nlp/text6.jpg" alt=""></p>
<p>LSTM包含三个数据的输入，C的输入为简单的相乘与相加，因此能够保留较长序列的信息。LSTM之间传递的h信息，有一份来自前一层的信息以及当前LSTM的输入信息。LSTM中间分层三个部分，分别为遗忘门，记忆门，选择输出门。</p>
<p><strong>GRU</strong></p>
<p>Gate Recurrent Unit是循环神经网络的一种，和LSTM相似，用来解决长期记忆和反向传播中梯度等问题而提出来的。GRU和LSTM的表现类似，但是GRU的计算复杂度比较小。</p>
<p><img src="/images/nlp/text7.png" style="zoom:25%;"></p>
<p>即：</p>
<p><img src="/images/nlp/text8.jpg" style="zoom:55%;"></p>
<p><img src="/images/nlp/text10.jpg" style="zoom:45%;"></p>
<p><img src="/images/nlp/text9.jpg" style="zoom:55%;"></p>
<p>GRU用一个1-z的方式，用同一个门代替了遗忘门和记忆门的工作，在使用的时候，GRU参数较小，能够使得算法复杂度下降。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>nlp和cv问题在处理上比起来，一个很大的不同在于数据预处理部分有比较多的工作需要完成。这些工作包括了文本数据的清洗，单词的划分tokenize，对每个句子生成word2vec，然后下面才搭建模型，和cv部分就比较类似了。</p>
<p>我觉得从上面的代码里头可以总结从一份代码，这份代码专门处理文本数据的预处理问题。</p>
<p>本文的代码地址：<a href="https://github.com/WenHui-Zhou/NLP_pratice/tree/master/text_classification" target="_blank" rel="noopener">https://github.com/WenHui-Zhou/NLP_pratice/tree/master/text_classification</a></p>
<h3 id="inference"><a href="#inference" class="headerlink" title="inference"></a>inference</h3><p><a href="https://www.kaggle.com/chiranjeevbit/movie-review-prediction" target="_blank" rel="noopener">https://www.kaggle.com/chiranjeevbit/movie-review-prediction</a></p>
</div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>WenHui Zhou</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2019/12/02/NLP实践-文本分类任务/">https://wenhui-zhou.github.io/2019/12/02/NLP实践-文本分类任务/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>| 本博客所有文章除特别声明外，均采用 <a href="&quot;http://creativecommons.org/licenses/by-nc-sa/3.0/cn/&quot;" rel="&quot;external" nofollow&quot;="" target="&quot;_blank&quot;">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！ </li></ul></div><br><div class="tags"></div><div class="post-nav"><a class="pre" href="/2019/12/05/NLP实践-基于注意力机制的文本匹配/">NLP实践 基于注意力机制的文本匹配</a><a class="next" href="/2019/12/02/图像的去噪/">图像的去噪</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'civq9nKD49NpRALooR9Llqmf-gzGzoHsz',
  appKey:'JggO9HaSi1Lfx17nt16oDfsI',
  placeholder:'Shall we talk',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/3D重建/">3D重建</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Mac/">Mac</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tensorflow/">Tensorflow</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/effective-cpp/">effective cpp</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/super-resolution/">super resolution</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tool/">tool</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/webSearch/">webSearch</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/xigua/">xigua</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/动画/">动画</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/建站/">建站</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/手撕系列/">手撕系列</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/推荐系统/">推荐系统</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/模型评价/">模型评价</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/比赛/">比赛</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习/">深度学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习总结/">深度学习总结</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法扫盲/">算法扫盲</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/统计学习方法/">统计学习方法</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程/">编程</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/论文复现/">论文复现</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/论文阅读/">论文阅读</a><span class="category-list-count">23</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/论文阅读/3D重建/">3D重建</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/面试准备/">面试准备</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目/">项目</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目总结/">项目总结</a><span class="category-list-count">6</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/项目总结/论文阅读/">论文阅读</a><span class="category-list-count">3</span></li></ul></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/SR/" style="font-size: 15px;">SR</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/tags/c/" style="font-size: 15px;">c++</a> <a href="/tags/项目总结/" style="font-size: 15px;">项目总结</a> <a href="/tags/leetcode/" style="font-size: 15px;">leetcode</a> <a href="/tags/3D重建/" style="font-size: 15px;">3D重建</a> <a href="/tags/netStation/" style="font-size: 15px;">netStation</a> <a href="/tags/论文阅读/" style="font-size: 15px;">论文阅读</a> <a href="/tags/tool/" style="font-size: 15px;">tool</a> <a href="/tags/interview/" style="font-size: 15px;">interview</a> <a href="/tags/dialog/" style="font-size: 15px;">dialog</a> <a href="/tags/tip/" style="font-size: 15px;">tip</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/learning-cpp/" style="font-size: 15px;">learning cpp</a> <a href="/tags/—-leetcode/" style="font-size: 15px;">— leetcode</a> <a href="/tags/职业规划/" style="font-size: 15px;">职业规划</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/tips/" style="font-size: 15px;">tips</a> <a href="/tags/超分辨率/" style="font-size: 15px;">超分辨率</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/12/19/github指令/">github指令</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/C-STL-library/">C++ STL library</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/07/18/海量地震数据超分辨率恢复及三维可视化/">海量地震数据超分辨率恢复及三维可视化的细枝末节</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/20/跟着面试打补丁/">跟着面试打补丁</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/17/地震数据超分辨率实验部分/">地震数据超分辨率实验部分</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/15/RDSNet细枝末节/">RDSNet细枝末节</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/14/ms-一个结束，一个开始/">ms:一个结束，一个开始</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/06/面试试题准备/">面试试题准备</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/04/手势识别/">手势识别</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/27/常见的目标检测网络/">常见的目标检测网络</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/WenHui-Zhou" title="GITHUB" target="_blank">GITHUB</a><ul></ul><a href="http://www.google.com/" title="GOOGLE" target="_blank">GOOGLE</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">WenHuiZhou.</a> 访问人数:<span id="busuanzi_value_site_uv"></span> 
访问量:<span id="busuanzi_value_site_pv"></span></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script> </div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>