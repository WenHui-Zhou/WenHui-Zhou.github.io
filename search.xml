<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[linux 环境安装]]></title>
    <url>%2F2019%2F09%2F06%2Flinux-%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[linux 环境下的python3.6安装，以及Linux系统的一些设置。 前言今天进行了linux环境的配置，感觉十分的尽兴，安装过程十分的舒适，一些配置环节比较知道来龙去脉，配置起来很过隐。感触是对linux环境比较熟悉，对这一块的帮助是很大的，其次是也知道了其他人做法其中的道理。 python3.6的安装linux系统默认的python版本有两个，分别是python2.7和python3.5，这次想安装一个比较常用的python3.6。现实条件是我只是一个用户权限的使用者，因此很多sodu操作无法执行。下面基础部分我跳过，重点放在linux环境的配置上。 去官网下载python3.6.tgz安装包，然后安装的时候因为没有root权限（正常安装python3.6，安装文件会放在/usr/bin,/local/bin这些地方），我在目录下新建了一个python3.6目录用来存放安装文件。安装过程：https://my.oschina.net/moonrain/blog/739612，其中`./configure` 修改为./configure --prefix=./python36。 因为默认的python的版本是2.7,这时候需要修改成python3.6，（其实比较明智的做法是用virtualenv创建一个以pyhton3.6版本的环境就可以了。）首先在.bashrc中添加python3.6中bin的路径： 1PATH=&apos;./python/bin:$PATH&apos; 然后创建别名： 1alias python=./python3.6/bin/python3.6 最后source ./bashrc修改完成。 然后还差一点，pip指向的是系统的python2.7，pip3指向的是python3.6，我尝试过修改别名，发现不起效果，最后发现原来系统配置的时候都会source 一下系统的bash，将pip修改为原来的。没办法着时候转向virtualenv。 virtualenv用了好久了virtualenv之后，现在才意识到这个环境包的好用之处，相比annaconda简洁多了，推荐指数max。安装过程如下： 1pip3 install virtualenv virtualenv中默认使用的python是当前python指向的python版本，当然也可以自己设置成自己指定python的版本。 1virtualenv -p ./python3.6/bin/python3 zhou_env 激活virtualenv： 1source zhou_env/bin/activate 下面就可以正常的在python3.6的环境中使用pip了，嗑盐了。 退出虚拟环境： 1deactivate 下面贴一个关于linux文件夹先后顺序的链接： https://perper.site/2019/04/24/linux%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83/]]></content>
      <categories>
        <category>tool</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Image Super-Resolution Using Very Deep Residual Channel Attention Networks(RCAN)]]></title>
    <url>%2F2019%2F09%2F05%2FImage-Super-Resolution-Using-Very-Deep-Residual-Channel-Attention-Networks-RCAN%2F</url>
    <content type="text"><![CDATA[RCAN这篇文章是2018年发表在ECCV上的一篇poster，作者Yunlun Zhang也是该领域的一个大牛。在文中作者对比了各项性能指标，均达到了state of the art的效果。在目前超分辨率领域越做越细的前提下，以提升指标性能为目的的文章越来越不好发表了。下面介绍一下文章的思路、highlight希望能够有点启发。 arxiv： https://arxiv.org/pdf/1807.02758.pdf github：https://github.com/yulunzhang/RCAN 摘要在超分辨率领域中，深度的卷积模型十分的重要，但是训练起来十分的困难；低频的输入或特征中有着很丰富的信息，但是这些信息在网络中被同等的对待，阻碍了卷积网络表达特征的能力。 为了解决上述问题，作者提出一个残差通道注意力网络（RCAN），通过提出RIR（residual in residual）模块来构建深度的网络，RIR中包含着许多的RG（residual group），RG中包含着许多的residual block，以及许多长连接跳跃（LSC）。RIR允许低频信息通过多个跳跃直接传播，使得网络集中学习图像中的高频部分。作者提出CA（channel attention） 通道注意力机制，通过考虑通道间的相互依赖性，来重新调整通道特征。 介绍作者在这部分内容中列举了很多网络，目的是说明深度的网络在超分辨率问题上是有效果的。作者提出的RIR结构，提升网络的深度。对于低频信息的相互依赖性问题，作者提出了CA方法来调整通道的特征。 Residual Channel Attention Network（RCAN）RCAN的网络结构如下图所示： RCAN网络结构由四部分组成，第一部分是卷积浅层特征提取模块，第二部分是RIR深层特征提取模块，第三部分是上采样模块，第四部分是重建模块，网络最后的卷积层具有三个通道，表示输出的颜色。 RCAN网络损失函数采用L1损失：$$L(\Theta)=\frac{1}{N} \sum_{i=1}^{N}\left|H_{R C A N}\left(I_{L R}^{i}\right)-I_{H R}^{i}\right|_{1}$$ Residual in Residual (RIR)RIR结构中包含着若干个（10）residual groups（RG）结构以及long skip connection。每一个RG中包含着如果个（20）residual channel attention block（RCAB）模块，内部含有许多短的连接。 RIR结构通过堆叠残差块，利用skip connection这种结构来克服网络难以训练的问题。 channel attention（CA） 输入是一个 H×W×C（64） 的特征，我们先进行一个空间的全局平均池化得到一个 1×1×C 的通道描述。接着，再经过一个下采样层和一个上采样层得到每一个通道的权重系数，将权重系数和原来的特征相乘即可得到缩放后的新特征，整个过程实际上就是对不同通道的特征重新进行了加权分配。 其中，下采样和上采样层都利用 1×1 的卷积来实现，下采样层的通道数减少 r 倍，激活函数为 Relu，上采样层的激活函数为 Sigmoid。在论文中，作者采用的通道数 C=64，r = 16。 Residual channel attention Block（RCAB） 输入一个特征 input，我们首先进行一个卷积-Relu-卷积操作得到 f，然后 f 再经过一个 CA 模块进行重新缩放得到 x，最后将 x 和 input 相加得到输出特征。其中，卷积操作都采用 3×3 的卷积核。 实现的细节RIR中RG个数：10；RG中RCAB的个数：20，conv的大小：3 x 3，channel：64 通道下采样的scale：16，C/16 = 4。]]></content>
      <categories>
        <category>super resolution</category>
      </categories>
      <tags>
        <tag>SR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[超分辨率论文摘要阅读]]></title>
    <url>%2F2019%2F09%2F03%2F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87%E8%AE%BA%E6%96%87%E6%91%98%E8%A6%81%E9%98%85%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[这篇博客的主要目的为了记录阅读的超分辨率论文的摘要部分，了解超分辨率领域的研究前沿进度。 值得注意的网页 github上关于超分辨率领域的SOAT论文的整理：https://github.com/YapengTian/Single-Image-Super-Resolution 知乎上关于超分辨率一些大牛的主页： https://www.zhihu.com/search?type=content&amp;q=%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87 论文阅读Xuaner Zhang, Qifeng Chen, Ren Ng, and Vladlen Koltun. Zoom to Learn, Learn to Zoom, CVPR 2019. [Paper]作者将超分辨率方法应用在数字变焦中，他认为真实的图片能够比生成的图片更能保留数据的细节，网络的性能也将更好。那些在制作的数据集上训练的模型，通常在实际场景下性能不好，因此本文使用单反去直接制作数据集。高分辨率使用长焦距拍摄，低分辨率使用短焦距拍摄。 由于使用单反采集的数据高低配置无法完全对齐，因此作者提出了CoBi loss function，完美的解决了这个问题。这就是本文的主要insight。 Image Super-Resolution Using Very Deep Residual Channel Attention Networks]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>超分辨率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[电阻率成像数据分析]]></title>
    <url>%2F2019%2F08%2F30%2F%E7%94%B5%E9%98%BB%E7%8E%87%E6%88%90%E5%83%8F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[电阻率成像(ERI) 是一种地球物理技术，用于通过在表面或在一个或多个钻孔中的电极进行的电阻率测量来对底层亚表面结构进行成像。 电阻率数据的采集方位电阻率成像测井是在双侧向测井基础上发展起来的，在主电极或屏蔽电极中部沿圆周剖分成12个长方形小电极（见图），每个电极的定向方位成30°辐射，12个方位电极电位彼此相等。 电流的大小反映了该方向内地层电阻率的变化。测量每个方位电极的供电电流和环状监督电极M 3 （M 4 ）相对铠装电缆钢丝外皮的电位，可计算该方向地层的视电阻率。 地层中不同的岩石（泥岩、砂岩、石灰岩）、流体其电阻率是不同的，通过测量井壁 各点的电阻率值，然后将电阻率值的相对高低用灰度或色度图表示出来。井壁可以表示成一张黑白/彩色图像。 颜色映射如下： 得到的电阻率成像图像如下： 电阻率数据的分析微电极测井使用的电极紧贴井壁，电阻数据是测井井周一圈的数据，因此同一个水平面上数据的空间位置十分的接近。数据在空间关系上有一定的相关性。  上图中的绿线是地层的分层线。对电阻率的分析过程是将电阻率数据传入一个专业软件中，将会自动生成一些简单的分层线，然后采用人工标注的方式，对电阻率数据标注进行完善。最终得到完善的电阻率标注图。 对超分辨率问题来说，有什么内在的约束？ 得到新数据时，需要明白测量的精度（2.5mm），井口的大小这些数据。 反演的概念：通过一些观察到的局部信息，反推相关过程发生的原因以及机制。根据结果或信息反推事件发生的过程称为反演，而对事件发生过程的预测则称为正演。例如根据地表上探测到的部分数据，来推测地表以下的地质结构。]]></content>
      <categories>
        <category>super resolution</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[并查集，python示例]]></title>
    <url>%2F2019%2F08%2F27%2F%E5%B9%B6%E6%9F%A5%E9%9B%86%EF%BC%8Cpython%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[并查集是一种数据结构，在合并不相交的集合，用来判断一个图中是否有环这种问题时，具有很高的性能。 并查集并查集的主要操作就是为一个集合中的元素找到一个代表（根节点）。并查集的基本操作是合并两个集合，当拿到两个节点，第一步需要找到各自节点的根，然后选择一个节点作为新的代表，那么就完成了两个集合的合并。 并查集实现并查集可以使用一个数组来表示，数组表示图上的节点，下标表示节点的编号，数组的值表示该下标的父节点是哪一个。例如A[0] = 1 表示节点0的父节点是节点1. 并查集的实现过程主要分为两步，一步是实现节点的根的查找，另一步是实现两个集合的合并，这里包含了节点的路径压缩。 下面实现find_root算法： 12345678joint = 10parent = [-1]*10def find_root(parent,x): x_root = x while parent[x_root] != -1: x_root = parent[x_root] return x_root 上面代码说明当x不是根节点时，循环继续往上找，当x时根节点时则返回。 下面是union的算法： 123456789def union_joint(parent,x,y): x = find_root(parent,x) y = find_root(parent,y) if x == y: print('circle') return 0 else: parent[x] = y return 1 上诉代码如果返回的结果是0的话则说明存在一个环，否则不存在环。 存在一种极端的情况，即每次union合成的集合它形成了一个很长的链，每次寻找一个节点的根需要遍历一下整个节点，复杂度太高，下面在union中引入路径压缩的思想，即引入另一个数组rank，表明当前节点的位置，当进行union的时候，rank小的数连接到rank大的树底下，当两个rank相同的时候，可以随意连接，但是连接之后作为父节点的rank需要加1： 123456789101112131415rank = [0]*jointdef union(parent,x,y,rank): x = find_root(parent,x) y = find_root(parent,y) if x == y: print('circle') return 0 else: if rank[x] &gt; rank[y]: parent[y] = x elif rank[x] &lt; rank[y]: parent[x] = y else: parent[x] = y rank[y] += 1 在判断一个图是否存在环的时候，依次遍历图的所有边，如果union返回的结果是0的话，表明有环。 下面是一道lettcode的题目，思路就是用并查集来求解： 547.Friend Circles 思路是将朋友的关系用边来表示，最后看parent数组中有多少根节点（等于-1）。 解法代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution(object): def findCircleNum(self, M): """ :type M: List[List[int]] :rtype: int """ edge = [] if M == [] or M[0] == []: return 0 for i in range(len(M)): for j in range(len(M[0])): if i &lt;= j: break if M[i][j] == 1: edge.append([i,j]) parent = [-1]*len(M) rank = [0]*len(M) def find_root(parent,x): x_root = x while parent[x_root] != -1: x_root = parent[x_root] return x_root def union_joint(parent,x,y,rank): x = find_root(parent,x) y = find_root(parent,y) if x != y: if rank[x] &lt; rank[y]: parent[x] = y elif rank[x] &gt; rank[y]: parent[y] = x else: parent[x] = y rank[y] += 1 for e in edge: union_joint(parent,e[0],e[1],rank) ans = 0 for i in parent: if i == -1: ans += 1 return ans]]></content>
      <categories>
        <category>算法扫盲</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[哈希表，python示例]]></title>
    <url>%2F2019%2F08%2F25%2F%E5%93%88%E5%B8%8C%E8%A1%A8-python%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[哈希表一直都是一个很重要的数据结构，从上大学开始，一直有听闻，面试题也有相当的涉及，接下来继续扫盲。 哈希表哈希表根据关键码值(Key value)而直接进行访问的数据结构。也就是说，它通过把关键码值通过哈希函数映射到表中一个位置来访问记录，以加快查找的速度。 哈希表的工作原理如下 首先拿到key值，通过哈希函数将key值转化为数组的下标，在插入元素之前，判断该下标位置上是否已经存在元素，若已经存在元素则称为collision（碰撞）。 当元素发生碰撞时，存在很多方法来处理这种碰撞，常用的方法有链接法（java hashmap的实现），每一个index位置连一个链表，用来存储发生碰撞的元素。 另一种解决碰撞的方法为开放寻址法（python中dict的实现）。 开放寻址法指当前位置发生了碰撞，采用某种方法（线性，二次，双倍散列）对哈希表中其他位置进行访问。如果哈希表全都装满了则需要对哈希表进行扩容。 python 中dict常用方法遍历操作： 123for i in dicts: print(i) print(dicts[i]) 删除操作： 123dicts.pop(key)dicts.popitem() #删除最后一个加入的元素del dicts #直接删除元素]]></content>
      <categories>
        <category>算法扫盲</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[堆排序，python实现]]></title>
    <url>%2F2019%2F08%2F22%2F%E5%A0%86%E6%8E%92%E5%BA%8F%EF%BC%8Cpython%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[堆排序这个名称一直困扰着我，现在扫一下盲。 堆首先介绍一下堆的概念：堆是一棵完全二叉树，即指允许最后一层的叶子是不满的，其他层都是满的。叶子节点的出现顺序也是从左边开始向右边累加，不允许中断。父结点必须比子节点要大。 堆排序堆排序的算法复杂度是O(nlog(n))。由于节点满足完全二叉树，因此可以通过下标的关系找到父节点，子节点。 例如当前节点为i，父节点：(i - 1) /2。左孩子：2i+1,右孩子：2i+2。因此堆排序的策略如下： 堆排序步骤 构造堆结构，从最后一个元素（叶子）的父节点开始，循环到根节点，每次执行heapify函数（三个节点，找最大的放到根位置）。 位于根节点的元素是最大的，每次将根节点的数拿出来，作为排序的最后一个值。然后将最后一个叶节点放到根的位置。依次循环下去，直到结束。 实现代码12345678910111213141516171819202122232425262728293031323334nums = [9,3,4,1,5,6,8,7]def heapify(nums,n,i): ''' i 表示要进行调换的根节点位置 ''' c1 = 2*i + 1 c2 = 2*i + 2 max_index = i if c1 &lt;= n and nums[c1] &gt; nums[i]: max_index = c1 if c2 &lt;= n and nums[c2] &gt; nums[max_index]: max_index = c2 if max_index != i: nums[max_index],nums[i] = nums[i],nums[max_index]def build_heap(nums): n = len(nums) - 1 last_index = (n - 1) // 2 for i in range(last_index+1)[::-1]: heapify(nums,n,i)def heap_sort(nums): print(nums) build_heap(nums) print(nums) for i in range(len(nums))[::-1]: print(i) nums[0],nums[i] = nums[i],nums[0] heapify(nums,i-1,0) print(nums) heap_sort(nums)]]></content>
      <categories>
        <category>算法扫盲</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习代码的框架]]></title>
    <url>%2F2019%2F08%2F16%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%A3%E7%A0%81%E7%9A%84%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[以pytorch为例，梳理一下深度学习中，数据的读取，神经网络的搭建，NMS，以及各个指标的计算流程。 main 函数，程序入口，以及代码配置通常main函数中，通过实现argparse功能包，从函数的外部接受参数的传入，对数据，网络等进行一些基本的配置。argparse的使用方法：https://docs.python.org/zh-cn/3/library/argparse.html main函数中一些常用的配置项： 数据集的格式：coco，csv，pascal voc等等 数据的路径，包括训练集，测试集的路径等等 网络的一些细节配置，如深度，backbone 类型 一些功能的开关设置，如数据的增强等 训练过程中，一些变量的设置，比如epoch的设置，batch_size的设置等等 数据读取部分数据读取部分的操作包括数据集文件的读取，对图片进行数据的增强，继承dataloader实现数据的批量读取。 数据文件的读取这部分读取任务主要包括读取annotation文件，以及class_id文件，这里以csv格式的数据集文件为例。 首先实现一个CSVDataset类，继承至torch.utils.data.Dataset类。该类必须实现__len__,__getitem__两个方法。 在CSVDataset方法的__init__中，进行数据集文件的读取，最终将得到： self.classes self.image_names : list 包含所有的数据集图片路径 self.image_data: dict[image_name] = [ {x1,y1,x2,y2,class_name},…] __getitem__函数中需要实现的方法有根据下标来得到image，以及其对应的标注。最终返回的格式为： sample = {&#39;img&#39;: img, &#39;annot&#39;: annot}。在返回之前，如果有数据增强部分，还需要进行数据的增强。 数据增强数据增强的方法有很多种，常用的图片的翻转，切割，resize，归一化等等。数据增强利用一张图片，得到它的许多副本，有效的增大数据集。数据增强能够起效果的一个本质因素在于，卷积操作对位移，视角，图片大小，光照等因素具有不变性。数据增强有线下增强和线上增强两种方式，后一种方式在dataloader提取数据的时候，才对数据进行增强。 数据增强的方法通常可以写成一个类，通过pytorch中的transforms.Compose([Augumenter(),Resizer()]) 来对所有的增强方法进行整合。 Normalizer 实现一个Normalizer类，覆盖其中的__call__方法，对每张图片做一个正则化。 1234567891011class Normalizer(object): def __init__(self): self.mean = np.array([[[0.485, 0.456, 0.406]]]) self.std = np.array([[[0.229, 0.224, 0.225]]]) def __call__(self, sample): image, annots = sample['img'], sample['annot'] return &#123;'img':((image.astype(np.float32)-self.mean)/self.std), 'annot': annots&#125; argument 实现对图片的翻转，需要注意对标注也要进行处理。 Resizer 该方法意图将图片的大小限制在一定范围以内。因此在缩放的时候，需要找到最大的缩放比例,同时保证图片能够被32整除。 123456789101112131415161718192021222324252627282930313233class Resizer(object): """Convert ndarrays in sample to Tensors.""" def __call__(self, sample, min_side=608, max_side=1024): #将图片resize到608，1024以下的大小 image, annots = sample['img'], sample['annot'] # 不能超过这个尺寸（有一边等于这个尺寸） rows, cols, cns = image.shape smallest_side = min(rows, cols) # rescale the image so the smallest side is min_side scale = min_side / smallest_side # check if the largest side is now greater than max_side, which can happen # when images have a large aspect ratio largest_side = max(rows, cols) if largest_side * scale &gt; max_side: scale = max_side / largest_side # resize the image with the computed scale image = skimage.transform.resize(image, (int(round(rows*scale)), int(round((cols*scale))))) rows, cols, cns = image.shape pad_w = 32 - rows%32 pad_h = 32 - cols%32 new_image = np.zeros((rows + pad_w, cols + pad_h, cns)).astype(np.float32) new_image[:rows, :cols, :] = image.astype(np.float32) # 两个边长需要保证被32整除，少掉的的那部分使用0来补全 annots[:, :4] *= scale return &#123;'img': torch.from_numpy(new_image), 'annot': torch.from_numpy(annots), 'scale': scale&#125; 数据调用 dataloaderpytorch通过实现dataloader方法来实现网络训练时，每次iteration的数据的输出。dataloader的逻辑是，每次从dataset中调用__getitem__()获取单个数据，然后组合成batch，在使用collate_fn参数对batch进行一些操作。 torch.utils.data.Dataloader中的参数： dataset(Dataset) – dataset from which to load the data. batch_size(int, optional) – how many samples per batch to load (default: 1). shuffle(bool, optional) – set to Trueto have the data reshuffled at every epoch (default: False). sampler(Sampler, optional) – defines the strategy to draw samples from the dataset. If specified, shufflemust be False. batch_sampler(Sampler, optional) – like sampler, but returns a batch of indices at a time. Mutually exclusive with batch_size, shuffle, sampler, and drop_last. num_workers(int, optional) – how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0) collate_fn(callable, optional) – merges a list of samples to form a mini-batch. pin_memory(bool, optional) – If True, the data loader will copy tensors into CUDA pinned memory before returning them. drop_last(bool, optional) – set to Trueto drop the last incomplete batch, if the dataset size is not divisible by the batch size. If Falseand the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False) timeout(numeric, optional) – if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: 0) worker_init_fn(callable, optional) – If not None, this will be called on each worker subprocess with the worker id (an int in [0, num_workers - 1]) as input, after seeding and before data loading. (default: None) 算法中使用如下参数： 1dataloader_train = DataLoader(dataset_train, num_workers=3, collate_fn=collater, batch_sampler=sampler) 其中dataset_train为Dataset类的对象，如上实现数据问价读取的部分。num_workers设置了这个类的线程数。batch_sampler 设置了每次从数据集中返回一个batch的sample的策略。collate_fn 将一系列的样本融合成一个小的mini-batch。 首先是batch_sampler: 继承至采样器类，需要实现其中的__len__方法，__iter__方法。该参数的作用是将数据集做成许多group组成的一个list。 12345678910111213141516171819202122232425class AspectRatioBasedSampler(Sampler): def __init__(self, data_source, batch_size, drop_last): self.data_source = data_source self.batch_size = batch_size self.drop_last = drop_last self.groups = self.group_images() def __iter__(self): random.shuffle(self.groups) for group in self.groups: yield group def __len__(self): if self.drop_last: return len(self.data_source) // self.batch_size else: return (len(self.data_source) + self.batch_size - 1) // self.batch_size def group_images(self): # determine the order of the images order = list(range(len(self.data_source))) order.sort(key=lambda x: self.data_source.image_aspect_ratio(x)) # divide into groups, one group = one batch return [[order[x % len(order)] for x in range(i, i + self.batch_size)] for i in range(0, len(order), self.batch_size)] 如上，这个方法将数据分别存入group中，然后组成一个groups的list。通过一个__iter__()方法，迭代的方式将数据输出。每次输出一个batch大小的数据。 collate_fn参数： 该参数接受来自batch_sampler的数据，对数据进行进一步的处理。 12345678910111213141516171819202122232425def collater(data): imgs = [s['img'] for s in data] annots = [s['annot'] for s in data] scales = [s['scale'] for s in data] widths = [int(s.shape[0]) for s in imgs] heights = [int(s.shape[1]) for s in imgs] batch_size = len(imgs) max_width = np.array(widths).max() max_height = np.array(heights).max() padded_imgs = torch.zeros(batch_size, max_width, max_height, 3) for i in range(batch_size): img = imgs[i] padded_imgs[i, :int(img.shape[0]), :int(img.shape[1]), :] = img max_num_annots = max(annot.shape[0] for annot in annots) if max_num_annots &gt; 0: annot_padded = torch.ones((len(annots), max_num_annots, 5)) * -1 if max_num_annots &gt; 0: for idx, annot in enumerate(annots): #print(annot.shape) if annot.shape[0] &gt; 0: annot_padded[idx, :annot.shape[0], :] = annot else: annot_padded = torch.ones((len(annots), 1, 5)) * -1 padded_imgs = padded_imgs.permute(0, 3, 1, 2) return &#123;'img': padded_imgs, 'annot': annot_padded, 'scale': scales&#125; 上面的操作，将同一个batch中的图片的大小统一同样的大小。annotation的维度也统一到同样大小的维度。然后进行RGB通道的变换之后，放回一个dict。 上面这些步骤就完成了数据的loader，通过for循环从其中取得元素。 retinanet网络结构下面从数据流动的角度分析一下retinanet的各个结构的组成。 retinanet的特征提取部分，使用的是resnet，resnet有多种深度的选择，分别有18，34，50，101，152五种深度。常用的网络深度为50，101: 123456789def resnet50(num_classes, pretrained=False, **kwargs): """Constructs a ResNet-50 model. Args: pretrained (bool): If True, returns a model pre-trained on ImageNet """ model = ResNet(num_classes, Bottleneck, [3, 4, 6, 3], **kwargs) if pretrained: model.load_state_dict(model_zoo.load_url(model_urls['resnet50'], model_dir='.'), strict=False) return model 让我们一行一行来看，第一个调用了ResNet()类，创建了一个ResNet对象。ResNet继承至nn.Module,需要实现函数__init__以及forward()两个方法，通常将可学习的参数放到构造函数__init__()中，在forward中实现网络数据的流动，即可实现网络的自动求导机制。 ResNet resnet首次提出残差的思想，传统的卷积网络或者全连接网络在信息传递的时候或多或少会存在信息丢失，损耗等问题，同时还有导致梯度消失或者梯度爆炸，导致很深的网络无法训练。ResNet通过学习残差的方式，在一定程度上解决了网络退化和梯度消失的问题。ResNet通过大量叠加残差块的方式，加深网络的深度的同时，保证了网络的梯度不消失。ResNet有着两种不同的残差单元。分别是basicBlock 和 bottleneck结构。深层次网络使用bottleneck结构，每次经过残差结构之前都对数据进行一次降维，大大降低了网络的参数量。 bottleneck的结构feature经过第一个1x1的卷积层，将特征的维度压缩，对压缩后的特征进行3x3的卷积，然后经过1x1卷积层，将特征的维度放大到原来的大小。 bottleneck的代码如下： 1234567891011121314151617181920212223242526272829303132class Bottleneck(nn.Module): expansion = 4 def __init__(self, inplanes, planes, stride=1, downsample=None): super(Bottleneck, self).__init__() self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False) self.bn1 = nn.BatchNorm2d(planes) self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False) self.bn2 = nn.BatchNorm2d(planes) self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False) self.bn3 = nn.BatchNorm2d(planes * 4) self.relu = nn.ReLU(inplace=True) self.downsample = downsample self.stride = stride def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return out pytorch中常用的搭建网络的函数如下： Conv2d卷积： 12345678910111213141516import torch.nn as nnnn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)参数：in_channels(int) – 输入信号的通道out_channels(int) – 卷积产生的通道kerner_size(int or tuple) - 卷积核的尺寸stride(int or tuple, optional) - 卷积步长padding(int or tuple, optional) - 输入的每一条边补充0的层数dilation(int or tuple, optional) – 卷积核元素之间的间距groups(int, optional) – 从输入通道到输出通道的阻塞连接数bias(bool, optional) - 如果bias=True，添加偏置输入：input: (N,C_in,H_in,W_in) 输出：output: (N,C_out,H_out,W_out)计算公式：Fout = (Fin + 2*padding-kernel)/stride + 1 batchNorm2d： 在训练时，该层计算每次输入的均值与方差，并进行移动平均。移动平均默认的动量值为0.1。 在验证时，训练求得的均值/方差将用于标准化验证数据。 12345678BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True)参数：num_features： 来自期望输入的特征数，该期望输入的大小为'batch_size x num_features x height x width'eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。momentum： 动态均值和动态方差所使用的动量。默认为0.1。affine： 一个布尔值，当设为true，给该层添加可学习的仿射变换参数。输入：（N, C，H, W) - 输出：（N, C, H, W）值得至于的是，参数num_feature写channel数即可。 ReLU：修正线性单元函数 1234nn.ReLU(inplace=False)参数：inplace：表示是否进行覆盖计算，节省内存不会引起数据维度的变化 MaxPool2d 层 1234567891011nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)参数：kernel_size(int or tuple) - max pooling的窗口大小stride(int or tuple, optional) - max pooling的窗口移动的步长。默认值是kernel_sizepadding(int or tuple, optional) - 输入的每一条边补充0的层数dilation(int or tuple, optional) – 一个控制窗口中元素步幅的参数return_indices - 如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助ceil_mode - 如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作输入: (N,C,H_&#123;in&#125;,W_in) 输出: (N,C,H_out,W_out)计算公式：Fout = (Fin + 2*padding - kernel)/stride + 1 nn.Upsample 上采样操作对channel进行采样： 12nn.Upsample(size=None, scale_factor=None, mode='nearest', align_corners=None)给定上采样策略mode，上采样的大小：scale_factor nn.Sequential一个有序的容器，神经网络模块将按照在传入构造器的顺序依次被添加到计算图中执行，同时以神经网络模块为元素的有序字典也可以作为传入参数。 12345downsample = nn.Sequential( nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes * block.expansion), ) 网络结构类继承至nn.Module,需要实现函数__init__以及forward()两个方法，通常在init中完成网络层的初始化工作，定义各类的网络层。在forward中完成网络层数据的流动。 retinanet金字塔模型的代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class PyramidFeatures(nn.Module): def __init__(self, C3_size, C4_size, C5_size, feature_size=256): super(PyramidFeatures, self).__init__() # upsample C5 to get P5 from the FPN paper self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0) self.P5_upsampled = nn.Upsample(scale_factor=2, mode='nearest') self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1) # add P5 elementwise to C4 self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0) self.P4_upsampled = nn.Upsample(scale_factor=2, mode='nearest') self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1) # add P4 elementwise to C3 self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0) self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1) # "P6 is obtained via a 3x3 stride-2 conv on C5" self.P6 = nn.Conv2d(C5_size, feature_size, kernel_size=3, stride=2, padding=1) # "P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6" self.P7_1 = nn.ReLU() self.P7_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=2, padding=1) def forward(self, inputs): C3, C4, C5 = inputs P5_x = self.P5_1(C5) P5_upsampled_x = self.P5_upsampled(P5_x) P5_x = self.P5_2(P5_x) P4_x = self.P4_1(C4) P4_x = P5_upsampled_x + P4_x P4_upsampled_x = self.P4_upsampled(P4_x) P4_x = self.P4_2(P4_x) P3_x = self.P3_1(C3) P3_x = P3_x + P4_upsampled_x P3_x = self.P3_2(P3_x) P6_x = self.P6(C5) P7_x = self.P7_1(P6_x) P7_x = self.P7_2(P7_x) return [P3_x, P4_x, P5_x, P6_x, P7_x] retinanet在金字塔之后，接了一个回归网络以及分类网络，分别对边框位置以及类别进行分类。 回归网络简单的接了五个卷积层，保持feature的大小不变，每一个channel的维度最终降为num_anchors x 4，即每一个channel需要回归出num_anchors x 4 个坐标点。 1234567891011121314151617181920212223242526272829303132333435363738class RegressionModel(nn.Module): def __init__(self, num_features_in, num_anchors=9, feature_size=256): super(RegressionModel, self).__init__() self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1) self.act1 = nn.ReLU() self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act2 = nn.ReLU() self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act3 = nn.ReLU() self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act4 = nn.ReLU() self.output = nn.Conv2d(feature_size, num_anchors*4, kernel_size=3, padding=1) def forward(self, x): out = self.conv1(x) out = self.act1(out) out = self.conv2(out) out = self.act2(out) out = self.conv3(out) out = self.act3(out) out = self.conv4(out) out = self.act4(out) out = self.output(out) # out is B x C x W x H, with C = 4*num_anchors out = out.permute(0, 2, 3, 1) return out.contiguous().view(out.shape[0], -1, 4) 上诉最后一行值得注意一下view()函数相当于numpy中的reshape函数，但是要求数据必须在内存中是连续存储的。由于permute函数，改变了数据的分布（浅拷贝）。因此在使用view之前，需要执行contiguous函数使得数据内存连续分布。最终out的shape为[batch_size，w x h ，4]。上诉得到的out最终输入criterion中，计算loss。 分类模型的网络结构和回归模型的结构相同，唯一不同的地方在于最终输出的channel的大小。分类模型输出的channel大小为anchor的数量乘以类别（num_anchor x num_classes）。即每一个框都要预测一个类别信息。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class ClassificationModel(nn.Module): def __init__(self, num_features_in, num_anchors=9, num_classes=80, prior=0.01, feature_size=256): super(ClassificationModel, self).__init__() self.num_classes = num_classes self.num_anchors = num_anchors self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1) self.act1 = nn.ReLU() self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act2 = nn.ReLU() self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act3 = nn.ReLU() self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1) self.act4 = nn.ReLU() self.output = nn.Conv2d(feature_size, num_anchors*num_classes, kernel_size=3, padding=1) self.output_act = nn.Sigmoid() def forward(self, x): out = self.conv1(x) out = self.act1(out) out = self.conv2(out) out = self.act2(out) out = self.conv3(out) out = self.act3(out) out = self.conv4(out) out = self.act4(out) out = self.output(out) out = self.output_act(out) # out is B x C x W x H, with C = n_classes + n_anchors out1 = out.permute(0, 2, 3, 1) batch_size, width, height, channels = out1.shape out2 = out1.view(batch_size, width, height, self.num_anchors, self.num_classes) return out2.contiguous().view(x.shape[0], -1, self.num_classes) 最后一行首先将out的维度控制在anchor x num_classes，然后通过一个view将其变为[x.shape[0],W x H x anchor, num_classes]，每一个值表示一个框的类别，然后到criterion中去做预测。 Torch.cat 用法：https://blog.csdn.net/qq_39709535/article/details/80803003 接下来需要生成anchor。 anchor的生成anchor的设置上面，对于retinaNet最终的P3，P4，P5，P6，P7均有一个不同的设置。anchor的长宽比和scale的大小分别有三种设置，一共有9种组合。anchor的大小与feature map的大小也是相关的。 12self.ratios = np.array([0.5,1,2])self.scales = np.array([2**0,2**(1.0/3.0),2**(2.0/3.0)]) 几个常用的函数： 1234a = [1,2,3]a = np.tile(a,(2,3))# a = [[1,2,3,1,2,3,1,2,3] [1.2,3,1,2,3,1,2,3]] np.repeat 1234a = [1,2,3]a = np.repeat(a,2)# a = [1,1,2,2,3,3]# 与np.tile的区别是，他是一个元素一个元素的增加后进行排序的。tile则是一起增加。 生成anchor的代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293class Anchors(nn.Module): def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None): super(Anchors, self).__init__() if pyramid_levels is None: self.pyramid_levels = [3, 4, 5, 6, 7] if strides is None: self.strides = [2 ** x for x in self.pyramid_levels] if sizes is None: self.sizes = [2 ** (x + 2) for x in self.pyramid_levels] if ratios is None: self.ratios = np.array([0.5, 1, 2]) if scales is None: self.scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]) def forward(self, image): # image = [2,3,640,832] image_shape = image.shape[2:] image_shape = np.array(image_shape) image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels] # compute anchors over all pyramid levels all_anchors = np.zeros((0, 4)).astype(np.float32) for idx, p in enumerate(self.pyramid_levels): anchors = generate_anchors(base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales) shifted_anchors = shift(image_shapes[idx], self.strides[idx], anchors) all_anchors = np.append(all_anchors, shifted_anchors, axis=0) all_anchors = np.expand_dims(all_anchors, axis=0) return torch.from_numpy(all_anchors.astype(np.float32)).cuda()def generate_anchors(base_size=16, ratios=None, scales=None): """ Generate anchor (reference) windows by enumerating aspect ratios X scales w.r.t. a reference window. """ if ratios is None: ratios = np.array([0.5, 1, 2]) if scales is None: scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]) num_anchors = len(ratios) * len(scales) # 9个点 # initialize output anchors anchors = np.zeros((num_anchors, 4)) # 每一个位置上都有9个点，每个点都有四个坐标值 # scale base_size,feature 的大小与scale相乘，得到每一层anchor的大小 anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T # compute areas of anchors areas = anchors[:, 2] * anchors[:, 3] # correct for ratios 构造长宽比 anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales))) anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales)) # transform from (x_ctr, y_ctr, w, h) -&gt; (x1, y1, x2, y2) anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T return anchors def shift(shape, stride, anchors): shift_x = (np.arange(0, shape[1]) + 0.5) * stride shift_y = (np.arange(0, shape[0]) + 0.5) * stride shift_x, shift_y = np.meshgrid(shift_x, shift_y) # shifts = [shape[0]*shape[1],4] shifts = np.vstack(( shift_x.ravel(), shift_y.ravel(), shift_x.ravel(), shift_y.ravel() )).transpose() # add A anchors (1, A, 4) to # cell K shifts (K, 1, 4) to get # shift anchors (K, A, 4) # reshape to (K*A, 4) shifted anchors A = anchors.shape[0] K = shifts.shape[0] # 下面这一行进行了广播赋值，每一行都赋予维度不同的行进行广播， # 最终形成[1,A,4] + [k,1,4] = [k,A,4],其中k = shape[0]*shape[1] # 也就是说每一个像素位置都将产生9个anchor，每个anchor有四个坐标。 shape的大小则是由计算产生的 # 每张图片在每个level处的大小在__init__处进行初始化 all_anchors = (anchors.reshape((1, A, 4)) + \ shifts.reshape((1, K, 4)).transpose((1, 0, 2))) all_anchors = all_anchors.reshape((K * A, 4)) return all_anchors 每一行进行分析就是先设置每一层feature map的level，stride，sizes，ratios，scales的值。然后在forward里面generate_anchor()，对每一个level的feature生成符合要求的size的anchor，长宽比组合后共9种anchor。具体的设置可看代码。 然后进入shift()函数，shift()函数的作用是将anchor散布到每一个位置上。流程大概是，一张图片进来，分别计算出这种图片在每一层level上的size大小，然后根据每一层的anchor的大小，每一个像素点位置取9个anchor，然后返回一个$[shape[0]shape[1]9,4]$ 大小的矩阵。 几个函数： 123456789np.meshgrid(x,y)# 将x中元素与y中元素一一对应起来组合成坐标的形式。np.vstack((x,y))# 将x，y中元素按照垂直方向叠加#ravel()a = [[2,2],[1,1]]a.ravel() # 将多维数组拉平，不存生新的副本 a = [2,2,1,1]a.flatten() # 作用与上面函数相同，将返回一个数据副本np.squeeze([[1],[2],[3]]) # 对维度为1的数据进行压缩，得到[1,2,3]a = a.reshape(-1) # 同样能够得到1维的数据a.transpose() # 不指定参数表示对矩阵进行转置 经过上面的过程，在for循环部分，将5层的anchor全部装入一个list中，anchor生成完毕。 torch.cat函数 1234a = [1,2,3]b = [3,4,5]torch.cat((a,b),0) # 垂直方向 [[1,2,3],[3,4,5]]torch.cat((a,b),1) # 水平方向 [[1,2,3,4,5,6]] focalLoss部分focalLoss紧接着上面的一部分。现在回过头来梳理一下网络中数据流动到的位置： 将图片输入ResNet中，通过一个多层金字塔结构，输出5个不同深度feature map（P3，P4，P5，P6，P7），依次将这些层输入到regression网络和classification网络中，每一层都将得到$[batch,wh,4]$的输出和$[batch,wh*anchors,class_nums]$的输出，然后将所有结果cat到一起（水平拼接），即所有level上的anchor 的预测框会被cat到regression_anchor 和classification_anchor中。接下来要做的是判断这些anchor的好坏。根据我们的先验知识，我们产生了一部分anchor的设置，我们将网络产生的anchor和我们预生成的anchor输入focalLoss中，对anchor进行过滤，计算产生的loss。 下面介绍focalLoss： focalLoss部分按batch为单位，每次输入一个batch的数据，然后进行loss的计算。首先计算预设置的anchor与当前图片GT的IoU。（重叠部分 / 相并部分） 12345678910111213def calc_iou(a, b): area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1]) iw = torch.min(torch.unsqueeze(a[:, 2], dim=1), b[:, 2]) -\ torch.max(torch.unsqueeze(a[:, 0], 1), b[:, 0]) ih = torch.min(torch.unsqueeze(a[:, 3], dim=1), b[:, 3]) -\ torch.max(torch.unsqueeze(a[:, 1], 1), b[:, 1]) iw = torch.clamp(iw, min=0) ih = torch.clamp(ih, min=0) ua = torch.unsqueeze((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), dim=1) + area - iw * ih ua = torch.clamp(ua, min=1e-8) intersection = iw * ih IoU = intersection / ua return IoU focalLoss主要对每一个anchor进入classification的分类结果，focalLoss的原理如下： 整个网络的loss其实由两部分组成，一部分是分类loss，一部分是回归loss。分类loss即focal loss，回归部分的loss为边框回归的loss。实现代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126class FocalLoss(nn.Module): #def __init__(self): def forward(self, classifications, regressions, anchors, annotations): alpha = 0.25 gamma = 2.0 batch_size = classifications.shape[0] classification_losses = [] regression_losses = [] anchor = anchors[0, :, :] anchor_widths = anchor[:, 2] - anchor[:, 0] anchor_heights = anchor[:, 3] - anchor[:, 1] anchor_ctr_x = anchor[:, 0] + 0.5 * anchor_widths anchor_ctr_y = anchor[:, 1] + 0.5 * anchor_heights for j in range(batch_size): classification = classifications[j, :, :] regression = regressions[j, :, :] bbox_annotation = annotations[j, :, :] bbox_annotation = bbox_annotation[bbox_annotation[:, 4] != -1] if bbox_annotation.shape[0] == 0: regression_losses.append(torch.tensor(0).float().cuda()) classification_losses.append(torch.tensor(0).float().cuda()) continue classification = torch.clamp(classification, 1e-4, 1.0 - 1e-4) IoU = calc_iou(anchors[0, :, :], bbox_annotation[:, :4]) # num_anchors x num_annotations IoU_max, IoU_argmax = torch.max(IoU, dim=1) # num_anchors x 1 #import pdb #pdb.set_trace() # compute the loss for classification # target 的维度为类别的个数 targets = torch.ones(classification.shape) * -1 targets = targets.cuda() # lt : less than 如果IoU_max的面积小于0.4，那么就认为没有匹配上 targets[torch.lt(IoU_max, 0.4), :] = 0 positive_indices = torch.ge(IoU_max, 0.5) num_positive_anchors = positive_indices.sum() # IoU_argmax记录着当前的anchor与哪一个GT比较匹配 # 下面这个赋值语句就是给对应的anchor选择一个GT # 第一个参数选择候选的anchor，第二个参数将候选anchor的坐标值都取到 assigned_annotations = bbox_annotation[IoU_argmax, :] targets[positive_indices, :] = 0 # 下面一句表明对每个满足IoU条件的anchor，赋予一个类别。形成一个one hot编码（原先target的维度长度等于类别的个数） targets[positive_indices, assigned_annotations[positive_indices, 4].long()] = 1 alpha_factor = torch.ones(targets.shape).cuda() * alpha alpha_factor = torch.where(torch.eq(targets, 1.), alpha_factor, 1. - alpha_factor) # 对focal weight进行统一的计算，然后赋值 focal_weight = torch.where(torch.eq(targets, 1.), 1. - classification, classification) focal_weight = alpha_factor * torch.pow(focal_weight, gamma) # 当y=1,即只有targets=1参与计算 当y=0，即只有targets=0参与 bce = -(targets * torch.log(classification) + (1.0 - targets) * torch.log(1.0 - classification)) # cls_loss = focal_weight * torch.pow(bce, gamma) cls_loss = focal_weight * bce # 注意对target的处理，当IoU在【0.4，0.5】之间时target=-1，不提供loss，其他情况均赋予一个cls_loss cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, torch.zeros(cls_loss.shape).cuda()) # 计算所有的loss在正例中的平均值 classification_losses.append(cls_loss.sum()/torch.clamp(num_positive_anchors.float(), min=1.0)) # compute the loss for regression #只有预测为正例的部分参与边框的回归，下面一部分为回归loss。 if positive_indices.sum() &gt; 0: assigned_annotations = assigned_annotations[positive_indices, :] anchor_widths_pi = anchor_widths[positive_indices] anchor_heights_pi = anchor_heights[positive_indices] anchor_ctr_x_pi = anchor_ctr_x[positive_indices] anchor_ctr_y_pi = anchor_ctr_y[positive_indices] gt_widths = assigned_annotations[:, 2] - assigned_annotations[:, 0] gt_heights = assigned_annotations[:, 3] - assigned_annotations[:, 1] gt_ctr_x = assigned_annotations[:, 0] + 0.5 * gt_widths gt_ctr_y = assigned_annotations[:, 1] + 0.5 * gt_heights # clip widths to 1 gt_widths = torch.clamp(gt_widths, min=1) gt_heights = torch.clamp(gt_heights, min=1) targets_dx = (gt_ctr_x - anchor_ctr_x_pi) / anchor_widths_pi targets_dy = (gt_ctr_y - anchor_ctr_y_pi) / anchor_heights_pi targets_dw = torch.log(gt_widths / anchor_widths_pi) targets_dh = torch.log(gt_heights / anchor_heights_pi) targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh)) targets = targets.t() targets = targets/torch.Tensor([[0.1, 0.1, 0.2, 0.2]]).cuda() negative_indices = 1 - positive_indices regression_diff = torch.abs(targets - regression[positive_indices, :]) regression_loss = torch.where( torch.le(regression_diff, 1.0 / 9.0), 0.5 * 9.0 * torch.pow(regression_diff, 2), regression_diff - 0.5 / 9.0 ) regression_losses.append(regression_loss.mean()) else: regression_losses.append(torch.tensor(0).float().cuda()) return torch.stack(classification_losses).mean(dim=0, keepdim=True), torch.stack(regression_losses).mean(dim=0, keepdim=True) 边框回归部分学习一个边框的平移以及缩放关系： 最终将得到的分类loss以及regression loss的平均值整合成一个stack，返回下一步。 几个函数： 12345torch.cat(a,b) #水平方向将a与b进行拼接torch.clamp(a,min_val,max_val) # 将a中的值控制在min_val与max_val之间，小于取min_val，大于取max_valmax_val, max_index = torch.max(a,dim = 1) # 返回每一列最大值以及每一列的最大值的索引torch.lt(a,0.4) # 返回a中值小于0.4的元素的下标，ge均类似torch.where(condition,true_val,false_val) # 如果满足条件者该位置为true_val,否则为false_val,其中参数的维度均相同（比如都为三维） 训练阶段训练部分有几个需要完成的工作： 初始化网络，设置优化器等等 将数据从dataloader中取出来 将数据输入网络中，得到网络的loss值 对loss进行反向传播，一些操作如learning rate的降低，梯度的裁剪可以在其中完成 打印出每个batch训练的结果 当训练次数到达一定的epoch时，对网络进行evaluate 保存mAP较高的网络 下面通过代码来解读： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 将训练过程迁移到gpu上 use_gpu = Trueif use_gpu: retinanet = retinanet.cuda()retinanet = torch.nn.DataParallel(retinanet).cuda()retinanet.training = True # 设置优化器为adamoptimizer = optim.Adam(retinanet.parameters(), lr=1e-5) # ；learning rate的缩减器scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)loss_hist = collections.deque(maxlen=500) # 实现了两端的快速添加删除retinanet.train()retinanet.module.freeze_bn()print('Num training images: &#123;&#125;'.format(len(dataset_train)))# 从dataloader中取数据 for epoch_num in range(parser.epochs): retinanet.train() retinanet.module.freeze_bn() epoch_loss = [] for iter_num, data in enumerate(dataloader_train): try: # 清空梯度，由于pytorch在每次backward的时候， # 会进行梯度的累积，这样的做法方便训练RNN模型 # 但是在训练普通模型的时候，需要将累积的梯度清空。 # 清空后做backward梯度方向有利于梯度的整体下降 optimizer.zero_grad() # 将数据传入网络中，得到loss classification_loss, regression_loss = retinanet([data['img'].cuda().float(), data['annot']]) classification_loss = classification_loss.mean() regression_loss = regression_loss.mean() loss = classification_loss + regression_loss if bool(loss == 0): continue # 误差的反向传播 loss.backward() # 梯度裁剪函数,第二个参数表明允许最大的梯度为0.1 torch.nn.utils.clip_grad_norm_(retinanet.parameters(), 0.1) optimizer.step() loss_hist.append(float(loss)) epoch_loss.append(float(loss)) print('Epoch: &#123;&#125; | Iteration: &#123;&#125; | Classification loss: &#123;:1.5f&#125; | Regression loss: &#123;:1.5f&#125; | Running loss: &#123;:1.5f&#125;'.format(epoch_num, iter_num, float(classification_loss), float(regression_loss), np.mean(loss_hist))) del classification_loss del regression_loss except Exception as e: print(e) continue if parser.dataset == 'coco': print('Evaluating dataset') # 验证集验证模型的有效性 coco_eval.evaluate_coco(dataset_val, retinanet) elif parser.dataset == 'csv' and parser.csv_val is not None: print('Evaluating dataset') mAP = csv_eval.evaluate(dataset_val, retinanet) scheduler.step(np.mean(epoch_loss)) # 保存训练好的模型 torch.save(retinanet.module, '&#123;&#125;_retinanet_&#123;&#125;.pt'.format(parser.dataset, epoch_num)) retinanet.eval()torch.save(retinanet, 'model_final.pt'.format(epoch_num)) 需要注意的点： 在网络进行训练或验证时，通常先进行一次： 123model.train()# or evaluatemodel.eval() 这样的目的是模型在train和eval的时候，需要执行的操作是不一样的。例如batchNorm和Dropout在eval的时候是不需要执行的。因此需要提前对网络进行设置。 eval 验证eval作为验证网络的性能，被安排在网络执行的最后，在每个batch结束，或者达到设定的epoch的时候，对网络进行测试。并以此为依据，是否对网络进行存储。 eval部分常用的指标是mAP，该指标通过计算recall以及precision的值来得到最终的结果。首先得到网络的eval的结果，然后从标注数据中得到anno的结果，进行mAP的计算。 得到网络的结果如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def _get_detections(dataset, retinanet, score_threshold=0.05, max_detections=100, save_path=None): """ Get the detections from the retinanet using the generator. The result is a list of lists such that the size is: all_detections[num_images][num_classes] = detections[num_detections, 4 + num_classes] # Arguments dataset : The generator used to run images through the retinanet. retinanet : The retinanet to run on the images. score_threshold : The score confidence threshold to use. max_detections : The maximum number of detections to use per image. save_path : The path to save the images with visualized detections to. # Returns A list of lists containing the detections for each image in the generator. """ all_detections = [[None for i in range(dataset.num_classes())] for j in range(len(dataset))] retinanet.eval() with torch.no_grad(): for index in range(len(dataset)): data = dataset[index] scale = data['scale'] # run network scores, labels, boxes = retinanet(data['img'].permute(2, 0, 1).cuda().float().unsqueeze(dim=0)) scores = scores.cpu().numpy() labels = labels.cpu().numpy() boxes = boxes.cpu().numpy() # correct boxes for image scale boxes /= scale # select indices which have a score above the threshold indices = np.where(scores &gt; score_threshold)[0] if indices.shape[0] &gt; 0: # select those scores scores = scores[indices] # find the order with which to sort the scores # 得到score从大到小的下标，然后选择其中的max_detections那么多个 scores_sort = np.argsort(-scores)[:max_detections] # select detections score从大到小 image_boxes = boxes[indices[scores_sort], :] image_scores = scores[scores_sort] image_labels = labels[indices[scores_sort]] image_detections = np.concatenate([image_boxes, np.expand_dims(image_scores, axis=1), np.expand_dims(image_labels, axis=1)], axis=1) # copy detections to all_detections for label in range(dataset.num_classes()): # 每一张图片均表示成一个index，对所有的label都遍历一边，每个label保存若干个anchor,没有的话则不保存 all_detections[index][label] = image_detections[image_detections[:, -1] == label, :-1] else: # copy detections to all_detections for label in range(dataset.num_classes()): all_detections[index][label] = np.zeros((0, 5)) print('&#123;&#125;/&#123;&#125;'.format(index + 1, len(dataset)), end='\r') return all_detections 从标注文件中读取图片的标注信息： 123456789101112131415161718def _get_annotations(generator): """ Get the ground truth annotations from the generator. The result is a list of lists such that the size is: all_detections[num_images][num_classes] = annotations[num_detections, 5] # Arguments generator : The generator used to retrieve ground truth annotations. # Returns A list of lists containing the annotations for each image in the generator. """ all_annotations = [[None for i in range(generator.num_classes())] for j in range(len(generator))] for i in range(len(generator)): # load the annotations annotations = generator.load_annotations(i) # copy detections to all_annotations for label in range(generator.num_classes()): all_annotations[i][label] = annotations[annotations[:, 4] == label, :4].copy() print('&#123;&#125;/&#123;&#125;'.format(i + 1, len(generator)), end='\r') return all_annotations 得到标注数据之后，开始计算mAP指标，mAP指标由recall（判断正确的占所有正确类别的百分比），precision（判断正确的占预测结果中认为正确的百分比）。通过对这两个指数的积分来计算最终的mAP结果。 recall = TP/(TP + FN) 即真正预测对的，占所有正类的比例 precision = TP/(TP + FN) 即真正预测对的，占预测结果为正的比例 TP,FP,TN,FN这几个指标第一个字母表示预测是不是对的，第二个字母表示，预测的内容是什么（正类或者负类）。关于mAP的计算可以看： 这里 下面代码计算mAP的内容： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117def compute_overlap(a, b): """ Parameters ---------- a: (N, 4) ndarray of float b: (K, 4) ndarray of float Returns ------- overlaps: (N, K) ndarray of overlap between boxes and query_boxes """ area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1]) iw = np.minimum(np.expand_dims(a[:, 2], axis=1), b[:, 2]) - np.maximum(np.expand_dims(a[:, 0], 1), b[:, 0]) ih = np.minimum(np.expand_dims(a[:, 3], axis=1), b[:, 3]) - np.maximum(np.expand_dims(a[:, 1], 1), b[:, 1]) iw = np.maximum(iw, 0) ih = np.maximum(ih, 0) ua = np.expand_dims((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), axis=1) + area - iw * ih ua = np.maximum(ua, np.finfo(float).eps) intersection = iw * ih return intersection / uadef _compute_ap(recall, precision): """ Compute the average precision, given the recall and precision curves. Code originally from https://github.com/rbgirshick/py-faster-rcnn. # Arguments recall: The recall curve (list). precision: The precision curve (list). # Returns The average precision as computed in py-faster-rcnn. """ # correct AP calculation # first append sentinel values at the end mrec = np.concatenate(([0.], recall, [1.])) mpre = np.concatenate(([0.], precision, [0.])) # compute the precision envelope for i in range(mpre.size - 1, 0, -1): mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i]) # to calculate area under PR curve, look for points # where X axis (recall) changes value i = np.where(mrec[1:] != mrec[:-1])[0] # and sum (\Delta recall) * prec ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1]) return apdef evaluate( generator, retinanet, iou_threshold=0.5, score_threshold=0.05, max_detections=100, save_path=None): """ Evaluate a given dataset using a given retinanet. # Arguments generator : The generator that represents the dataset to evaluate. retinanet : The retinanet to evaluate. iou_threshold : The threshold used to consider when a detection is positive or negative. score_threshold : The score confidence threshold to use for detections. max_detections : The maximum number of detections to use per image. save_path : The path to save images with visualized detections to. # Returns A dict mapping class names to mAP scores. """ # gather all detections and annotations all_detections = _get_detections(generator, retinanet, score_threshold=score_threshold, max_detections=max_detections, save_path=save_path) all_annotations = _get_annotations(generator) average_precisions = &#123;&#125; for label in range(generator.num_classes()): false_positives = np.zeros((0,)) true_positives = np.zeros((0,)) scores = np.zeros((0,)) num_annotations = 0.0 for i in range(len(generator)): detections = all_detections[i][label] annotations = all_annotations[i][label] num_annotations += annotations.shape[0] detected_annotations = [] for d in detections: scores = np.append(scores, d[4]) if annotations.shape[0] == 0: # 表示当前图片没有标注，因此你的标注结果都是错误的 false_positives = np.append(false_positives, 1) true_positives = np.append(true_positives, 0) continue overlaps = compute_overlap(np.expand_dims(d, axis=0), annotations) assigned_annotation = np.argmax(overlaps, axis=1) # 对每个框找出覆盖最多的一个标注,返回标注所在的下标 max_overlap = overlaps[0, assigned_annotation] if max_overlap &gt;= iou_threshold and assigned_annotation not in detected_annotations: false_positives = np.append(false_positives, 0) true_positives = np.append(true_positives, 1) detected_annotations.append(assigned_annotation) else: false_positives = np.append(false_positives, 1) true_positives = np.append(true_positives, 0) # no annotations -&gt; AP for this class is 0 (is this correct?) if num_annotations == 0: average_precisions[label] = 0, 0 continue # sort by score indices = np.argsort(-scores) false_positives = false_positives[indices] true_positives = true_positives[indices] # compute false positives and true positives # 得到一个累加的数组的结果 false_positives = np.cumsum(false_positives) true_positives = np.cumsum(true_positives) # compute recall and precision recall = true_positives / num_annotations precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps) # compute average precision average_precision = _compute_ap(recall, precision) average_precisions[label] = average_precision, num_annotations print('\nmAP:') for label in range(generator.num_classes()): label_name = generator.label_to_name(label) print('&#123;&#125;: &#123;&#125;'.format(label_name, average_precisions[label][0])) return average_precisions 几个函数： 123np.argsort(scores) # 根据从小到大返回元素的下标，小的在前np.argmax(overlaps,axis = 1) # 找出每一列的最大值，返回他的下标np.cumsum(nums) # 返回一个数组，数组中内容从头开始累加到当前位置 总结经过上面几个流程我们大致梳理了一下一个网络的搭建，数据的传递，loss的计算，以及最后的验证的过程。 总结一下： 构造dataloader，在这里头完成数据的读取，增强等工作 完成网络的搭建 完成网络的训练 完成验证集的测试工作]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[normalization]]></title>
    <url>%2F2019%2F07%2F24%2Fnormalization%2F</url>
    <content type="text"><![CDATA[Normalization 正则化在wikipedia上的解释是，使得某个东西更加正规和正常化的一个过程。深度学习中，正则化使用十分广泛，通常网络通过修改loss，添加参数的正则项，对参数的分布进行控制；或是在数据预处理阶段，对数据进行正则化操作。正则化操作通常指的是将数据大小范围缩放到[0,1]之间。 对数据集的正则化操作 Normalization is useful when your data has varying scales and the algorithm you are using does not make assumptions about the distribution of your data, such as k-nearest neighbors and artificial neural networks. 正则化使用场景是数据特征范围差异大，且数据的分布未知。 对于一般的数据集来说，我们不需要对其进行正则化操作。但如果数据集不同特征的数据范围相差过大时，我们需要对其进行正则化操作。因为数据范围大的数据，其波动对精度的影响很大，而数据范围小的特征，数据波动的影响不会有这么大，这样造成了结果精度无法提升。因此需要对数据进行正则化操作。使得数据局限在一个固定的范围内。 正则项我们知道，当一个网络与数据过度拟合，这个网络能够很好的反应训练数据，但是它的泛化性能也会大大下降。为了避免这种过拟合现象，做法通常有： 削减特征的数量（难以确定哪些特征是需要丢弃的） 减少特征的参数，控制参数的分布，即使用正则项方法 正则项的目的是为了对参数进行控制，包括： 实现参数的稀疏化，即某些参数为0。参数的稀疏化能够自动对数据的特征进行筛选，过滤掉一些不需要的特征，同时起到简化模型的作用，避免过拟合。 最小化正则项能够尽量保持参数较小，参数小的好处在于计算方便，且在网络求导的过程中，产生的导数通常比较小，结果比较稳定。 范数 （norm）在线性代数领域中，范数是一个函数，它为向量空间中的每个向量分配严格正长度或大小 。 L0 范数：指向量空间中非0向量的个数 无穷范数：指所有向量中欧式距离的最大值作为无穷范数 参数正则项L0正则项：模型参数中，不为0的参数的个数 ​ L0正则化通过最小化不为0的参数的个数，以达到参数稀疏化的目的，使得模型自动选择特征。在使用时，由于L0正则项是一个NP hard问题，L1是L0的最优凸优化，因此通常用L1来代替L0。 L1正则项：各个模型参数的绝对值之和 ​ 最小化L1正则项能够将模型的参数变小，沿着0的方向靠近，降低网络的模型复杂度。添加L1正则项后方程如下：$$L = L_0 + \frac{\lambda}{n}\sum_{w}|W|$$L2 正则项：各个参数的平方和再开根号。 ​ 最小化L2正则项可以使得参数变小接近于0，当参数不会变成0（可以看下面的图来理解），因此L2将选择更多的特征，权重比较小，避免过拟合。方程如下：$$C=C_{0}+\frac{\lambda}{2 n} \sum_{w} w^{2}$$lasso回归与岭参数 L1正则化又称为losso回归，将L1正则项作为loss的惩罚函数。L2正则项又称为岭参数。同样可以将L2正则项作为公式的约束项。可以画图如下,其中等值线为原始的Loss，L1为正方形（绝对值），L2为一个圈（平方根）。可以看出来，图中的交点满足条件的点，因此可以看出L1正则项可以得到更多的稀疏解。 标准化操作（standardization） Standardization is useful when your data has varying scales and the algorithm you are using does make assumptions about your data having a Gaussian distribution, such as linear regression, logistic regression and linear discriminant analysis. 标准化使用场景是数据特征范围差异大，假设数据服从高斯分布。 将数据标准化是指将数据rescale，使得数据的 $mean = 0,\sigma = 1$。数据的标准化操作如下：$$z=\frac{x-\mu}{\sigma}$$标准化操作对于很多机器学习的算法，在网络训练上有着很重要的作用。例如对于梯度下降法来说，处于中心（mean = 0）范围的数据，中心权重的参数更新将会加快。对于一些loss而言（MSE），利用欧式距离作为网络优化的目标，因此标准化操作是很重要的。 Batch Normalization（批量标准化）其步骤如下，对一个batch中的数据进行标准化后，并学习$r,\beta$ 两个参数，对得到标准化后的值进行一个偏移，得到最终的结果： 当进来一个batch的时候，具体的做法是，在数据输入到下一层神经元激活函数之前，计算整个batch的mean，variance，偏移后最终得到下一层的输入。 为什么要加入Batch Normalization层？ 由于深层网络的输入，经过多层神经网络层的作用后发生偏移（ReLu激活函数输出均大于0，因此整体输出的mean将往大于0的方向偏移）。导致网络训练难以收敛，落入梯度饱和区导致梯度消失等问题。BN层重新通过将数据拉回N(0,1)的正态分布上，是的输入值落入激活函数梯度敏感的区域，避免梯度消失，加速网络的训练。（输入变小也有助于降低模型计算复杂度）。 但是仅仅做到这一步还不行，由于我们引入非线性的激活函数，使得网络能够学到一些非线性的性质。我们通过BN将输出拉回到N(0,1)分布上，削弱了激活函数的非线性部分的作用。因此BN通过学习两个参数$\gamma, \beta$ 来对输出做一个scale和shit操作。恢复学习到的非线性部分知识。最终得到的$y_i$ 在正态分布和非线性性质中做了一个trade off。 Batch Normalization的作用 batch normalization极大的提升了网络训练的速度 每次BN都将网络的输出控制在一个范围内，近似于符合正态分布，能够起到正则项的作用 对参数的初始化要求降低，调参变得简单 layer normalization layer normalization 正则化的方向是沿着feature的方向对CHW归一化，batch normalization 正则化的方向是以sample为单位，对NHW做归一化。]]></content>
      <categories>
        <category>super resolution</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[image upsample-downsample method]]></title>
    <url>%2F2019%2F07%2F23%2Fimage-upsample-downsample-method%2F</url>
    <content type="text"><![CDATA[图像尺度的放大，缩小是图形学中一个十分常见的问题。然而这个过程并不是无损的，缩放的过程是一个非线性的过程，因此存在许多算法在效率，平滑度，清晰度和速度上进行一些权衡（trade-off）。在图形的缩放过程中，存在插值，采样等一些关键的步骤，下面对一些在图像缩放过程中使用的算法进行简要的介绍，这些算法均有其优缺点。 参考资料：https://clouard.users.greyc.fr/Pantheon/experiments/rescaling/index-en.html#bicubic 问题定义在处理图片的缩放问题时，需要解决的问题是： 在放大过程中，新增的像素的颜色如何确定。 在缩小过程中，哪些像素需要被保留。 图形缩放下面用一个1D的问题举例,如下图，y轴表示灰度图的灰度值: 现在对这个图形进行进行放大，有两种做法： 使用最近邻方法，用左边的像素填补这个位置的像素 使用线性插值的方法，利用前后位置的像素值生成该位置上的像素 将这个问题一般化，我们通过引入卷积来完成这个操作。例如对于最近邻方法，可以使用[1,1,0]卷积核，对于插值法，可以使用[0.5,1,0.5]卷积核。 与上述思路相同，我们将卷积核推广到2D的情况，同时在x和y方向上做卷积，各个像素的取值由卷积权重决定。 Nearest Neighbor Resampling（最近邻采样） 用这种方式得到的图像块状比较明显，但是这种方法执行效率最快。 Bilinear Resampling (B-spline order 1) （双线性插值） 上诉公式是沿着x方向的线性差值的值，对于y方向同样用这种方式进行插值。 Bicubic Resampling （双三次插值） 该方法需要选取的最近的16个像素点作为计算目标图像B(X,Y)处像素值的参数。每个位置的权重与像素值，以及像素的变化率有关。当a取-0.5是，bicubic函数有以下的形状： 该算法在各中图像的缩放过程中使用的最多。其中心点像素计算公式如下：$$\sum_{i=0}^{3} \sum_{j=0}^{3} a_{i j} x^{i} y^{j}$$其中参数a需要根据临近的四个点的像素值，偏导数等等来计算。具体的计算过程可以看wiki上的解释。 最后在处理具体问题时，我们知道一张图片在显示屏上是以点阵的方式排列的。当我们要放大，或者缩小时，例如用双三次插值时，对于每个像素点，无论是放大还是缩小，我们总能找到最邻近的16个位置，可以很方便的对图片进行缩放。此外，用卷积的方式进行求解，能够并行对图片进行处理，提高图片的处理效率。]]></content>
      <categories>
        <category>super resolution</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Deep Learning for image Super-resolution: a Survey]]></title>
    <url>%2F2019%2F07%2F23%2FDeep-Learning-for-image-Super-resolution-a-Survey%2F</url>
    <content type="text"><![CDATA[本篇论文是2019年2月份，发表在arxiv上的篇关于超分辨率的一篇综述。这篇文章系统且全面的介绍了一些基于深度学习的超分辨率方法。其中包括： 超分辨率问题的定义 (problem setting) benchmark datasets 性能评价指标 (performance metrics) 基于深度学习的超分辨率方法 特定领域的超分辨率应用 (domain-specific application) 此外作者对比每个超分辨率方法，指出了网络的优点以及局限。最后对该领域的一些开放性问题(open issuse) 以及挑战提出了未来研究的方向。 超分辨率问题的定义（problem setting）图像的超分辨率要解决的问题是：从一张低分辨率（LR）的图像中，恢复出一张高分辨率（HR）的图像。 通常来说，我们通过下面的方式得到低分辨率的图像：$$I_{x}=\mathcal{D}\left(I_{y} ; \delta\right)$$$I_x$ 表示低分辨率图像，$I_y$ 表示高分辨率图像，$D()$ 表示下采样的映射函数，$\delta$ 表示映射函数的参数。图片清晰度不够的原因可能有很多种，例如聚焦，图片压缩，传感器噪声等问题。一些学者提出了下面的模型来模拟这种失真的映射。$$\mathcal{D}\left(I_{y} ; \delta\right)=\left(I_{y} \otimes \kappa\right) \downarrow_{s}+n_{\zeta},{\kappa, s, \zeta} \subset \delta$$$I_{y} \otimes \kappa$ 表示HR图片与模糊核（blur kernel）k的卷积操作，下箭头表示下采样，$n_{\zeta}$ 表示方差为$\zeta$ 的白高斯噪声。 目前大多数的数据库，产生LR图像的方法是直接对SR图像做一些下采样(双三次插值算法(bicubic interpolation))，同时对图片作抗锯齿（anti aliasing），去模糊等操作(blur) 。关于下采样，上采样的一些方法可以看 这个课件，或这里。 关于超分问题，我们更关注的是如何由低分辨率的图片得到高分辨率的图片，即：$$\hat{I}_{y}=\mathcal{F}\left(I_{x} ; \theta\right)$$其中$\mathcal{F}()$ 为超分模型，由低分辨率图片生成高分辨率的图片。 对于一个典型的超分辨率问题来说，我们需要从一个LR图像中恢复出它的HR版本。模型的目标是最小化我们恢复出来的图片与原始HR图片的差距，目标方程如下：$$\hat{\theta}=\underset{\theta}{\arg \min } \mathcal{L}\left(\hat{I}_{y}, I_{y}\right)+\lambda \Phi(\theta)$$其中$\mathcal{L}\left(\hat{I}_{y}, I_{y}\right)$ 为生成的HR图像与原始图像的Loss，公式尾项是一个正则项。目前使用较多的loss为像素级别的MSE loss，同时一些组合型的loss也经常被使用。引入正则项的目的是控制参数的变化，使得网络更容易收敛。正则项可以看这里。 Benchmark dataset在一个典型的超分辨率的文章中，通常需要对一些公开数据集上进行试验，在这些公开数据集上的效果指标作为这个算法性能的评价标准。主要使用的数据集有： Image Quality Assessment图片质量的评价是一个与感知，视觉相关的问题。通常存在客观和主观的两类方法。其中客观方法根据指标计算模型直接计算得出，如MSE。主观方法则与人们的感知更为接近。下面介绍一下常用的评价方法。 Peak Signal-to-Noise Ratio(峰值信噪比)峰值信号比是一种图像的客观评价标准。他用最大值信号与背景噪声信号（重建与原始信号的差）的比值作为评价标准：$$\begin{aligned}\operatorname{MSE} &amp;=\frac{1}{N} \sum_{i=1}^{N}(I(i)-\hat{I}(i))^{2} \\\operatorname{PSNR} &amp;=10 \cdot \log _{10}\left(\frac{L^{2}}{\mathrm{MSE}}\right) \\\end{aligned}$$其中L为图像点颜色的最大数值，若采样点采样8位表示，那么L = 255。该指标更加注重像素点之间的误差。典型的PSNR值在20到40之间。指标越高越好。 但是由于PSNR指标更多的放映相同位置上像素值的差异，而未考虑到人眼的视觉感知，因此作为质量评价指标是存在缺陷的。但这个指标仍是目前使用最多的一个指标。 人眼视觉特征 对空间频率较低的对比差异敏感度高 人眼对亮度对比差异的敏感度较色度高 人眼对一个区域的感知结果会影响到周围邻近区域 SSIM（Structural Similarity 结构相似性）SSIM分别从亮度，对比度，结构三个方面度量图片的相似性。 首先计算图片的mean和variance：$$\begin{aligned}\mu_{I} &amp;=\frac{1}{N} \sum_{i=1}^{N} I(i) \\\sigma_{I} &amp;=\left(\frac{1}{N-1} \sum_{i=1}^{N}\left(I(i)-\mu_{I}\right)^{2}\right)^{\frac{1}{2}} \\\end{aligned}$$亮度（luminance）指标（$\hat{I}$ 指生成的图片）:$$\mathcal{C}_{l}(I, \hat{I})=\frac{2 \mu_{I} \mu_{\hat{I}}+C_{1}}{\mu_{I}^{2}+\mu_{\hat{I}}^{2}+C_{1}}$$对比度（contrast）指标：$$\mathcal{C}_{c}(I, \hat{I})=\frac{2 \sigma_{I} \sigma_{\hat{I}}+C_{2}}{\sigma_{I}^{2}+\sigma_{\hat{I}}^{2}+C_{2}}$$结构对比度（structure comparison）指标：$$\begin{aligned}\sigma_{I \hat{I}} &amp;=\frac{1}{N-1} \sum_{i=1}^{N}\left(I(i)-\mu_{I}\right)\left(\hat{I}(i)-\mu_{\hat{I}}\right) \\\mathcal{C}_{s}(I, \hat{I}) &amp;=\frac{\sigma_{I \hat{I}}+C_{3}}{\sigma_{I} \sigma_{\hat{I}}+C_{3}} \\\end{aligned}$$其中$C_1 = (K_1L)^2$,$C_2 = (K_2L)^2$,$C_3 = C_2 / 2$。 SSIM的指标有三面三个指标组合而成：$$\operatorname{SSIM}(I, \hat{I})=\left[\mathcal{C}_{l}(I, \hat{I})\right]^{\alpha}\left[\mathcal{C}_{c}(I, \hat{I})\right]^{\beta}\left[\mathcal{C}_{s}(I, \hat{I})\right]^{\gamma}$$通常使用下面这个形式：$$\operatorname{SSIM}(I, \hat{I})=\frac{\left(2 \mu_{I} \mu_{\hat{I}}+C_{1}\right)\left(\sigma_{I \hat{I}}+C_{2}\right)}{\left(\mu_{I}^{2}+\mu_{\overline{I}}^{2}+C_{1}\right)\left(\sigma_{I}^{2}+\sigma_{\tilde{I}}^{2}+C_{2}\right)}$$一般的，$k_1 = 0.01,k_2 = 0.03, L =255$。 此外还有一些主观的评价方法（mean opinion score），利用志愿者对生成图片的质量进行五个等级的评价，来确定图片的质量。 对于图片的颜色空间来说，常用的颜色空间有RGB空间与YCbCr。 基于有监督的超分辨率方法超分辨率框架分类超分辨率框架总结下来有以下四种： Pre-upsampling Super-resolution Post-upsampling Super-resolution Progressive Upsampling Super-resolution Iterative Up-and-down Sampling Super-resolution 如下图： Pre-upsampling Super-resolution 该方法在将图片送入网络前先用传统方法进行图片的放大（bicubic interpolation上采样），将图片放大到输出的要求大小，然后送入CNN网络中，学习一个端到端的从LR到HR的映射。 该方法的优点在于神经网络仅需要学习一张粗糙的（传统方法放大的）图片到HR图片的映射，大大降低了网络学习的难度；同时这种结构可以任意控制图片放大倍数。该方法框架也成为了一种较为主流的框架。 该方法的缺点在于：传统的图片放大算法中通常需要包含去噪，去模糊等操作，需要花费很大的时间以及空间。 Post-upsampling Super-resolution 该方法将LR到HR的整个过程作为网络学习的目标，上采样层在网络的末端，这种设计可以极大发挥网络的潜力，同时能够显著降低网络训练时消耗的时间与空间。在train和inference阶段速度带来了很大的提升。 缺点：仅通过一个upsample层来放大图片，使得网络学习的难度大大提升；由于upsample层的放大尺度是固定的，如果更换一个倍数，就要更换一个训练模型。 Progressive Upsampling Super-resolution 渐进式的上采样可以解决上诉post结构的问题（例如LapSRN网络 laplacian pyramid SR network）。该结构采用许多CNN的级联结构，每个阶段进行一个上采样重构HR，生成放大2倍，4倍，8倍等结果。 该模型的缺点是结构复杂，训练难度大等等。 Iterative Up-and-down Sampling Super-resolution 该结构反复的放大，缩小图片，试图学习到一种后映射（back projection）的关系，该模型可以很好的学习到LR与HR之间的映射关系。基于该框架的网络DBPN也获得了NTIRE 2018的冠军。尽管这种up-down的结构设计标准还未确定，DBPN网络中存在着大量的复杂的结构设计以及繁重的人工设计过程，但是这种结构有很大的研究潜力。还需要进一步探索。 传统插值上采样算法 最近邻插值 线性插值 双三次插值 详见这里 事实上，所有的差值算法完全通过图片自身的内容来实现超分辨率，因此他们并不能提供多于图片的信息，此外这些差值算法还引入了一些边界效应，例如计算复杂度，噪声，模糊等等。 基于学习的上采样方法转置卷积层 （transposed/ deconvolution layer） 转置卷积层的作用与正常卷积层的操作是相反的。转置卷积通过在像素间插入0来扩大图片的分辨率。下面是转置卷积层的工作原理： 首先对一张图片，每个像素点之间添加一个0值，然后用一个3 X 3 的卷积核，padding= 1 ，stride = 1对它进行卷积操作，最终得到一个大小为原先两倍的图片。 这种做法能够使得网络实现端到端的映射，但是他的缺点是，产生的图片会产生一些不等的重叠，从坐标轴上看，容易形成棋盘的割裂感，一定程度上伤害了SR的性能。 子像素卷积（sub-pixel layer） 子像素卷积在超分辨率领域使用十分广泛，用于扩大图片的像素。他的工作原理是执行一次卷积之后，产生一个多通道的feature map。然后将这些多通道的像素reshape到一个二维平面上。原理图如下： 例如要将原始的feature map大小变大s倍，那么卷积核的channel数达到$s^2$。例如输入图片的大小为$w*h*c$，经过卷积操作后变为$w*h*s^2 c$ ，然后进过reshape成$sh*sw*c$，即完成了放大的操作。在原图的基础上放大了s倍。 子像素的上采样方法有一个重要的优点在于他有更大的感受野，能够提供更多的图片信息。但是感受野的分布是不对齐的，同时卷积层使用重复的感受野会导致不同卷积边界的不真实感。 网络的设计超分辨率发展到今天，需有有效的网络结构得到了验证，例如残差学习，密集连接块。这些结构结合上面提到的四种框架能够组合出各种有效的网络结构。 残差学习 （residual Learning） 残差学习最早由何凯明的resNet提出，在超分辨率领域残差学习主要有以下两种结构： 全局残差学习 global residual learning 由于在SR问题中，网络通常是端到端的，输入的图片与输出的图片有着很大的相关性。因此有些研究者通过直接学习输入与输出之间的残差，在输入与输出之间连接一条high way达到这个目的。因此网络仅仅需要学习输入与输出之间的残差部分（图片中的高频部分数据）。由于残差网路中绝大多数的区域值接近零，因此在网络的学习过程中能够大大降低运算量，尤其在pre-upsample框架中。 局部残差学习（local residual learning） 局部残差学习与resNet中的残差模块类似，在缓解网络退化，改善网络的学习能力上具有很好的效果。 递归学习（Recursive Learning） 为了不引入过多的参数同时实现更大的感受野并学习更高级别的特征，递归学习（其是指以递归方式多次应用相同模块）被引入到超分辨率领域中。很多工作中引入卷积结构、残差结构作为递归块，均在performance上有比较好的表现。 很多学者提出了很多与递归块结合的网络结构，例如将一个大的缩放因子分解成很多子问题，然后用力对结构解决这些子问题；将image upsample作为递归块等等。由于递归块同样面临着梯度的消失和梯度爆炸的问题，因此很多残差学习，多监督学习通常也会被引入到地柜结构中，来解决这些问题。 多路径学习（multi-path learning）多路径学习将特征传入模型的不同分支中，每个分支有着不同的结构，以此来提高模型的超分能力。 全局多路径学习 Global Multi-path Learning 全局的多路径学习通过利用不同路径来学习图片中的不同特征，例如用一些分支学习一些亚频特征；学习visible特征；学习全局结构；学习低频或高频部分；用于upsample图片等等 这种思路能够提升网络的特征提取能力。 局部的多路径学习（local multi-path learning） 受到inception结构的影响，引入一个block，这个block中使用不同的路径，进行不同尺度的特征提取。如下图： 分别对feature map应用一个3X3和5X5大小的核，在不同的尺度上对特征进行提取。通过这种方式可以在不同尺度上对特征进行提取，能够有效的提升网络的性能。 特定尺度的路径学习（scale-specific multi-path learning） 由于多分辨率问题对图片的方法尺度不同，网络需要重新训练，但是网络结构都是相同的。这种策略就是保留网络的主干部分（结构以及参数），在网络的头部和尾部添加一个与尺度相关的预处理路径以及一个upsample路径，每次对于特定的分辨率需求，选择相关的路径，而网络的特征提取以及中间部分都得到了保留。 密集连接块（Dense Connections）只从密集连接块被提出之后，这种结构就广泛的应用在超分辨率领域，结构如下： 该种结构将当前层之前的feature map都作为这一层的输入，能够有效的避免梯度消失，增强信号的传递、特征的复用等。此外还有很多结构是在块级上做密集的连接，该结构证明在超分辨率领域中同样有效。 通道注意力机制（channel attention）通道注意力机制目的是给不同的channel赋予不同的权重，不同的channel在超分辨率问题上的作用是不同的，作者使用“压缩激发模块（squeeze-and-excitation）”对不同通道进行权重的赋值。 作者通过一个全局pooling将image的size变成1 X 1 X C，然后通过两个卷积层，得到每一个channel的权重。然后对feature map重新赋值，得到赋予权重的feature map。 先进的卷积层（advanced convolution）空洞卷积 dilated convolution 空洞卷积即在原始的卷积的基础上加上空洞，目的是为了增加图片的感受野。 将这种卷积应用在超分辨率问题上也能够使得模型性能得到提升。 分组卷积（group convolution） 分组卷积的概念是对feature map进行分组（channel维度上的划分），按童谣的比例划分卷积核，然后将每个分组再进行卷积，最终将卷积结果组合成一个feature输出。这种卷积的方式大大减少了参数的计算量，在性能上仅仅下降了一点。 像素递归学习 pixel recurisive大多数的SR方法在处理图像时像素之间是独立的，无法得到像素间的相关性，因此一些学者提出pixel by pixel的生成器，通过两个网络，分别学习图像的纹理结构信息以及像素间的序列依赖关系来生成HR图像。这种方法在某些方面有一个比较好的效果，但是训练过程十分的困难，计算量比较大。 金字塔pooling引入金字塔模型能够有效的利用图片全局以及局部的特征，在EDSR-PP网络中使用金字塔模型能够有效的提升网络的精度。 小波变换小波变换可以很方便的的将图片的信号分解成高频的纹理细节和低频的拓扑结构。将小波变换应用在超分辨率问题上，从低分辨率的图片中提取出低频信息作为输入，输出高分辨率的高频信息。 学习策略Loss Functions​ 在超分辨率领域，损失函数用来衡量生成的HR图片与原始的HR图片之间的差距，同时指导模型的优化。下面简要介绍一下存在的一些损失函数的形式。其中$\hat{I}$ 表示原始超分辨图像，$I$ 表示生成的超分辨率图像。 像素级别的loss （pixel loss） 对比GT与生成的图片在像素级别上的L1以及L2 loss：$$\begin{aligned}\mathcal{L}_{\text {pixel_L1 }}(\hat{I}, I) &amp;=\frac{1}{h w c} \sum_{i, j, k}\left|\hat{I}_{i, j, k}-I_{i, j, k}\right| \\\mathcal{L}_{\text {pixel_L2}}(\hat{I}, I) &amp;=\frac{1}{h w c} \sum_{i, j, k}\left(\hat{I}_{i, j, k}-I_{i, j, k}\right)^{2} \\\end{aligned}$$L2 loss 相比较于L1 loss 来说，更加的惩罚比较大的误差，而对一些小的误差的容忍度更大。L1 loss在对性能和最终的收敛上比L2更好。对于指标PSNR来说，最小化pixel loss就可以达到最大化PSNR的目的。但是pixel loss没有将图片的质量考虑在内，因此生成的图片过于平滑，失去了高频的细节信息。 满意度损失（content loss） 基于感知的满意度损失，这个loss是一个L2 loss。他的不同点在于，我们将GT与生成的图片，分别输入一个欲训练好的分类网络中，取其高层特征（第$l$ 层）进行pixel wise上的loss计算。$$\mathcal{L}_{\text {content }}(\hat{I}, I ; \phi, l)=\frac{1}{h_{l} w_{l} c_{l}} \sqrt{\sum_{i, j, k}\left(\phi_{i, j, k}^{(l)}(\hat{I})-\phi_{i, j, k}^{(l)}(I)\right)^{2}}$$其中h,w,h是抽取出来的特征层的大小。 这个loss更加强调图片在生成上的相似性，最常用的分类网络是VGG，resNet。 纹理损失（Texture Loss） 一些文章认为图片的纹理由特征不同通道的相关性组成，定义为下面Gram matrix：$$G_{i j}^{(l)}(I)=\operatorname{vec}\left(\phi_{i}^{(l)}(I)\right) \cdot \operatorname{vec}\left(\phi_{j}^{(l)}(I)\right)$$上式中表示两个不同通道的向量的点乘结果。即第 $l$ 层特征向量的i通道和j通道的点乘结果。纹理损失依旧是L2损失，输入是生成图片和GT之间的纹理表示。$$\mathcal{L}_{\text {texture }}(\hat{I}, I ; \phi, l)=\frac{1}{c_{l}^{2}} \sqrt{\sum_{i, j}\left(G_{i, j}^{(l)}(\hat{I})-G_{i, j}^{(l)}(I)\right)^{2}}$$通过这种损失可以很好的得到较为真实的图片。但是仍然有一个难以解决的问题是，用于计算纹理损失的图片patch（方块，补丁）大小的确定依旧要根据经验来确定，太大或太小的patch使得生成的纹理不够真实。 对抗损失（adversarial loss） 我们使用一个SR模型作为生成器，另外我们需要定义一个判别器，下面的判别器D使用交叉熵来表示。生成器希望生成的样本判别器无法辨认，判别器希望能够鉴别出生成器生成的样本是假的。$$\begin{aligned}\mathcal{L}_{\text {gan_ce_g}}(\hat{I} ; D) &amp;=-\log D(\hat{I}) \ \mathcal{L}_{\text {gan_ce_d }\left(\hat{I}, I_{s} ; D\right)} &amp;=-\log D\left(I_{s}\right)-\log (1-D(\hat{I})) \\\end{aligned}$$下面还有使用最小平方差最为判别器，能够得到更加真实的且高质量的结果。$$\begin{aligned}\mathcal{L}_{\text{gan_ls_g}}(\hat{I} ; D) &amp;=(D(\hat{I})-1)^{2} \ \mathcal{L}_{\text{gan_ls_d}}\left(\hat{I}, I_{s} ; D\right) &amp;=(D(\hat{I}))^{2}+\left(D\left(I_{s}\right)-1\right)^{2} \end{aligned}$$下面是使用hinge loss形式的对抗损失：$$\begin{aligned} \mathcal{L}_{\text{gan_hi_g}}(\hat{I} ; D) &amp;=-D(\hat{I}) \ \mathcal{L}_{\text{gan_hi_d}}\left(\hat{I}, I_{s} ; D\right) &amp;=\min (0, D(\hat{I})-1)+\min \left(0,-D\left(I_{s}\right)-1\right) \\\end{aligned}$$使用对抗损失很大程度上带来的感知质量的提升，虽然PSNR指数有所下降，但是MOS指数有上升，取得了一个很好的视觉效果，生成的图片更加的真实。 循环连续损失 （Cycle Consistency Loss） 改损失受到循环GAN的启发，所用的网络不仅需要从LR到SR，还需要从SR到LR，重新生成的LR需要和输入一致，因此loss 如下：$$\mathcal{L}_{\text {cycle }}\left(I^{\prime}, I\right)=\frac{1}{h w c} \sqrt{\sum_{i, j, k}\left(I_{i, j, k}^{\prime}-I_{i, j, k}\right)^{2}}$$总差异损失（total variation loss） 这个算是是为了压制在生成图像过程中生成的噪声对图像质量产生的影响。他的loss有相邻像素的差异组合成。$$\mathcal{L}_{\mathrm{TV}}(\hat{I})=\frac{1}{h w c} \sum_{i, j, k} \sqrt{\left(\hat{I}_{i, j+1, k}-\hat{I}_{i, j, k}\right)^{2}+\left(\hat{I}_{i+1, j, k}-\hat{I}_{i, j, k}\right)^{2}}$$基于先验损失（prior based loss） 对于特定的数据，可以引入一下数据所特有的先验特征。通过这种先验特征可以很快的提升网络对这类数据恢复的性能。 Batch NormalizationBN的提出是为了消除网络训练过程中内部参数的偏移问题。具体做法是对每一个bach做一个归一化操作，并且训练两个变量用于还原网络的表达能力。因此我们在训练过程中可以使用更高的学习率，以及不用太在意参数的初始化值。因此BN在SR网络中同样得到了广泛的应用。 但是有一些学者认为BN使得网络丧失了尺度信息，使得网络失去灵活度，同样有些网络中去除BN后，取得了一个很好的性能。 课程学习 Curriculum learning渐进性的课程学习方法指的是网络从一个简单的任务出发，逐渐增加问题的难度，以此来得到一个鲁棒的模型。 超分辨率问题本质上是一个病态问题（ill-posed problem），即一些干扰对结果的影响非常的大，且系统十分不稳定，难以从结果反推回输入。这些干扰包括噪声，图片的模糊度，以及超分辨的倍数等等。 课程学习可以通过渐进学习的方式来解决这些问题，对于放大倍数很大（例如8）的任务，可以利用该思想，现训练简单的情况，例如可以先放大2，4，8倍来解决这个问题，这种方式能够大大缩短网络的训练时间，提升性能。 多监督问题 （multi-supervision）多监督问题在loss 中增加一些变量，用来对某些信号进行监督，最终能够得到一个性能较好的模型。 其他有用的方法context-wise network fusion 这种方式使用多个不同结构的网络，分别进行超分辨率的训练，然后依次将这些训练结果通过卷积层组合最终的结果（SRCNN），使用这种方法能够也能够达到state of art的效果，同时效率也是可以接受的。 data augmentation 数据增强方面，常用于网络中的方法有random cropping, flipping,scaling,rotation, color jittering, 此外还有一个特殊的增强方式，random shuffle RGB,随机打乱RGB的颜色值，这种方法能够消除颜色带来的偏差。 multi-task learning 多任务学习指的是将多种任务于SR任务结合，例如语义分割网络于SR网络结合（SFT-GAN）等，将去噪声网络和SR网络结合（DNSR），这种方式能够提供数据的先验，能够更好的提升SR的效果。 network interpolation 网络的结合，将基于pxiel loss和基于感知loss的两种方法结合起来，得到一种中间状态的网络，这种网络同时在PSNR和真实感上有很好的表现。 无监督的方法在超分辨率问题上，由于很难获得真实数据的超分辨率结果，因此 通常的做法是使用一个下采样方法，从超分辨率图像中得到他的低分辨率版本你，组成一个数据对，因此监督学习更像是学习这个方法的逆方法，人们通常忽略了提前定义好的下采样方法给数据带来的副作用。对于无监督方法来说，直接使用高分辨率的图片，更加符合现实中的场景。无监督方法上，目前仍然有很多值得探索的地方。 zero-shot super-resolution这个方法训练了一个预测核函数直接针对每张图片都生成一个下采样（degradation）核方法，使用这个核方法来构造数据集，采用不同的缩放尺度得到测试数据，然后训练一个CNN网络来实现SR。由于这个模型需要为每一张图片构造一个函数，因此需要更多的时间。 weakly-supervised Super-resolution弱监督的学习方法有两个思路，第一种是不是用传统的HR-to-LR的退化函数，而是学习一个网络来实现这个过程，然后构造一个数据集，然后使用这个数据集来训练SR模型。另一种是cycle-in-cycle的方法，同时学习LR-to-HR和HR-to-LR两方面。 learning degradation 有学者提出了一个两个阶段的学习网络，提出一个GAN网络，学习HR to LR，用这个网络生成一个LR-HR配对的数据集，然后训练一个LR to SR的GAN网络使用上诉的数据集进行训练，最终结果能够显著提升数据恢复的真实性。 cycle in cycle super resolution CinCGAN网络使用了四个生成器，两个判别器。生成器分别为noise LR -&gt; clean LR -&gt; clean HR，另外两个生成器进行反方向的生成。然后生成器用于判别生成了LR和SR的真实性，这其中引入了大量的损失函数，来保证这一过程的合理性。此外，这个方法还有很多改进的地方，来降低它训练的难度。 图像的深度先验使用一个随机初始化参数的CNN，对一张输入的图像，直接恢复他的超分辨率图像，仅仅利用CNN的结构先验来解决这个问题。模型的效果比传统的双线性插值要好些，但是效果不如其他监督方法，这种方法给我门提供了一种思路，仅仅利用一些手工制作的先验对图像进行超分辨率的恢复。 领域相关的应用高光谱影像 （Hyperspectral Image Super-resolution）高光谱影像在视觉任务中有着很多的用途，但是由于硬件的约束，收集到高质量的高光谱数据是十分的困难的，高光谱数据的分辨率因此也十分的低。因此在高光谱数据领域应用超分辨率方法是很有前景的。 基于高光谱的超分辨率工作有以下几种： W. Huang, L. Xiao, Z. Wei, H. Liu, and S. Tang, “A new pan- sharpening method with deep neural networks,” GRSL, vol. 12, 2015. G. Masi, D. Cozzolino, L. Verdoliva, and G. Scarpa, “Pansharp- ening by convolutional neural networks,” Remote Sensing, vol. 8, 2016. Y.Wei,Q.Yuan,H.Shen,andL.Zhang,“Boostingtheaccuracyof multispectral image pansharpening by learning a deep residual network,” GRSL, vol. 14, 2017. Y. Qu, H. Qi, and C. Kwan, “Unsupervised sparse dirichlet-net for hyperspectral image super-resolution,” in CVPR, 2018. 未来的研究方向网络结构设计结合图片局部和全局信息： 更大的感受野能够帮助网络获得更多图片的纹理细节。 结合图片中的高低频数据：cnn网络的浅层部分能够获取图像的颜色和边界信息，深层网络能够获取图像的语义信息。 纹理注意力机制：不同的纹理反应出来的细节特征是不同的，引入注意力机制能够增强图片的真实性。 轻量级的结构：预测一张DIV2k的图片，EDSR模型需要花费20s，这是难以接受的，因此我们需要精简网络结构。 上采样层：当前使用的上采样层均存在着不同程度的缺陷，提出一个好的上采样层，能够提升网络效能。 学习策略损失函数： 当前仍未找到一个很好的损失函数，能够兼顾感知和pixel wise Normalization：BN归一化方法十分花费时间，需要找到它的替代结构 评价指标当前的评价指标有PSNR，SSIM，MOS三种，其中PSNR容易生成过于平滑的图像，SSIM根据图片的光照，对比度，结构来评价，当时离人的感知还有一定距离，MOS与人的感知比较接近，但是统计起来十分的耗费人力及复杂。 现实场景的使用无监督学习方向上，可以学习一个degradation函数，用于数据的上采样，更符合现实数据的现状。 一些特定领域的应用方面，超分辨率可以作为整个流程的一部分。]]></content>
      <categories>
        <category>super resolution</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些提升效率的方法]]></title>
    <url>%2F2019%2F07%2F23%2F%E4%B8%80%E4%BA%9B%E6%8F%90%E5%8D%87%E6%95%88%E7%8E%87%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在word或ppt中插入公式 使用mathpix snipper工具，从截图中获取latex公式。 进入这个网站：https://www.latex4technics.com/ 输入latex公式，在右下角转化为mathml格式。 打开word，插入公式。以纯文本的格式粘贴mathml代码，word自动转化为公式。 ppt中需要从word得到的公式复制过来，不支持直接转换。 使用jupyter链接服务器jupyter有几个好处，他可以单步执行，单步调试。可以在浏览器上看执行的结果，包括图片的显示这些。当跑的代码比较简单，是测试功能的代码的时候，可以使用jupyter。 jupyter的配置：jianshu.com/p/4012f7149eb8 用mac连接远程服务器： 服务器端输入：jupyter notebook –no-browser –port=8898 本地输入：ssh-N -f -L 127.0.0.1:8898:127.0.0.1:8898 zhouwenhui@remote-machine 最后在浏览器访问：http://127.0.0.1:8898/]]></content>
      <tags>
        <tag>tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xigua-支持向量机]]></title>
    <url>%2F2019%2F07%2F21%2Fxigua-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[支持向量机主要目的在于找到 一个位于两类训练样本的正中间，该分界面对样本的局部扰动的鲁棒性最好。通过该分界面能够最大限度的对数据进行分类。]]></content>
      <categories>
        <category>xigua</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[xigua-神经网络]]></title>
    <url>%2F2019%2F07%2F20%2Fxigua-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[“神经网络是具有适应性的简单单元组成的广泛，并行互连的网络，能够模拟生物神经系统对真实世界物体所作出的交互反应。” 神经网络的发展1943年，神经网络模型最早是由心理学家和数理逻辑学家在提出的MP模型，它揭示了“大脑活动是靠脑细胞的组合连接实现的。” 1949年，心理学家Hebb提出 “脑细胞间某种通路在参与某种活动时被加强。” 用现在的观点来看这一说法，即我们可以通过调整网络参数（权重），来改善网络的性能。 1956年，达特茅斯会议上，明斯基，麦卡锡，西蒙等人首次提供人工智能的概念，使得人工智能在成为计算机科学的一个分支。 1962年，感知机模型正式提出，它具有输入层，输出层和中间层。 1969年，明斯基的《percetion》一书出版，指出感知机不能解决高阶谓词问题，人工智能发展陷入低谷。 1982年，hopfield向美国科学院提出了关于神经网络的报告，引起美国军方的注意，引起了神经网络的第二次高潮。在这次高潮中，hopfield网络，boltzmann机以及BP算法得到提出。 2006年之后，hiton提出深度学习，引起了神经网络的第三次浪潮。 神经网络模型1943年提出的“M-P神经元模型”如下： 输入乘以权重之后，减去一个偏置$\theta$ ，然后通过激活函数，得到这个神经元的输出。在早期，使用的激活函数为sigmoid函数： $$\sigma(z)=\frac{1}{1+\mathrm{e}^{-z}}$$sigmoid函数如图： sigmoid 的导数形式如下：$$\sigma(z)’=\frac{\mathrm{e}^{-z}}{(1+\mathrm{e}^{-z})^{2}} = \frac{1+\mathrm{e}^{-z}-1}{(1+\mathrm{e}^{-z})^{2}} = \sigma(z)*(1 - \sigma(z))$$由于sigmoid的导数函数形式简单，取值变化范围在(0,1)之间。神经网络就是有无数个像这样的神经元结构组合而成的一个包含许多参数的数学模型。 激励函数激励函数的作用是将无限域的变换指定到有限范围内进行输出。同时增加网络的非线性建模能力，复杂程度。 Bengio对激活函数有如下的定义： 激活函数是映射h：R-&gt;R，且几乎处处可导。 具有软饱和函数的性质：$\lim_{s-&gt;\inf} f’(s) = 0$ ，软饱和性质只当x趋向去正无穷或负无穷的时候，函数的导数为0。硬饱和指存在一个区域c，当x接近c边缘时，导数值变为0. ReLu激活函数：该激活函数能够在一定程度上克服梯度消失的问题。 relu在$x&lt;0$部分为硬饱和，导数为0。在$x&gt;0$部分，导数为1，能够保持网络梯度不衰减，缓解梯度消失问题。 当部分输入落入饱和区时，将导致网络的稀疏性，同时导致对应的权重无法更新（神经元死亡）。relu的输出同时具有偏移现象，即输出的值均值大于0，偏移与神经元死亡是其主要弊病。 误差反向传播BP算法沿着负梯度方向减小误差，利用链式法则对每一个梯度求一个$\Delta$ 值，用于更新网络的参数。当网络陷入一个极小点时，在该点处不存在负梯度方向，因此参数无法进行更新。此时网络可能陷入局部极小点或全局最小点。 如果网络陷入局部极小点，我们希望在网络的训练过程中，函数能够跳出该极小点。可以使用的方法有 模拟退火法，即在每一步迭代，以一定的概率接受次优解，可以一定程度上避免陷入局部极小。 随机梯度下降法，每次选择部分数据进行梯度的计算，因此该梯度方向不一定是全局的下降方向，随着函数的迭代，网络误差可以慢慢降到一个可以接受的水平。 深度学习深度学习模型是深层次的神经网络，通过增加网络的层次，提高网络的容量，使得它能学到更加复杂的问题。但是多层神经网络难以用传统的BP算法进行训练，因此后来的学者们也提出了许多其他的算法。 神经网络的实现pytorch中的torch.nn包pytorch中关于网络结构的函数在torch.nn这个包里头，此外torch.nn.functional中也有于torch.nn对应的相关函数。他们的区别在于torch.nn中的参数是可训练的，可变的。torch.nn.functional中的函数是不可训练的，进行一些数学运算，类似于tensor于Variable的区别。因此搭建网络结构的时候使用torch.nn，激活函数则使用torch.nn.functional。 贴一个解释很清楚的文章：https://blog.csdn.net/hawkcici160/article/details/80140059 pytorch中的torch.autograd包autograd.Variable是包的中央类，包含一个张量，并支持几乎所有定义的操作，在完成计算后，调用.backward()并自动计算所有梯度。可以通过.data属性访问原始张量，而将此变量的梯度累加到.grad。 Variable类中比tensor类多了几个其他的属性：data,grad_fn,grad,variable变量可以用来计算梯度。 下面这个文章有有详细介绍：https://www.jianshu.com/p/cbce2dd60120 可以用variable来定义网络的参数。]]></content>
      <categories>
        <category>xigua</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[19/7/2019 preview]]></title>
    <url>%2F2019%2F07%2F19%2F19-7-2019-preview%2F</url>
    <content type="text"><![CDATA[welcome to my blog,enter password to read. Incorrect Password! No content to display! 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]]></content>
      <tags>
        <tag>dialog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前缀树🌲:trie]]></title>
    <url>%2F2019%2F07%2F17%2F%E5%89%8D%E7%BC%80%E6%A0%91%F0%9F%8C%B2-trie%2F</url>
    <content type="text"><![CDATA[前缀树是一种存储数据的树形结构。是一种高效的检索字符串的方法，是一种多叉树的结构。它的插入与删除的效率比较高，时间复杂度为O(m). 前缀树前缀树的结构如下图所示： 前缀树的结构特点为： 根节点不包含字符，除根结点外，其他节点只包含一个字符 从根节点出发叶子结点，组成一个完整的字符串 每个节点包含的字符均不相同 前缀树的实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class Trie(object): def __init__(self): """ Initialize your data structure here. """ self.res = &#123;&#125; def insert(self, word): """ Inserts a word into the trie. :type word: str :rtype: None """ a = self.res for i in word: if i not in a: a[i] = &#123;&#125; a = a[i] a['end'] = &#123;&#125; def search(self, word): """ Returns if the word is in the trie. :type word: str :rtype: bool """ a = self.res for i in word: if i not in a: return False a = a[i] if 'end' in a: return True else: return False def startsWith(self, prefix): """ Returns if there is any word in the trie that starts with the given prefix. :type prefix: str :rtype: bool """ a = self.res for i in prefix: if i not in a: return False a = a[i] return True # Your Trie object will be instantiated and called as such:# obj = Trie()# obj.insert(word)# param_2 = obj.search(word)# param_3 = obj.startsWith(prefix) 上面代码用dict代替书的结构，一级一级的向下延展，前缀树由根节点往下，每一个节点的字节点就是他的key的数目，选择其中一个key，然后一级一级往下，当一个单词结束的时候，填入end作为终结符。]]></content>
      <tags>
        <tag>— leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xigua:决策树]]></title>
    <url>%2F2019%2F07%2F14%2Fxigua-%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树是一类常见的机器学习算法，决策过程是基于树的结构进行的。叶子结点对应了树的决策结果，子节点对应了属性的测试（例如西瓜的颜色）。决策树的最终目的是产生一棵泛化能力强的树。 决策树基本知识决策树子节点的生成决策树的生成方式是一个递归的过程，有根结点开始，生成子节点的情况有下面三种： 当前节点包含的样本全属于一个类别，无需划分 当前节点上所有样本的属性为空（例如缺失了身高这个数据），因此设置节点时，将该节点设置成样本中类别比例最大的那个。 当前节点所包含的样本集合为空时，采用样本的先验概率（例如身高为170的样本最多）来设置样本类 信息熵熵： entropy，希腊语原意为 内向性，即一个系统不受外部干扰时，往内部最稳定状态发展的特性。 熵同时可以作为一个系统的混乱程度的度量，即根据热力学第二定律，一个系统倾向于向增加混乱的程度发展，例如抛一枚硬币，最终的统计结果是正反面都是0.5的概率，对于预测来说，预测正面或者反面的不确定性都是最大的。 信息熵： 信息熵是指接受数据中包含的信息量的平均值，是一种不确定性的度量，越随机的信源，熵越大。熵定义为概率分布的对数的相反数。也即是说，当一个事件发生的可能性越小，当这个事件出现的时候，提供的信息就越多，不确定性越大，熵就越大。$$\mathrm{H}(X)=\mathrm{E}[\mathrm{I}(X)]=\mathrm{E}[-\ln (\mathrm{P}(X))]$$当数据取自有限样本是：$$\mathrm{H}(X)=\sum_{i} \mathrm{P}\left(x_{i}\right) \mathrm{I}\left(x_{i}\right)=-\sum_{i} \mathrm{P}\left(x_{i}\right) \log _{2} \mathrm{P}\left(x_{i}\right)$$信息增益： 信息增益指期望信息的有效减少量。例如决策树，在一个分支上，选择一个属性进行划分，得到的信息增益越大证明划分结果不确定性越小，纯度越高。$$\operatorname{Gain}(D, a)=\operatorname{Ent}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} E n t\left(D^{v}\right)$$然而信息增益趋向于选择分类更加细致的属性（分类越多，每一类的纯度也会越大），为了克服这个毛病，引入了信息增益率：$$g_{R}(D, A)=\frac{g(D, A)}{H_{A}(D)}$$其中：$$H_{A}(D)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} \log _{2} \frac{\left|D_{i}\right|}{|D|}$$信息增益率趋向于选择分类少的属性。（分类多，分母大） 基尼指数： 基尼指数比较直观，他反映了连续抽取两个样本，他们不一样的概率。因此越小表明纯度越纯。$$\operatorname{Gini}(\mathrm{p})=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}$$决策树缺失属性的处理情况： 当属性缺失的情况下，选择最优的属性划分：可以修改信息增益函数，加上无缺失样本所占比例，无缺失样本中第k类所占比例，以及无缺失样本中某个属性所占比例等修正，得到划分的标准 当选定划分属性时，该属性缺失：将这些样本按照不同的概率，加入到所有的分支中]]></content>
      <categories>
        <category>xigua</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[xigua:线性模型(linear model)]]></title>
    <url>%2F2019%2F07%2F14%2Fxigua-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-linear-model%2F</url>
    <content type="text"><![CDATA[线性模型形式简单，易于建模，具有很好的解释性质。 基本概念线性模型试图学到一个通过属性的线性组合来进行预测的函数，线性模型将要学到下面的一个函数形式：$$f(x) = \omega^T x + b$$简单的来说，即通过训练数据 (x,y) 来学的线性模型的$\omega$ 和b，即可确定模型。 线性模型 pytorch实现在实现一个线性模型之前，我们首先确定一下算法实现的pipeline。 数据准备：训练数据，label，以及测试数据的格式与读取形式。 模型的建立：模型类继承torch.nn.Module，实现其中的__init__(),forward()函数。 确定网络的criterion以及optimizer。 训练过程：每过一个step进行参数的更新。 数据准备部分在这个例子中，我们使用较为简单的数据作为输入： 1234import torchfrom torch.autograd import Variablex_data = Variable(torch.Tensor([[1.0],[2.0],[3.0]]))y_data = Variable(torch.Tensor([[2.0],[4.0],[6.0]])) Vari3able 变量于Tensor的区别在于variable变量是可以计算梯度的，在梯度反向传播的时候进行梯度的计算。 模型的建立pytorch中模型类均需要继承一个父函数：torch.nn.Module. torch.nn.module 是所有网络的基类，我们定义的网络类，都需要继承自这个类。torch.nn这个类中包含各种网络层结构，linear，conv等等。对于我们的线性模型来说，我们可以定义一个网络类，然后在init中定义linear。 1234567891011121314151617class LinearRegressionModel(torch.nn.Module): """ 定义自己的网络需要继承torch.nn.Module类，实现其中的init以及forward方法: torch.nn.Module: torch.nn是专门为神经网络设计的模块化接口。nn构建于autograd之上，可以用来定义和运行神经网络 nn.Module是nn中十分重要的类,包含网络各层的定义及forward方法。 一般把网络中具有可学习参数的层放在构造函数__init__()中 """ def __init__(self): super(LinearRegressionModel,self).__init__() """ 线性模型：torch.nn.Linear(in_features,out_features,bias=True) """ self.linear = torch.nn.Linear(1,1) # one in one out def forward(self,x): y_pred = self.linear(x) return y_pred criterion and optimizerCriterion 即为网络训练过程中，输出的预测值与groundTruth之间的差距，通常在二分类问题上可以使用MSE loss，crossentropy等等。如torch.nn.MSELoss() Optimizer 可以使用torch.optim.SGD(linear_model.parameters(),lr = 0.01)。 train网络训练过程中，设置训练的次数，首先将数据传如入网络中，然后使用criterion求出输出与groundtruth之间的偏差。在每一次参数更新时，首先将梯度置零，然后进行梯度的向后传播。 1234567for epoch in range(500): pre = linear_model(x_data) loss = criterion(pre,label) # 清空参数 optimizer.zero_grad() loss.backgrad() # 参数向后传播 optimzer.step() evaluate网络测试部分比较简单，将输入输入网络中，得到其输出。 12result = linear_model(new_var)print('result &#123;&#125;'.format(result.data[0]))]]></content>
      <categories>
        <category>xigua</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Something about keras]]></title>
    <url>%2F2019%2F05%2F24%2FSomething-about-keras%2F</url>
    <content type="text"><![CDATA[PART I : keras progress prepare data,process data create model,loss,optimizer feed data to model,set hyperparamers add some callbacks method train and save model,save the log there is a example go through the process PART II: data prepare 生成数据部分，数据基本上是存储为coco，或csv格式。将数据从硬盘中读入内存。然后构造一个生成器，目的在于批量的（batch size大小）读出数据，预处理数据。生成器简单的使用如下： 12345def generate_func(): for i in range(10): yield ifor item in generate_func(): print(item) 另一种做法是实现类的__next__()方法，每次调用一次该类，即间接调用该方法。 1234567class generate(object): def __init__(self): pass def __next__(self): ... data processing return batch_size data 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import numpy as npimport mathfrom keras.models import Sequentialfrom keras import layers# some layer in layers Dense,Dropout,Activation,Flatten# cnn layerfrom keras.layers import Convolution2D,MaxPooling2Dfrom keras.utils import np_utils # useful to transfrom datafrom keras.datasets import mnistfrom keras.callbacks import ModelCheckpoint # save modelfrom keras import callbacksfrom keras.models import load_model# prepare data(x_train,y_train),(x_test,y_test) = mnist.load_data()print(x_train.shape)#from matplotlib import pyplot as plt#plt.imshow(x_train[0])# tensorflow input(HxWxC)x_train = x_train.reshape(x_train.shape[0],28,28,1)x_test = x_test.reshape(x_test.shape[0],28,28,1)x_train = x_train.astype('float32') /255x_test = x_test.astype('float32') /255print(x_train.shape)print(y_train.shape)# convert label to one hotprint(y_train[:10])y_train = np_utils.to_categorical(y_train,10)y_test = np_utils.to_categorical(y_test,10)print(y_train[:10])### define modelmodel = Sequential()# 32,3,3 : output channel ,kernel_sizemodel.add(Convolution2D(32,3,3,activation = 'relu',input_shape=(28,28,1)))print(model.output_shape)model.add(Convolution2D(32,3,3,activation='relu'))model.add(MaxPooling2D(pool_size = (2,2)))model.add(layers.Dropout(0.25))model.add(layers.Flatten())model.add(layers.Dense(128,activation='relu'))model.add(layers.Dropout(0.5))model.add(layers.Dense(10,activation='sigmoid'))### define loss and optimizer,and then compile itmodel.compile(loss = 'categorical_crossentropy',optimizer='adam',metrics=['accuracy'])print(model.summary())#print(model.get_config())# callback，when a epoch/batch_size start/end,it will be calledcheckpointer = ModelCheckpoint(filepath='best_model.h5',verbose=1,save_best_only=True)earlyStopping = callbacks.EarlyStopping(monitor='loss',patience=20,verbose=1,mode = 'auto')reduce_lr = callbacks.ReduceLROnPlateau(monitor='loss',factor = 1/math.e,verbose=1,patience=10,min_lr=0.0001)tensorboard = callbacks.TensorBoard(log_dir='./log')# write log to csvcsv_historyger = callbacks.CSVLogger('training.history',separator=',',append='True')### feed data to the network#print('exist model')#del model#print('loading model ...')#model = load_model('./best_model.h5')history = model.fit(x_train,y_train,batch_size=32,epochs=2,verbose=1,validation_data=(x_test,y_test),callbacks = [checkpointer,earlyStopping,reduce_lr,tensorboard,csv_historyger])score = model.evaluate(x_test,y_test,verbose=0)print(score)print(history.history)print(history.epoch)print(history.history['val_loss']) 数据读取部分主要读取csv文件的image name，以及annotation。]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RetinaNet 原理记录]]></title>
    <url>%2F2019%2F05%2F16%2FRetinaNet-%E5%8E%9F%E7%90%86%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[RetinaNet作为一个one stage 的检测算法，通过对图片进行网格划分。在每个feature上选取anchor，然后对这些anchor进行边框回归以及类别的回归。 RetinaNet和大多数的one stage算法相同，直接对图片进行边框的回归，这导致了在一开始回归的时候，算法产生了大量的anticipate anchor（two stage 算法产生anchor的方式是通过region proposal的方式产生1k～2k的边框），这些anchor大部分都不包含object，即作者提到的easy negativate。 因此anchor导致了正负样本的不均衡。 正负样本不均衡主要有以下两个问题： 在网络进行训练时，一些easy negativate 样本对loss不起作用，网络收敛速度很慢。 由于存在大量的easy negativate 样本，因此在loss回归的过程，easy negativate样本将会覆盖掉真正有益的收敛方向，导致模型精度下降。 基于上面的分析，作者提出了一种对新型的loss，这种loss能够对不同的easy，hard样本进行权重的赋值。使得loss更加倾向于学习一些hard样本。 Focal Lossfocal loss 由标准的cross entropy loss 演化而来，为了简单期间，我们从二分类的cross entropy入手，开始介绍： 从上面的loss可以看出来，当一个样本为正样本时，其预测值越高，CE loss就越小。但是这个loss对所有的anchor都同等对待，当一些样本p很大或很小的时候，基本可以断定它的类别，这些样本对边框回归，类别分类的时候，起到很小的作用，因此需要被忽略，但是CE loss无法突出这一点，因此RetinaNet的focal loss就是为了解决这个问题提出来的。 当p很大时，即可以轻松判断这个anchor的类别的时候，1-p将取得一个较小的值，通过前面的参数，可以大大减小其对loss的影响。即降低了对简单样本的权重，同样的，对于难分样本来说，loss的形式可以增加其在loss中的权重。 RetinaNetRetinaNet是作者为了验证这个loss的有效性而提出的。RetinaNet主要由一个resnet作为backbone，分类部分使用了FPN，特征金字塔的形式进行特征的分类。它的网络结构如下如所示： 事实上，RetinaNet最终输出了五层feature map，在这五层feature map进行anchor的选取。 首先由Resnet 最后的三层C3，C4，C5产生P3，P4，P5，然后在C5的后面接着生成了P6，P7。 由于不方便画图，放一下keras retinanet的代码：github 12345678910111213141516171819202122232425262728293031323334def __create_pyramid_features(C3, C4, C5, feature_size=256): """ Creates the FPN layers on top of the backbone features. Args C3 : Feature stage C3 from the backbone. C4 : Feature stage C4 from the backbone. C5 : Feature stage C5 from the backbone. feature_size : The feature size to use for the resulting feature levels. Returns A list of feature levels [P3, P4, P5, P6, P7]. """ # upsample C5 to get P5 from the FPN paper P5 = keras.layers.Conv2D(feature_size, kernel_size=1, strides=1, padding='same', name='C5_reduced')(C5) P5_upsampled = layers.UpsampleLike(name='P5_upsampled')([P5, C4]) P5 = keras.layers.Conv2D(feature_size, kernel_size=3, strides=1, padding='same', name='P5')(P5) # add P5 elementwise to C4 P4 = keras.layers.Conv2D(feature_size, kernel_size=1, strides=1, padding='same', name='C4_reduced')(C4) P4 = keras.layers.Add(name='P4_merged')([P5_upsampled, P4]) P4_upsampled = layers.UpsampleLike(name='P4_upsampled')([P4, C3]) P4 = keras.layers.Conv2D(feature_size, kernel_size=3, strides=1, padding='same', name='P4')(P4) # add P4 elementwise to C3 P3 = keras.layers.Conv2D(feature_size, kernel_size=1, strides=1, padding='same', name='C3_reduced')(C3) P3 = keras.layers.Add(name='P3_merged')([P4_upsampled, P3]) P3 = keras.layers.Conv2D(feature_size, kernel_size=3, strides=1, padding='same', name='P3')(P3) # "P6 is obtained via a 3x3 stride-2 conv on C5" P6 = keras.layers.Conv2D(feature_size, kernel_size=3, strides=2, padding='same', name='P6')(C5) # "P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6" P7 = keras.layers.Activation('relu', name='C6_relu')(P6) P7 = keras.layers.Conv2D(feature_size, kernel_size=3, strides=2, padding='same', name='P7')(P7) return [P3, P4, P5, P6, P7] anchor的设置在设置anchor的时候，作者选用了一下几种设置： anchor-size = [32, 64, 128, 256, 512] 对应P3～P7 anchor—scale = [2 xx0 ，2 xx(1/3 )，2 xx (2/3)] anchor-wh = [1:2 ，1 ，2:1] 每一层anchor的大小为anchor-size 乘以 anchor-scale。然后使用三种长宽比，每一层，每一个位置得到九种大小的anchor。随后对这些位置的anchor进行边框回归以及类别的回归。 Loss 的形式以及计算稍后补充]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pytorch 张量操作]]></title>
    <url>%2F2019%2F05%2F12%2Fpytorch-%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[在编写网络，传入传出数据时，对数据的维度的操作，把握是很重要的，因此这篇文章介绍一下pytorch在数据维度的改变上的一些方法。 对于两个数组来说，融合方式有很多种，最常见的是沿着横向融合以及沿着纵向融合。在方法的参数体现上： dim = 0 ：数据沿着纵向融合。 dim = 1： 数据沿着横向融合。 torch.cat() torch.cat 方法对数据沿着不同方向进行如何，dim参数决定了融合的方向，需要注意的是需要融合方向上维度需要一致： 1234567891011121314&gt;&gt;&gt; atensor([[1., 1., 1.], [2., 2., 2.]])&gt;&gt;&gt; btensor([[3., 3., 3.], [4., 4., 4.]])&gt;&gt;&gt; torch.cat((a,b),0) # 纵向tensor([[1., 1., 1.], [2., 2., 2.], [3., 3., 3.], [4., 4., 4.]])&gt;&gt;&gt; torch.cat((a,b),1) # 横向tensor([[1., 1., 1., 3., 3., 3.], [2., 2., 2., 4., 4., 4.]]) torch.view() torch.view 在保证数组个数不变的前提下，任意改变数组的形状（需要注意的是 -1参数表明在满足其他维度大小的需求后，该维度的大小）： 12345678910111213141516&gt;&gt;&gt; atensor([[1., 1., 1.], [2., 2., 2.]])&gt;&gt;&gt; a.view(1,-1)tensor([[1., 1., 1., 2., 2., 2.]])&gt;&gt;&gt; a.view(3,-1)tensor([[1., 1.], [1., 2.], [2., 2.]])&gt;&gt;&gt; a.view(1,2,3)tensor([[[1., 1., 1.], [2., 2., 2.]]])&gt;&gt;&gt; a.view(2,1,3)tensor([[[1., 1., 1.]], [[2., 2., 2.]]]) torch.squeeze() 压缩维度，使得为1的维度塌陷，维度缩减方向为dim = 0纵向，dim=1横向： 1234567891011121314151617181920212223&gt;&gt;&gt; btensor([[[1., 1., 1.], [2., 2., 2.]]])&gt;&gt;&gt; b.shapetorch.Size([1, 2, 3])&gt;&gt;&gt; b = torch.squeeze(b,dim = 0)&gt;&gt;&gt; btensor([[1., 1., 1.], [2., 2., 2.]])&gt;&gt;&gt; b.shapetorch.Size([2, 3])&gt;&gt;&gt; b = a.view(1,2,3)&gt;&gt;&gt; btensor([[[1., 1., 1.], [2., 2., 2.]]])&gt;&gt;&gt; b.shapetorch.Size([1, 2, 3])&gt;&gt;&gt; b = torch.squeeze(b,dim = 0) #纵向&gt;&gt;&gt; btensor([[1., 1., 1.], [2., 2., 2.]])&gt;&gt;&gt; b.shapetorch.Size([2, 3]) torch.Tensor.narrow() 删除元素的维度缩减方式，torch.Tensor.narrow(dim,start,length),dim表示缩减的方向（0，1），start表示起始的位置，length表示保留维度的长度： 123456&gt;&gt;&gt; atensor([[1., 2., 3.], [4., 5., 5.]])&gt;&gt;&gt; a.narrow(1,1,2)tensor([[2., 3.], [5., 5.]]) torch.Tensor.permute() 张量维度之间的顺序调换： 1234567&gt;&gt;&gt; atensor([[1., 2., 3.], [4., 5., 5.]])&gt;&gt;&gt; a.permute(1,0)tensor([[1., 4.], [2., 5.], [3., 5.]])]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GIT]]></title>
    <url>%2F2019%2F05%2F06%2FGIT%2F</url>
    <content type="text"><![CDATA[github本质上是一个存储代码的工具，如果你暂时没有这个需求的话，其实可以不用在意这个东西，但是如果你在开发一个项目，希望将代码存在云上，并且实时更新与本地一致，那么github以及git操作就显得很重要了。（以上全是废话，ps：第一次以对话的方式写博客有点🦢慌） 下面教程从github上创建一个repository开始，重复一下比较常用的重要的git步骤 👇 创建repository手动上github官网，可视化方式创建一个repository，并添加上REMEAD.md等。由于我正在做深度学习作业，因此下面都将以DL_HW repository为例。 git clonegit clone git@github.com:WenHui-Zhou/DL_HW.git 通过上面语句将项目clone到本地（前提是安装了git）。然后接下来所有操作都将在这个DL_HW文件夹下进行操作。 添加.gitignore.gitignore 文件是用来告诉git什么文件不需要上传，比如你写了一个深度学习的作业，其中用到的数据集图片，就可以不需要上传。例子如下： 123tmp # 忽略文件夹*.jpg # 忽略文件.DS_Store 关于.gitignore还有很多灵活的用法，但是我是二八原则的拥护者，留下个链接表示一下：gitignore 用法 add and commit本地的git维护着三棵树，第一个是工作目录，即本地的DL_HW。第二个是缓冲区index，临时保存改动，第三个是head，保存最后一次的提交结果。 git add * : 将所有修改添加到index中去，保存零时改动。比如刚刚写了一个.gitignore文件，这条指令把它添加到index 上。 git commit -m &quot;代码提交信息&quot;：将index中保存的改动提交到head上去。 git push前一步的操作仅仅是在本地进行的，并没有将代码真正的更新到GitHub上 git push origin master： 将head上的改动提交到master分支上，也可以换成其他分支。 分支老实说现在用不到，留着以后补充 some tip1.Git clone的时候会clone下来所有的历史内容，可以限制仅仅clone最近改动后的版本。 1git clone git@github.com:WenHui-Zhou/DL_HW.git --depth=1 2.当有多个机子clone了相同的项目，要保证本地的代码为最新的，需要如下操作： 1git pull origin master(分支可改) last分享一个很不错的 教程链接]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dog and cat -- USE tf.contrib.slim]]></title>
    <url>%2F2019%2F05%2F06%2Fdog-and-cat-USE-tf-contrib-slim%2F</url>
    <content type="text"><![CDATA[深度学习作业之一：猫狗分类。使用tensorflow的一个轻量级的库 tf.contrib.slim实现。 数据准备猫狗分类的数据可以从gaggle官网中下载：数据链接 解压后发现文件分为train和val，但并没有label，它的label通过文件名来区分。 将下载下来的猫狗图片转化为tfrecord格式tf.record: 二进制格式文件 To read data efficiently it can be helpful to serialize your data and store it in a set of files (100-200MB each) that can each be read linearly. This is especially true if the data is being streamed over a network. This can also be useful for caching any data-preprocessing. The TFRecord format is a simple format for storing a sequence of binary records. tensorflow使用其Dataset API来管理数据，将数据直接放在graph中进行处理，整体对数据集进行上述数据操作，使代码更加简洁。将图片，label转化为tf.record格式，方便大数据集的分批，快速读取，同时在进行数据预处理时简化代码，加快处理速度。 TFRecord 的核心内容在于内部有一系列的 Example ，Example 是 protocolbuf 协议下的消息体。定义了你需要的数据集的信息。 protocolbuf： protocolbuf是 Google出品的一种轻量 &amp; 高效的结构化数据存储格式，具体介绍可以看 这里 即通过将结构化的数据进行序列化(转为二进制)，更小更易于维护。 因此这一部分的目的就是将猫狗的数据，以及对应的label，重新生成为tf.record格式文件，随后使用tensorflow提供的API进行数据的读取。 ps: 123for i in range(10000): filename = "%05d.txt" % i open(filename, "w") 上面代码命名文件时，i长度不足10000时前面补0，保证长度为5。]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux配置环境]]></title>
    <url>%2F2019%2F04%2F24%2Flinux%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[linux 环境配置是一个很重要又很烦人的过程，下面简要记录一下环境变量配置的方法与原则。 系统配置文件的加载顺序登入linux并启动一个bash shell，默认情况下这时候系统将会去寻找环境变量的设置文件，为环境变量赋值。系统环境文件读取顺序如下： 用户登录系统后首先会加载/etc/profile全局环境变量文件，这是Linux系统上默认的shell主环境变量文件。系统上每个用户登录后都会加载这个文件。 之后执行/etc/profile.d目录下的所有脚本文件，完成一些字体，颜色的设置 之后开始运行～/.bash_profile(用户环境变量文件)，在这个文件中，又会去找$~/.bashrc（用户环境变量文件） 。在$～/.bashrc文件中又会去找/etc/bashrc（全局环境变量文件），若没有则不执行。 对于Vim的配置来说，在vim开启的时候将会对其进行一些基础的配置。全局配置一般在/etc/vim/vimrc或者/etc/vimrc，对所有用户生效。用户个人的配置在~/.vimrc，打开vim时自动执行。 linux bash查找执行的顺序shell执行命令时将去linux系统中寻找指令的执行代码。寻找顺序如下 别名，使用alias创建的命令 关键字，如if，for 函数 内置指令，如cd等等 外部指令，在PATH路径中寻找 Linux 系统目录结构​ 以前很多的环境变量配置不明白，就是由于不清楚linux的目录结构，以及每个文件的位置。 /bin普通用户可以使用的命令的存放目录，十分重要。例如cp，cd这种。类似的目录：/usr/bin，/usr/local/bin等等。这个目录中的文件都是可执行的。作为基础系统所需要的最基础的命令就是放在这里。 /lib此目录下包含系统引导和在根用户执行命令时候所必需用到的共享库。类似的目录还/usr/lib，/usr/local/lib。 /home在Linux机器上，普通用户主目录通常直接或间接地置在此目录下。用户可以在自己的目录下保存仅对自己的配置文件，定制文件，文档，数据等。 /root用户root的$HOME目录。 /etc全局的配置文件存放目录。系统和程序一般都可以通过修改相应的配置文件，来进行配置。类似的目录有 /usr/etc。用户也可以直接在HOME目录底下写配置文件，系统读取配置文件时，先读取HOME目录底下的文件，优先级最高。如果不存在配置文件的话，才去/etc下读取系统配置。 /usr安装程序的时候，默认就是安装在此文件内部某个子文件夹内。输入命令后系统默认执行/usr/bin下的程序。当然/usr/bin 需要加入PATH中。 /usr/local安装本地程序的一般默认路径。当我们下载一个程序源代码，编译并且安装的时候，如果不特别指定安装的程序路径，那么默认会将程序相关的文件安装到这个目录的对应目录下。例如，安装的程序可执行文件被安装(安装实质就是复制到了/usr/local/bin下面），/usr/local/include则用来存放文件。 环境配置因此看到这里，环境变量的配置就是针对我们安装的第三方库，它们一般存在于/usr/下的目录中，因此PATH需要添加到/usr/的路径。此外还有一种情况，就是当安装一个库时，可能会修改掉系统的文件的软链接，导致之前系统很多库无法使用。此时的做法是在用户目录下，创建虚拟环境，在虚拟环境的进行环境的配置，将配置文件写在/home/.bashrc 等文件中即可。 上面泛泛而谈，还需要大量实践来查缺补漏。 例子安装python3.7，同时保留python3.6，python2.7等：【链接】]]></content>
      <tags>
        <tag>tip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell 编程]]></title>
    <url>%2F2019%2F04%2F10%2Fshell-%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[shell 编程中常见，常用的语法。 在日常Linux上编写代码，整理文件时发现，学一点shell语句能够大大加快工作效率，事不宜迟，开始学习！ shell 简介shell脚本通常是以：#!/bin/bash 开头的一个文件。/bin/bash是bash编译器的路径。 bash命令序列通常使用分号 ; 或者换行符来表示。 终端的输出使用echo 来输出。下面是一个简单的shell脚本。 12#!/bin/bashecho hello world 变量shell中所有变量的类型都是字符串，且无需提前定义。此外shell中规定了一些环境变量来存储操作系统中一些特殊的值。 变量的赋值： val=“value” ，切记等号前后没有空格。val = value 这种形式是判断相等的操作。 输出变量：echo $val 或 echo ${val} 环境变量： 定义在系统父进程中，用于系统的设置，如HTTP_PROXY用与设置代理服务器。 export 命令可以用来设置环境变量，至此之后，shell脚本执行任何应用都会继承整个变量。 最常用接触到的环境变量为PATH，PATH变量通常包含以下： 12echo $PATH/home/zhouwenhui/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr /games PATH中路径根据 : 做为分割符，每当用户执行一条指令时，linux根据PATH中路径从前往后寻找可执行文件。PATH通常定义在 /etc/environment 或 /etc/profile 系统层次，或 ~/.bashrc 这种用户层次上。可以通过一下方式，增加寻找的路径： 1export PATH="$PATH:/new/folder" 补充trick： 1cat a.txt | tr 'replace' 'value' 将输出中的replace替换成value。 字符串长度： 12var=1234length=$&#123;#var&#125; UID 是用户类型的一个标示，root用户的UID是0. shell 数学计算let 语句可以直接执行基本的算术操作，在变量名前不需要添加$. 1234567#!/bin/bashno1=4;no2=5;let result=no1+no2let no1++;let no1--;let no1+=1 操作符[ ] 使用方法与let类似： 1result=[ $no1 + no2 ]; 上诉的指令只能用来进行整数操作，浮点数操作将使用到bc工具包： 123&gt;&gt; echo "4 * 0.56" | bc&gt;&gt; 2.24&gt;&gt;result='echo "$no1 * 1.5"|bc' 文件描述以及重定向将输出内容保存到temp.txt中： 1echo "this string will be save" &gt; temp.txt 追加内容： 1echo "add to the file temp" &gt;&gt; temp.txt 数组1arr=(1,2,3,4,5,6) 创建别名1alias new_command = 'command sequence' 直接写入配置文件： 1echo 'alias cmd="command seq"' &gt;&gt; ~/.bashrc 函数12345678910111213141516function fname()&#123; statements;&#125;# 调用fname; # 执行#传递参数fname arg1,arg2;fname()&#123; echo $1; # 第一个参数 echo $2; # 第二个参数 echo $@; # 所有参数，"$1" "$2" ...&#125; for 循环1234for var in list;do commanddone; while 循环1234while condition;do commanddone; util语句123456x=0;until [ $x -eq 9 ];do let x++; echo $x;done 逻辑运算，简短比较12[ condition ] &amp;&amp; action; # 若condition成立则执行action[ condition ] || action; # 若condition不成立，则执行action 比较与测试12345678if condition;then commands;else if condition; then; commands;else commands;fi 算术比较12[ $var -eq 0 ] # 判断是否相同[ $var -ne 0 ] # 当var非0时为真 -gt：大于 -lt：小于 -ge：大于或等于 -le：小于或等于 结合多个条件测试： 12[ $var1 -ne 0 -a $var2 -gt 2 ] # 使用逻辑与-a[ $var1 -ne 0 -o $var2 -gt 2 ] # 逻辑或 -o 文件属性测试123456[ -f $file_name ] file_name是一个正常的文件[ -x $var ] var 是可执行文件[ -d $var ] var是目录[ -e $var ] var是文件[ -w $var ] var为可写文件[ -r $var ] var为可读文件 字符串的比较12345678[[ $str1 = $str2 ]] # 字符串比较最好放在双中括号中，判断相等[[ $str1 &gt; $str2 ]] # 判断字符串大小[[ -z $str1 ]] # 字符串为空则为真[[ -n $str1 ]] # 字符串非空则为真if [[ -n $str1 ]] || [[ -z $str2 ]];then echo 'something'fi 执行Linux指令12345678910a=$(ls)for file in $a;do if [ -f $file ]; then echo 'afile' else echo 'not file' fidone cat 拼接1234cat file1 file2 file3cat -s file # 输出过滤掉多余的空行cat -T file # 显示制表符cat -n file # 显示行号 文件查找find12345678find base_path # 找出所有bash_path 底下的所有文件名find . -name 'car*' # 找含特定字符的文件find . \( -name "*.txt" -o -name "*.pdf" \) # 匹配多个find /home/ -path "*/slynux/*" # 匹配路径以及文件名find . ! -name '*.txt' # 不找txt结尾的find . -maxdepth 1 -name 'f*' # 深度为1，只找当前目录find . -type d #将所有目录输出来 f为普通文件，l为软链接find . -type f -name "*.swp" -delete # 删除匹配的文件 find选项-exec 与其他指令结合使用12find . -type f -name ".c" -exec &#123;&#125;\; #&#123;&#125;将匹配所有的文件，然后执行find . -type f -name ".jpg" -exec cp &#123;&#125; ./file/ \;# 拷贝 玩转 xargsxargs以标准的输入作为主要的数据流：command| xargs。xargs从stdin接收到的数据重新格式化，将其作为参数提供给其他指令。]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VOC 数据集]]></title>
    <url>%2F2019%2F04%2F09%2FVOC-%E6%95%B0%E6%8D%AE%E9%9B%86%2F</url>
    <content type="text"><![CDATA[本篇文章介绍VOC数据集的格式以及将CSV标注转化成CSV格式文件的方法。 VOC 数据集VOC 数据集可以从官网下载，通常有 train： VOCtrainval_11-May-2012.tar，VOCtrainval_06-Nov-2007.tar test：VOCtest_06-Nov-2007.tar 解压后得到的文件目录结构如下： VOCDevkit: Annotations：存放着图片类别以及box信息,一张图片对应一个xml文件 ImageSets：里头有几个文件夹，目标检测问题只要关注Main，里头将保存训练集，测试集的图片名，用txt文件进行保存。 JPEGImages：保存着数据集图片 SegmentationClass SegmentationObject 对于目标检测问题关注以上三个文件夹就可以了。 将scv文件转化为voc格式csv格式为： 1image_url,x1,y1,x2,y2,label 且同一张图片由于可能会有多个框，所以会有多条记录，代码需要完成图片的软链接建立，图片的命名，并建立新名字的txt文件，包括train和text。同时生成每张图片的xml。 代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import csvimport osimport globfrom PIL import Imagefrom traceback import print_excimport syscount = 1def write_anno_xml(img,annos): anno_folder = "./Annotations" im = Image.open('./JPEGImages/' + img) width, height = im.size xml_file = open((anno_folder + '/' + img.split('.')[0] + '.xml'), 'w') xml_file.write('&lt;annotation&gt;\n') xml_file.write(' &lt;filename&gt;' + img + '&lt;/filename&gt;\n') xml_file.write(' &lt;folder&gt;cartoon_VOC&lt;/folder&gt;\n') xml_file.write(' &lt;size&gt;\n') xml_file.write(' &lt;width&gt;' + str(width) + '&lt;/width&gt;\n') xml_file.write(' &lt;height&gt;' + str(height) + '&lt;/height&gt;\n') xml_file.write(' &lt;depth&gt;3&lt;/depth&gt;\n') xml_file.write(' &lt;/size&gt;\n') for anno in annos: xml_file.write(' &lt;object&gt;\n') xml_file.write(' &lt;name&gt;' + anno[-1] + '&lt;/name&gt;\n') xml_file.write(' &lt;pose&gt;Unspecified&lt;/pose&gt;\n') xml_file.write(' &lt;truncated&gt;0&lt;/truncated&gt;\n') xml_file.write(' &lt;difficult&gt;0&lt;/difficult&gt;\n') xml_file.write(' &lt;bndbox&gt;\n') xml_file.write(' &lt;xmin&gt;' + anno[0] + '&lt;/xmin&gt;\n') xml_file.write(' &lt;ymin&gt;' + anno[1] + '&lt;/ymin&gt;\n') xml_file.write(' &lt;xmax&gt;' + anno[2] + '&lt;/xmax&gt;\n') xml_file.write(' &lt;ymax&gt;' + anno[3] + '&lt;/ymax&gt;\n') xml_file.write(' &lt;/bndbox&gt;\n') xml_file.write(' &lt;/object&gt;\n') xml_file.write('&lt;/annotation&gt;') xml_file.close()for file_name in ['train','test']: ftxt = open(file_name+'.txt','w') with open(file_name+'_dataset.csv','r') as f: reader = csv.reader(f) img = '' pre_img = '' annos = [] reader = list(reader) for line in reader: if img != line[0] and img != '': # ceate soft link pos = line[0].split('/')[-1].find('.') img_id = '0'*(5-len(str(count)))+str(count) count += 1 try: os.symlink(line[0],'JPEGImages/'+img_id+line[0].split('/')[-1][pos:]) except Exception as e: print(e.__class__.__name__) print_exc() ftxt.write(img_id+'\n') write_anno_xml(img_id+line[0].split('/')[-1][pos:],annos) img = line[0] annos.clear() annos.append(line[1:]) else: img = line[0] annos.append(line[1:]) sys.stdout.write('&#123;&#125;/&#123;&#125;\r'.format(count,len(reader))) sys.stdout.flush() ftxt.close()]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch 基本语法]]></title>
    <url>%2F2019%2F03%2F29%2Fpytorch-%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本篇文章将记录pytorch使用过程中的一些值得记录的trick。 pytorch 基本工作流【0】引入必要的包 123import torchimport torch.nn as nnimport numpy as np 【1】准备数据： 123a = torch.tensor(1.,requires_grad=True)x = torch.randn(10,3) # 10*3的矩阵y = torch.randn(10,2) # 10*2 【2】网络搭建 12345# 定义网络层，网络输入输出参数等等，下面使用pytorch内置的函数linear = nn.Linear(3,2) # 搭建一个输入channel为3，输出channel为2的全连接网络print(linear.weight) # torch以及替我们定义好了参数print(linear.bias) 【3】损失函数以及优化器 12criterion = nn.MSELoss()optimizer = nn.optim.SGD(linear.parameters(),lr = 0.01) 【4】网络正向传播 1234pred = linear(x)# 计算lossloss = criterion(pred,y)print(loss) 这一点和tensorflow很不一样，tensorflow要先搭建好整个网络，然后将数据feed进去，pytorch则是动态构建网络图，边搭建网络边进行传值。 【5】网络后向传播 123456789# 后向传播loss.backward()print('dl/dw',linear.weight.grad)print('dl/db',linear.bias.grad)# 使用optimizer的方式更新参数optimizer.step() # 一次更新参数pred = linear(x)loss = criterion(pred,y) ## 重复上诉步骤直到完成参数拟合 torch与numpy相互转化123456import numpy as npimport torchx = np.array([[3,2],[1,4]]) # numpy.arrayy = torch.from_numpy(x) # torch.tensorz = y.numpy() # tensor to numpy torch 导入数据的pipline（流程）torchvision是torch中一个用于 生成图片，数据集，模型类，欲训练模型的包。它主要包含一下几个部分： torchvision.datasets: 用于导入一些比较流行的开源数据集（cifar等） torchvision.models: 包含了很多流行的网络框架，包括alexnet，VGG，resnet，以及一下欲训练模型 torchvision.transforms: 定义了一些常用的数据预处理的函数，如random crop，rotate等等 torchvision.utils: 里头定义了很多好用的函数，如保存图片等 torchvision.datasets 的使用下载，导入数据，以及按一定的batch取出数据： 12345678# get the datasettrain_data = torchvision.datasets.CIFAR10(root='.',train=True,transform=torchvision.transforms.toTensor(),download = True)image,label = train_data[0]# load dataloader = torch.utils.data.Dataloader(dataset = train_data,batch_size = 64,shuffle=True)#每次load 一个大小为64的batch的数据train_iter = iter(loader)image,label = train_iter.next() pytorch 训练minist数据集中的一些方法123456## 读取数据train = torchvision.datasets.MNIST(root='./',train=True,transform=torchvision.transforms.ToTensor(),download = True)# data loaderdata_loader = torch.utils.data.DataLoader(dataset = train,batch_size = 100,shuffle = True)for image,label in data_loader: pass 训练阶段123456789for epoch in num_epoch: for i ,(image,label) in enumerate(train_loader): output = model(image.reshape(-1,28*28)) loss = criterion(output,label) optimizer.zero_grad() #切记，在计算导数前要将导数置零 loss.backward() optimizer.step() if i+1 == 100: print('epoch:&#123;&#125;/&#123;&#125;,step:&#123;&#125;/&#123;&#125;,loss:&#123;:.4f&#125;'.format(epoch+1,num_epochs,i+1,total_step,loss)) 测试阶段1234567891011# 不算梯度with torch.no_grad(): correct = 0 total = 0 for image,label in val_loader: output = model(image.reshape(-1,28*28)) _,predict = torch.max(output.data,1) total+= label.size(0) correct += (predict == label).sum().numpy()print('accuracy: &#123;&#125;'.format(correct/total))torch.save(model.state.dict,'model_param.ckpt') 其中val,index = torch.max(matrix,1)，计算matrix中每一列的最大值，返回最大值以及他的下标。 构建网络结构torch.nn 主要复制网络的构建，但是很多时候，torch.nn中不满足我们需要的网络，因此我们需要自己定义。torch.nn继承至nn.Module，nn.Module为所有网络的基类。当我们的网络类继承这个方法时，需要实现__init__(),forward()两个函数。 1234567891011class NerualNet(nn.Module): def __init__(self,input_size,hidden_size,output_size): super(NerualNet,self).__init__() self.fc1 = nn.Linear(input_size,hidden_size) self.ReLu = nn.ReLU() self.fc2 = nn.Linear(hidden_size,output_size) def forward(self,x): out = self.fc1(x) out = self.ReLu(out) out = self.fc2(out) return out 调用时model = NerualNet(input_size,hidden_size,output_size)，每次使用model(x)即自动执行forward。 卷积层1nn.Conv2d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True) 构建一个sequencesequence 将在其中的网络层从上到下连接上一层的输出作为下一层的输入。 1234567891011121314# Example of using Sequentialmodel = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() )# Example of using Sequential with OrderedDictmodel = nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(1,20,5)), ('relu1', nn.ReLU()), ('conv2', nn.Conv2d(20,64,5)), ('relu2', nn.ReLU()) ])) 图片预处理集合123456# Image preprocessing modulestransform = transforms.Compose([ transforms.Pad(4), transforms.RandomHorizontalFlip(), transforms.RandomCrop(32), transforms.ToTensor()]) 其中transforms.ToTensor()将 PIL image tensor (H, W, C) in range [0,255] to a torch.Tensor(C, H, W) in the range [0.0, 1.0]。 pytorch 保存以及导入预训练参数12345678model = ResNet(residual,[2,2,2]).to(device)...torch.save(model,'model.ckpt') # save the structuretorch.save(model.state_dict(),'model_para.ckpt') # save the parameter# loadmodel = torch.load('model.ckpt')# 下面的resnet结构需要提前定义好 resnet.load_state_dict(torch.load('model_para.ckpt')) resent实现需要注意的地方1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# Residual blockclass ResidualBlock(nn.Module): def __init__(self, in_channels, out_channels, stride=1, downsample=None): super(ResidualBlock, self).__init__() self.conv1 = conv3x3(in_channels, out_channels, stride) self.bn1 = nn.BatchNorm2d(out_channels) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(out_channels, out_channels) self.bn2 = nn.BatchNorm2d(out_channels) self.downsample = downsample def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample: residual = self.downsample(x) out += residual out = self.relu(out) return out# ResNetclass ResNet(nn.Module): def __init__(self, block, layers, num_classes=10): super(ResNet, self).__init__() self.in_channels = 16 self.conv = conv3x3(3, 16) self.bn = nn.BatchNorm2d(16) self.relu = nn.ReLU(inplace=True) self.layer1 = self.make_layer(block, 16, layers[0]) self.layer2 = self.make_layer(block, 32, layers[1], 2) self.layer3 = self.make_layer(block, 64, layers[2], 2) self.avg_pool = nn.AvgPool2d(8) self.fc = nn.Linear(64, num_classes) def make_layer(self, block, out_channels, blocks, stride=1): downsample = None if (stride != 1) or (self.in_channels != out_channels): downsample = nn.Sequential( conv3x3(self.in_channels, out_channels, stride=stride), nn.BatchNorm2d(out_channels)) layers = [] layers.append(block(self.in_channels, out_channels, stride, downsample)) self.in_channels = out_channels for i in range(1, blocks): layers.append(block(out_channels, out_channels)) return nn.Sequential(*layers) def forward(self, x): out = self.conv(x) out = self.bn(out) out = self.relu(out) out = self.layer1(out) out = self.layer2(out) out = self.layer3(out) out = self.avg_pool(out) out = out.view(out.size(0), -1) out = self.fc(out) return out 这段代码在结构设计上，将residual从整个网络中剥离出来。residual部分在resnet中多次使用，可以起到代码复用。这residual这一部分同样继承了nn.module，在resnet中进行调用。在整个网络反向求导的过程中，同样可以反向传播。 12out = out.view(out.size(0),-1) # 即保持第一维不变，然后后面的所有的维度特征进行flatten展开。类似于reshape。out = out.shape(out.size(0),-1) 在对resent进行evaluate的时候，需要先执行model.eval()。这是因为bn，dropout这些操作在训练和测试的阶段不一样。 pytorch中的LSTM的调用1234567891011121314151617181920# Recurrent neural network (many-to-one)class RNN(nn.Module): def __init__(self, input_size, hidden_size, num_layers, num_classes): super(RNN, self).__init__() self.hidden_size = hidden_size self.num_layers = num_layers self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) self.fc = nn.Linear(hidden_size, num_classes) def forward(self, x): # Set initial hidden and cell states h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) # Forward propagate LSTM out, _ = self.lstm(x, (h0, c0)) # out: tensor of shape (batch_size, seq_length, hidden_size) # Decode the hidden state of the last time step out = self.fc(out[:, -1, :])# 因为有许多层，只要最后一层 return out 其中LSTM调用： 12345nn.LSTM(input_size,hidden_size,num_layers，batch_first=True)# input_size 指输入的一个数据含有的特征数（维度）# hidden_size 指隐藏输出具有的特征# num_layer 指共有多少个LSTM层叠在一起# batch_first 指LSTM输出的h和c第一个维度都为batch h：hidden 12345h0 = torch.zeros(self,num_layers,x.size(0),self.hidden_size)# 指hidden处的参数# num_layers值共有几层# batch_size=x.size(0) 共有几个batch# hidden_size: 输出的hidden特征数 c： 12c0 = torch.zeros(self.num_layers,x.size(0),self.hidden_size)# 参数与上相同 调用： 1out,(hn,cn) = self.lstm(x,(h0,c0)) 一个较大项目的代码布局逻辑train.py : 程序开始执行的地方，作为整个项目的核心指挥，负责对个个部分进行调度。他的主要思路如下： 【1】通过argparse接受传入的各种配置参数，包括数据集的路径，model的存储路径等等。 1234if __name__ = '__main__': parse = argparse.ArgumentParser() parse.add_element('--model_path',type=str,default='./models',help='saving training model') ... 【2】调用main() 函数，开始执行程序。 123456789101112131415161718192021222324# 首先执行image progressing步骤，对图片进行预处理部分transform = transforms.Compose([ transforms.RandomCrop(args.crop_size), trnasforms.RandomHorizontalFlip(), ...])# build data loader# 此处实现了一个继承torch.utils.data.Dataset的数据集处理类，实现了__init__以及__getitem__。data_loader = get_loader(args.image_dir, args.caption_path, vocab,transform, args.batch_size,shuffle=True, num_workers=args.num_workers) # build the model# 此处实现了一个model类，继承至nn.Moduleencoder = EncoderCNN(args.embed_size).to(device)decoder = DecoderRNN(args.embed_size, args.hidden_size, len(vocab), args.num_layers).to(device)# 显示loss以及optimizercriterion = nn.CrossEntropyLoss() params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters()) optimizer = torch.optim.Adam(params, lr=args.learning_rate) ## train the model,通过data loader来产生数据for epoch in range(args.num_epochs): # 获取batch数据，进行训练以及预测]]></content>
      <categories>
        <category>tool</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSD 复现]]></title>
    <url>%2F2019%2F03%2F29%2FSSD-%E5%A4%8D%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[SSD是经典的one-stage目标检测框架，在速度和精度上都比Faster RCNN，YOLO（V1？）要更胜一筹。这次复现SSD作为理解网络以及学习pytorch的一个机会，这篇文章将尽可能的详细记录SSD的复现细节。（好大的flag🍐） 在复现SSD之前，我想就pytorch的两大部件进行一下介绍，分别是数据集模块（torch.utils.data.Dataset）以及网络模块(torch.nn.Module)。 数据集模块 pytorch数据读取主要有三个类： Dataset DataLoader DataLoaderIter 他们使用的方式为Dataset做为参数传入DatasetLoader中，DataLoader做为参数传入DataLoaderIter中。 因此完成pytorch数据集读取模块第一步要做的是： 【1】定义数据集类。 torch.utils.data.Dataset 是一个抽象类，因此继承Dataset需要实现他的两个方法，__len__()，__getitem__()。 12345678910111213141516import torchimport torch.utils.data as datadata_set = &#123;1:'a',2:'b',3:'c'&#125;class CustomDataset(data.Dataset):#需要继承data.Dataset def __init__(self): # 1. Initialize file path or list of file names. self.data = data_set def __getitem__(self, index): # 每次读取一张图片以及对应的label， # 可以对图片进行一些flip等操作（torchvision.Transform). # 最终返回的是一个含有(image,label)的pair # 可以在init()函数的位置处生成csv_reader,或是一些list，集合 return index, self.data[index] def __len__(self): return len(self.data) 对于这个Dataset这个类，只要实现了这两个函数，然后每次调用的的时候都能出来一个img，label，内部无论是list，generator都是可行的。 在__getitem__() 处可以执行一些图片变换等工作，torchvision.transforms中有着许多对图片的增强操作。常用的有Resize , RandomCrop , Normalize , ToTensor (这个极为重要, 可以把一个PIL或numpy图片转为torch.Tensor） 【2】定义dataLoader dataLoader的定义如下： 1class torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=&lt;function default_collate&gt;, pin_memory=False, drop_last=False) 其中dataset即上面定义的dataset，batch_size指一次调用该函数，输出的样本个数。num_workers指线程数，当大于等于1时就表示多线程。collate_fn 用于定制输出的batch，通过传入lambda表达式来实现，即当一张图片对应多个边框的时候，就需要进行图片以及边框的匹配。 dataLoader还实现了一个__iter__() 函数，这个函数输入为dataLoader，输出为dataLoaderIter，是一个迭代器。 具体使用如下： 1234567dataset = CustomClass()dataloader = data.DataLoader(dataset,batch_size = 10,...)for data in dataloader: # data[0]为图片 # data[1]为标准 # 共有10对 pass 网络结构模块pytorch 使用nn.Module 来构建网络，在pytorch中每一个网络层都是一个nn.Module类，并且类之间相互嵌套。nn.Module中有两个比较重要的部分，分别是 __init__() ：完成逻辑模块的初始化。 forward()：完成计算图的正向传递的过程。例如nn.Linear模块的定义如下： 12345678910class MyLinear(nn.Module): def __init__(): super(MyLinear,self).__init__() self.w = nn.Parameter(torch.randn(outp,inp)) self.b = nn.Parameter(torch.randn(outp)) def forward(self,x): x = x @ self.w.t() + self.b return x pytorch中提供了许多现成的类可供使用： nn.conV2d nn.MaxPool2d nn.ReLu nn.BatchNorm2d 同时nn.Sequential实现了一个序列，用来构建网络模块。 1234567self.net = nn.Sequential( nn.Conv2d(in_size,out_size,kernel_size,1,1) nn.MaxPool2d(2,2) nn.ReLu() nn.BatchNorm2d(32) ...) 输入将按照网络层从上到下进行参数的传递。 此外nn.Module类还会对网络的参数进行管理，nn.parameters()中将会保存着网络所有的参数。便于参数的管理。 我们可以使用nn.module构建许多的网络层，然后通过输入输出传值的方式将他们连成一个计算图。 下面将按照数据的读入，网络的搭建，网络的训练，以及效果的评估几个方面进行。 SSD 复现参考github链接。 【1】数据的准备 数据集是一些由视频切帧而来的图片，一秒切一帧，对于每张图，由相应的标注信息，标注信息csv格式。通过读取csv数据集的方式，来完成数据的读取（github版本为使用pycocotool读取数据）。通过继承data.Dataset以及实现dataLoader的方式来获取数据。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import torchimport torch.utils.data as dataimport torchvision.transforms as transformsimport csvimport numpy as npimport cv2# csv格式为：url,x1,y1,x2,y2,labelTRAIN_ROOT = './data/train_dataset.csv'VAL_ROOT = './data/val_dataset.csv'def detection_collate(batch): targets = [] imgs = [] for sample in batch: imgs.append(sample[0]) targets.append(sample[1]) return imgs,targetsclass csv_loader(data.Dataset): def __init__(self,data_root,transform = transforms.ToTensor()): self.data_root = data_root self.dataset = &#123;&#125; self.imgs_index = &#123;&#125; self.transform = transform self.index = 0 with open(self.data_root,'r') as f: lines = csv.reader(f) for line in lines: if line[0] in self.dataset: self.dataset[line[0]].append(line[1:5]) else: self.dataset[line[0]] = [line[1:5],] self.imgs_index[self.index] = line[0] self.index += 1 def __getitem__(self,index): img_path = self.imgs_index[index] label = self.dataset[img_path] img = cv2.imread(img_path) img = self.transform(img) for i in range(len(label)): label[i][0] = float(label[i][0]) label[i][1] = float(label[i][1]) label[i][2] = float(label[i][2]) label[i][3] = float(label[i][3]) label = np.array(label) return img,label def __len__(self): return self.index+1dataset = csv_loader(TRAIN_ROOT)dataloader = data.DataLoader(dataset,batch_size = 2,collate_fn = detection_collate)for img,label in dataloader: print(img) print(label) break 如上，可以看出我们使用detection_collate方法来对每个batch size中读到的数据进行二次组织。 【2】网络的构建 数据已经准备好了，接下来要做的就是将网络搭建起来，然后将数据输入。 ssd的网络的backbone是vgg网络，利用vgg网络提取图片特征。 vgg的结构如下： 实现backbone的代码如下： 12345base = &#123; '300': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M', 512, 512, 512], '512': [],&#125; hard negative miningSSD 中对feature map位置的提取6个或4个边框，这些边框的尺寸由一些超参数决定。在进行网络训练之前，需要对生成的这些边框进行正负样本的标注，标注的标准在于这些边框与GT边框的IoU重合度，如果重合度大于0.5，则表示这个边框是证样本，小于0.3表示这个边框是负样本。 在对正负样本进行标注时，一般要保证正样本：负样本的个数为1:3。但是对于一张图片来说，其上大部分的框都是负样本，因此需要进行hard negative mining将一些得分较高的negative 做为hard negative。 hard negative mining一般是，有正负样本，然后分类器分出来一些分错的负样本（容易将负样本看成正样本的那些样本），即假阳性(false positive )，也就是说在对负样本分类时候，loss比较大（label与prediction相差较大）的那些样本，这些就是hard negative/困难样本，进行重新训练。 网络搭建部分主要继承nn.Module模块，继承init以及forward模块，实现网络结构的搭建，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205#!/usr/bin/env python #-*- coding: utf-8 -*-import torchimport torch.nn as nn# 记录各层的channelbase = [64,64,'M',128,128,'M',256,256,256,'C',512,512,512,'M',512,512,512] # M表示floor（边角舍弃）方式的Maxpooling，C表示ceil（补全）方式的Maxpooling# vgg之后的各各层extras = [256,'S',512,128,'S',256,128,256,128,256]#每一层每个像素位置将取出的边框个数mboxes = [4,6,6,6,4,4]def vgg(base,input_channel,batch_norm=None): ''' base: 各层的channel input_channel：传入数据的维度 batch_norm：是否使用bn 这个函数主要使用一个list，将每一层的函数存储起来，用base来控制当前层是什么 ''' layers = [] in_channels = input_channel for v in base: if v == 'M':# 表示是一个maxpooling layers += [nn.MaxPool2d(kernel_size=2,stride=2)] elif v == 'C': layers += [nn.MaxPool2d(kernel_size=2,stride=2,ceil_mode=True)] else: conv2d = nn.Conv2d(in_channels,v,kernel_size=3,padding=1) if batch_norm: layers += [conv2d,nn.BatchNorm2d(v),nn.ReLU(inplace=True)] # inplace=True 指它将直接修改input的值，而不重新分配空间 else: layers += [conv2d,nn.ReLU(inplace=True)] in_channels = v pool5 = nn.MaxPool2d(kernel_size=3,stride=1,padding=1) conv6 = nn.Conv2d(512,1024,kernel_size=3,padding=6,dilation=6) conv7 = nn.Conv2d(1024,1024,kernel_size=1) layers += [pool5,conv6,nn.ReLU(inplace=True),conv7,nn.ReLU(inplace=True)] return layersdef add_extras(extras,in_channel,batch_norm=None): # extra layers added to vgg for feature scaling layers = [] in_channels = in_channel flag = False for k,v in enumerate(extras): if in_channels!='S': if v == 'S': layers += [nn.Conv2d(in_channels,extras[k+1],kernel_size=(1,3)[flag],stride=2,padding=1)] else: layers += [nn.Conv2d(in_channels,v,kernel_size=(1,3)[flag])] flag = not flag in_channels = v return layersdef multibox(vgg,extras_layers,mbox,num_classes): loc_layers = [] conf_layers = [] vgg_source = [21,-2] # vgg的21层即conv4_3,和-2层即fc7 for k,v in enumerate(vgg_source): loc_layers += [nn.Conv2d(vgg[v].out_channels,mbox[k]*4,kernel_size=3,padding=1)] # location 有四个参数 conf_layers += [nn.Conv2d(vgg[v].out_channels,mbox[k]*num_classes,kernel_size=3,padding=1)] # 类别预测将有class_num个数 for k,v in enumerate(extras_layers[1::2],2): # 这里就是说取extras中奇数层，然后取bounding box，从第二层开始 loc_layers += [nn.Conv2d(v.out_channels,mbox[k]*4,kernel_size=3,padding=1)] conf_layers += [nn.Conv2d(v.out_channels,mbox[k]*num_classes,kernel_size=3,padding=1)] return vgg,extras_layers,(loc_layers,conf_layers)class priorBox(obejct): """ 在feature map上计算初始边框的坐标 """ def __init__(self,cfg): # 将config中的一些超参赋值过来 self.image_size = cfg['min_dim'] self.num_priors = len(cfg['aspect_ratios']) self.variance = cfg['variance'] or [0.1] self.feature_maps = cfg['feature_maps'] self.min_sizes = cfg['min_sizes'] self.max_sizes = cfg['max_sizes'] self.steps = cfg['steps'] self.aspect_ratios = cfg['aspect_ratios'] self.clip = cfg['clip'] self.version = cfg['name'] for v in self.variance: if v &lt;= 0: raise ValueError('Variances must be greater than 0') def forward(self): mean = [] for k ,f in enumerate(self.feature_maps): f_k = self.image_size / self.steps[k] s_k = self.min_sizes[k] / self.image_size s_k_prime = sqrt(s_k*(self.max_sizes[k]/self.image_size)) for i,j in product(range(f),repeat=2): # unit center x,y cx = (j + 0.5) / f_k cy = (i + 0.5) / f_k #aspect_ratio: 1 mean += [cx,cy,s_k,s_k] mean += [cx,cy,s_k_prime,s_k_prime] for ar in self.aspect_ratios[k]: mean += [cx,cy,s_k*sqrt(ar),s_k/sqrt(ar)] mean += [cx,cy,s_k/sqrt(ar),s_k*sqrt(ar)] # back to torch land output = torch.Tensor(mean).view(-1,4) if self.clip: output.clamp_(max=1,min=0) return outputclass SSD(nn.Module): def __init__(self,phase,size,base,extras,head,num_classes): ''' phase: train,test size: ssd输入图片的大小，也即是版本把 base: vgg的网络结构 extras: vgg之后的那些层 head: loc，conf 的boxes ''' super(SSD,self).__init__() self.phase = phase self.num_classes = num_classes self.cfg = (coco,voc)[num_classes == 21] # config.py 中对数据集的一些配置 self.priorbox = PriorBox(self.cfg) self.priors = Variable(self.priorbox.forward(),volatile=True) self.size = size ## ssd net self.vgg = nn.ModuleList(base) self.L2Norm = L2Norm(512,20) self.extras = nn.ModuleList(head[1]) self.loc = nn.ModuleList(head[0]) self.conf = nn.ModuleList(head[1]) if phase == 'test': self.softmax = nn.Softmax(dim = -1) ## detection.py self.detect = Detect(num_classes,0,200,0.01,0.45) def forward(self,x): sources = list() loc = list() conf = list() # apply vgg to conv4_3 relu for k in range(23): x = self.vgg[k](x) s = self.L2Norm(x) sources.append(s) # apply vgg up to fc7 for k in range(23,len(self.vgg)): x = self.vgg[k](x) sources.append(x) # apply extra layers for k,v in enumerate(self,extras): x = F.relu(v(x),inplace=True) if k%2 == 1: sources.append(x) for (x,l,c) in zip(sources,self.loc,self.conf): loc.append(l(x).permute(0,2,3,1).contiguous()) conf.append(c(x).permute(0,2,3,1).contiguous()) loc = torch.cat([o.view(o.size(0),-1) for o in loc],1) conf = torch.cat([o.view(o.size(0),-1) for o in conf],1) if self,phase == 'test': output = self.detect( loc.vire(loc.size(0),-1,4), self.softmax(conf.view(conf.size(0),-1,self.num_classes)), self.priors.type(type(x.data)) ) else: output = ( loc.view(loc.size(0),-1,4), conf,vire(conf.size(0),-1,self.num_classes), self.priors ) return output def load_weights(self,base_file): other,ext = os.path.splitext(base_file) if ext == '.pkl' or '.pth': self.load_state_dict(torch.load(base_file, map_location=lambda storage,loc:storage)) else: print('sorry wrong')def build_ssd(phase,size=300,num_classes=21): if phase != 'test' and phase != 'train': print('error') return if size != 300: print('error') return base_,extras_,head_ = multibox(vgg(base[str(size)],3),add_extras(extras[str(size)],1024), mbox[str(size)],num_classes) return SSD(phase,size,base_,extras_,head_,num_classes)]]></content>
      <categories>
        <category>论文复现</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手撕mAP]]></title>
    <url>%2F2019%2F03%2F22%2F%E6%89%8B%E6%92%95mAP%2F</url>
    <content type="text"><![CDATA[mAP在很多检测任务中使用十分频繁，微软的COCO数据集专门提供了一套API，实现预测模型的mAP计算（pycocotools），本篇文章打算用最原始的方式手撕mAP，希望使得对整个指标更好的理解。文章参考Retina-net，并在其基础上进行修改。 mAP是什么？mAP： mean Average Precision, 即各类别AP的平均值，例如COCO数据集，共有80+1类（背景），对每一个类别的物体求一个AP，mAP即为所有目标AP的平均值。 AP：AP为PR曲线（precision-recall）与x轴围成的面积 Pricision：TP/（TP+FP），即预测为真（预测结果放后面即TP）当中，真正为真的比例。 Recall：TP/（TP+FN），即预测为真当中真正为真的个数，占所有样本中真个数的比例。 对于TP，FP，TN，FN表示四种情况，其中T，F是从结果来看，是否预测正确。P，N则是从预测来看，预测正误。 TP：预测是对的，预测样本结果为真。该类样本的个数 FP：预测是错的，预测样本为真。该类个数。 TN：预测是对的，预测样本为假。该类个数。 FN：预测是错的，预测样本为假。该类个数。 真假鉴定：当预测边框与GT的边框重合程度，PASCAL数据集中，认为IoU大于0.5认为是真，小于0.5认为是假。 IoU：预测边框与GT边框的 重叠面积/两个边框并集 mAP-IoU[0.5, 0.95]：COCO要求IOU阈值在[0.5, 0.95]区间内每隔0.05取一次，将这个IoU作为真假边框的评判边界。可以计算出10个IoU下的mAP值，然后这10个还要再做平均，即为最后的mAP。 疑问指标虽然很多，但是都是很简单的指标，耐心的理解一下，也不辛苦的。看完上面的指标有几个疑问： IoU计算的时候需要边框与GT对应起来，每个GT对应一个边框后不再参与后面边框的匹配。那么与哪个GT边框对应呢，这是个问题？（置信度+IoU最大） AP在计算的时候需要计算AP曲线下方的面积，这个该怎么算呢？ 计算precision，recall的时候需要每个类单独算，然后用于之后算AP，感觉是几个循环，外循环是个遍历类别，内循环对每个预测边框做一下循环，具体怎么实现呢？ 实作这一部分将按照输入数据，数据处理，计算IoU，计算Precision，Recall，计算AP等步骤。 预测结果数据：假设经过模型预测得到一个csv格式的预测结果，格式如下： 1/path/to/1.jpg,10,78,25,34,face,0.9 分别是图片的位置，预测的边框（左上）（x,y,w,h)，label，以及置信度。 GT数据： 1/path/to/1.jpg,15,80,30,32,face 分别是图片的位置，边框位置以及标签。 数据处理： 为了更好的计算每一个类别预测的precision以及recall，直觉上来说，应该需要一个比较好的格式方便计算，我们可以将这种格式设置如下： 1all_detections = [img_index][label][box_index] 意思为每一个图片，对应若干个label（例如一张图上对应桌子，人），每个label对应若干个边框，（例如一张图片中有多个人）。 因此第一步需要把csv格式的数据转换为上面的格式，在转换之前需要借助道dcit字典。目的是为了对相同的图片的label。boxes进行汇总。dict的格式如下： 1234[ img1:&#123;label1:[[box1],[box2]...],label2:[[box1],[box2]...]..&#125; img2:&#123;label1:[[box1]]&#125; ] 即外围是一个list[],保存每一张图片的信息。每个图片是一个字典，key为图片名，val是另一个保存label和boxes的字典，这个字典的key为label名，val为多个boxes的list结构。 下面代码是读取csv文件，并将数据转化为上诉格式： 1234567891011121314151617181920212223242526272829303132import csvimg_name = &#123;&#125; # imgId: 1boxes_label_scores = &#123;&#125; # imgId: [[x,y,w,h,score],...]class_num = 1 # 表示类别个数pre_gt_csv = 'score_mintest.csv' with open(pre_gt_csv,'r') as f: reader = csv.reader(f) lines = list(reader) for line in lines: if len(line) &lt; 7: line.append('1') # 当为GT的时候，最后需要添上置信度为1 if float(line[-1]) &lt; score_threshold: continue if line[0] not in img_name.keys(): img_name[line[0]] = 1 temp = line[1:5] temp.append(line[-1]) # [x,y,w,h,score] box_dict = &#123;&#125; box_dict[line[5]] = [temp] # label:[[],[]] #&#123;img: &#123;label_name:[x,y,w,h,socre],[x2,y2,w2,h2,score2]...&#125;,img:&#123;...&#125;.. &#125; boxes_label_scores[line[0]] = box_dict else: # the image is exist temp = line[1:5] temp.append(line[-1]) if line[5] in boxes_label_scores[line[0]].keys(): boxes_label_scores[line[0]] [line[5]].append(temp) else: boxes_label_scores[line[0]][line[5]] = [temp] 在进行边框对比的时候，我们希望对置信度高的边框提前进行IoU的判断，因此对boxes_label_scores中的boxes进行置信度的排序(解决第一个问题)，如下： 123456for img in boxes_label_scores: labels = boxes_label_scores[img] for label in labels.keys(): boxes = boxes_label_scores[img][label] boxes = sorted(boxes, key=lambda x: float(x[-1]),reverse=True) boxes_label_scores[img][label] = boxes 由于上面字典的结构，key与label均为真实值，然后我们希望用all_detections这个list的结构来代替，因此需要引入图片与下标，label与下标的一一对应关系。 图片与下标对应：我们对测试集中读取的图片从上到下，依次进行计数。该数对应该图片的Id（切记，在进行GT比较时，顺序不能乱）。 label与下标对应： 将其转化为下标，从0开始一次进行计数。 也可以专门生成一张数字与图片，数字与类别一一对应的表格，比较直观。上诉方法则比较方便，但是容易混乱。因此将字典结构赋值给三重数组代码如下： 1234567891011# all_detections : [img1[label1],[label2]..] ; img2: label1,label2...all_detections = [[None for i in boxes_label_scores[img].keys()] for img in boxes_label_scores.keys()] #inds_keys = list(img_name.keys()) # [img1,img2,...n-1]inds = img_name.keys() # 充当图片的id，与图片一一对应# ind与图片路径一一对应for ind,img in (enumerate(inds)): # 每次得到img_name 即图片路径 # index 与label一一对应 for index,label in enumerate(boxes_label_scores[img].keys()): all_detections[ind][index] = boxes_label_scores[img][label] ## ind为图片，index为类别，从0开始return all_detections 计算precision，recall 生成数据之后需要根据数据去计算TP，FP，TN，FN等参数。一个直观的想法就是大循环是个label，然后每次算出一个类的AP之后，保存一下，循环结束了算一个平均。 计算precision即计算预测边框中真正预测对的部分占预测为真的个数。计算recall即计算预测边框中TP与总的GT的比例。因此我们以label为大循环，一次去遍历每一张图片，然后去更新TP，FP的值。如下代码： 1234567891011121314151617181920212223242526272829inds = list(range(len(img_name.keys()))) # 充当图片的id，与图片一一对应 for label in range(class_num): # false_positives = np.zeros((0,)) # precision = TP/（TP+FP）Recall = TP/（TP+FN） true_positives = np.zeros((0,)) scores = np.zeros((0,)) num_annotations = 0.0 for i in inds: detections = all_detections[i][label] # image：i，class：label annotations = all_annotations[i][label] num_annotations += len(annotations) #.shape[0] # boxes的个数 detected_annotations = [] for d in detections: scores = np.append(scores, float(d[4])) #if annotations.shape[0] == 0: if len(annotations) == 0: # 预测为真，但这个label的个数是0 false_positives = np.append(false_positives, 1) true_positives = np.append(true_positives, 0) continue overlaps = compute_overlap(np.expand_dims(d, axis=0), annotations) assigned_annotation = np.argmax(overlaps, axis=1) max_overlap = overlaps[0, assigned_annotation] if max_overlap &gt;= iou_threshold \ and assigned_annotation not in detected_annotations: # IoU满足条件，分配的标注没有被标注过 false_positives = np.append(false_positives, 0) # FP += 0 true_positives = np.append(true_positives, 1) # TP += 1 detected_annotations.append(assigned_annotation) else: # 标注已经使用过 false_positives = np.append(false_positives, 1) true_positives = np.append(true_positives, 0) 值得注意的是，false_positives与true_positives并不是直接算个和，而是将每一张图片是否为TP，FP按照1，0保留下来。如下： 1false_positives = [0,1,0,1,1,1] # 下标表示图片的序号，0表示否，1表示真 这样存储的好处在于随后计算AP（PR曲线下方面积）时，方便计算。 计算单个 label的AP上一个部分代码得到了每张图片的PR值结果。计算AP值即算PR曲线的下方面积，因为不能直接算积分，因此我们需要想想办法。PR图是一张recall为x轴，precision为y轴的曲线，随着图片进行叠加，分别计算出P，R值，然后绘制出曲线。为了保证PR值尽量准确，我们首先对图片进行置信度从高到低的一个排序，然后累加计算其PR值。 1234567891011121314151617 # no annotations -&gt; AP for this class is 0 (is this correct?)if num_annotations == 0: average_precisions[label] = 0, 0 continue # sort by scoreindices = np.argsort(-scores)false_positives = false_positives[indices] true_positives = true_positives[indices]# compute false positives and true positivesfalse_positives = np.cumsum(false_positives) # 依次累加true_positives = np.cumsum(true_positives)# compute recall and precision num_annotations也是据图片累加的recall = true_positives / num_annotationsprecision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps) 上诉代码首先根据scores对PR值进行排序，然后对每张图片从1…n累加计算出TP，FP值，因此最终得到的TP，FP也是一个长度为图片个数的数组。 计算AP的方法： 计算AP通常有两种方式，一种是07年以前的11点法，第二种是则是对每一个点都计算差值。 Calculating the interpolation performed in all points该部分参考github上的讲解。先看图，对于Precision与Recall的插值如下， 也就是说，对于precision来说，从末尾开始，precision每个点的取值都等于其前一个点与当前点的最大值，即mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])。当遇到更大的precision时，重新开始重复上面计算，得到许多矩形框如下,计算该面积即可： 代码如下： 12345678910111213141516171819202122232425def _compute_ap(recall, precision): """ Compute the average precision, given the recall and precision curves. Code originally from https://github.com/rbgirshick/py-faster-rcnn. # Arguments recall: The recall curve (list). precision: The precision curve (list). # Returns The average precision as computed in py-faster-rcnn. """ # correct AP calculation # first append sentinel values at the end mrec = np.concatenate(([0.], recall, [1.])) mpre = np.concatenate(([0.], precision, [0.])) # compute the precision envelope for i in range(mpre.size - 1, 0, -1): mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i]) # to calculate area under PR curve, look for points # where X axis (recall) changes value i = np.where(mrec[1:] != mrec[:-1])[0] # and sum (\Delta recall) * prec ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1]) return ap 计算IoU当我们在计算Precision与Recall的时候需要判断样本是否是真样本，因此需要计算IoU值，计算IoU的大致思路如下，首先对一张图片，拿到一个置信度最高的边框，然后对该边框与该图片所有的GT边框都计算一个IoU，选出一个IoU值最大的GT边框作为与该边框匹配的边框。 1234import compute_overlapoverlaps = compute_overlap(np.expand_dims(d, axis=0), annotations) assigned_annotation = np.argmax(overlaps, axis=1) max_overlap = overlaps[0, assigned_annotation] 其中compute_overlap库是一个动态链接库，即为一个.so文件，通过.c文件编译而来。overlap代码github地址。算法就是那样了，retina-net作者偷懒，直接用了fast rcnn的代码，我也偷个懒😂。 值得注意的是，每当一个GT边框被使用过之后，需要将其标记一下，避免下次重复计算。 总结求mAP的方法需要通过预测网络提前生成测试集的box，然后将pred_csv, GT_csv传入方法中，最终求返回每个类别的AP。]]></content>
      <categories>
        <category>手撕系列</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python Tip]]></title>
    <url>%2F2019%2F03%2F20%2Fpython-Tip%2F</url>
    <content type="text"><![CDATA[字符串查找元素123astr = '1234'astr.find('1') # 返回下标或-1astr.rfind('1') # 反向查找 python lambda 表达式12345678910g = lambda x:x+1 # x为输入，x+1为输出: g(1) = 2# python 中自带的lambda表达式# foo =[2, 18, 9, 22, 17, 24, 8, 12, 27]# 输出：[18, 9, 24, 12, 27]filter(lambda x:x%3 ==0,foo)# map,将foo中每个元素都算一下#输出：[14, 46, 28, 54, 44, 58, 26, 34, 64]map(lambda x: x * 2 + 10, foo)#reduce 类加reduce(lambda x, y: x + y, foo) 获取图片大小123from PIL import Imageim = Image.open('whatever.png')width, height = im.size python 🀄️的类123456class Animal(object): def __init__(self,name): self.name = name self.__sex = man ## 在属性前加上两个_ 变成私有变量 def greet(self): print('hello'+self.name) python中前后都有双下划线的变量是特殊变量，如__ver__,可以直接访问，定义式避免这种定义方式。例外，仅有一个下划线，如_name,这种变量表示不要轻易访问，但是它是可以被直接访问的。 获取变量信息例如dog = Animal(&#39;dog&#39;): type(dog) 来获取dog的类型。 isinstance(dog,Animal) 判断dog的类型 hasattr(obj, attr) 判断类是否有attr方法/属性 getattr(obj,attr[,default]):得到属性的值 setattr(obj, attr, value): 设置属性的值 dir(dog): 获取dog的所有属性和方法 类方法，静态方法可以使用类或实例直接访问： 1234567class A(object): @classmethod def class_info(cls): print(cls) @staticmethod def static_info(): print('something') 定制类以及魔法方法python中有一类方法，使用双下划线包裹起来：__new__等等，这类方法称为魔法方法，可以对类提供特殊的功能，方便定制类。 __new__(cls): 当创建一个类时，首先调用__new__(cls)方法，之后再调用__init__() __str__: 当我们直接输出一个实例时，如print(dog),得到的输出为：&lt;__main__.Animal object at 0x10c37aa50&gt;,通过覆盖__str__ 方法可以输出我们想要的内容。 __repr__: 当我们不用print时，调用该方法 1234567class A(object): def __str__(self): return 'Animal object (name: %s)' % self.name def __repr__(self): return 'lalal'print(Animal(dog)) ## 调用__str__()Animal(dog) ## 调用 __repr__() __iter__(),__next__(): 定义该方法使得类允许迭代调用,首先调用__iter__() 获得一个迭代器，然后每次迭代调用next。（可以不定义iter）。 __geitem__ 用于获取值，类似地，__setitem__ 用于设置值，__delitem__ 用于删除值，让我们看下面一个例子： 123456789101112131415161718192021class Point(object): def __init__(self): self.coordinate = &#123;&#125; def __str__(self): return "point(%s)" % self.coordinate def __getitem__(self, key): return self.coordinate.get(key) def __setitem__(self, key, value): self.coordinate[key] = value def __delitem__(self, key): del self.coordinate[key] print 'delete %s' % key def __len__(self): return len(self.coordinate) __repr__ = __str__ 调用： 1234567891011121314151617&gt;&gt;&gt; p = Point()&gt;&gt;&gt; p['x'] = 2 # 对应于 p.__setitem__('x', 2)&gt;&gt;&gt; p['y'] = 5 # 对应于 p.__setitem__('y', 5)&gt;&gt;&gt; p # 对应于 __repr__point(&#123;'y': 5, 'x': 2&#125;)&gt;&gt;&gt; len(p) # 对应于 p.__len__2&gt;&gt;&gt; p['x'] # 对应于 p.__getitem__('x')2&gt;&gt;&gt; p['y'] # 对应于 p.__getitem__('y')5&gt;&gt;&gt; del p['x'] # 对应于 p.__delitem__('x')delete x&gt;&gt;&gt; ppoint(&#123;'y': 5&#125;)&gt;&gt;&gt; len(p)1 __getattr__() 只有在属性不存在的情况下才会被调用。 与 __getattr__ 一起使用的还有 __setattr__, __delattr__，类似 obj.attr = value, del obj.attr: 12345678910111213141516171819202122232425262728293031323334353637383940class Point(object): def __init__(self, x=0, y=0): self.x = x self.y = y def __getattr__(self, attr): if attr == 'z': return 0 raise AttributeError("Point object has no attribute %s" % attr) def __setattr__(self, *args, **kwargs): print 'call func set attr (%s, %s)' % (args, kwargs) return object.__setattr__(self, *args, **kwargs) def __delattr__(self, *args, **kwargs): print 'call func del attr (%s, %s)' % (args, kwargs) return object.__delattr__(self, *args, **kwargs)&gt;&gt;&gt; p = Point(3, 4)call func set attr (('x', 3), &#123;&#125;)call func set attr (('y', 4), &#123;&#125;)&gt;&gt;&gt; p.z0&gt;&gt;&gt; p.z = 7call func set attr (('z', 7), &#123;&#125;)&gt;&gt;&gt; p.z7&gt;&gt;&gt; p.wTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 8, in __getattr__AttributeError: Point object has no attribute w&gt;&gt;&gt; p.w = 8call func set attr (('w', 8), &#123;&#125;)&gt;&gt;&gt; p.w8&gt;&gt;&gt; del p.wcall func del attr (('w',), &#123;&#125;)&gt;&gt;&gt; p.__dict__&#123;'y': 4, 'x': 3, 'z': 7&#125; __call__ 方法,对实例进行调用就好像对函数调用一样。 12345class A(object): def __call__(self): return 1+1a = A()a() # 将调用__call__方法 使用 __slots__ 来告诉 Python 只给一个固定集合的属性分配空间，如下： 12345678class Point(object): __slots__ = ('x', 'y') # 只允许使用 x 和 y def __init__(self, x=0, y=0): self.x = x self.y = ya = Point()a.z = 1 # 报错，只允许对x,y赋值 定义@property以及@setter 方法，第一个将方法当作属性来用，第二个将这个方法当作属性来赋值。 1234567891011121314151617181920212223242526class Exam(object): def __init__(self, score): self._score = score @property def score(self): return self._score @score.setter def score(self, val): if val &lt; 0: self._score = 0 elif val &gt; 100: self._score = 100 else: self._score = val&gt;&gt;&gt; e = Exam(60)&gt;&gt;&gt; e.score60&gt;&gt;&gt; e.score = 90&gt;&gt;&gt; e.score90&gt;&gt;&gt; e.score = 200&gt;&gt;&gt; e.score100 super():当使用子类与夫类方法相同时会发生覆盖，如果希望保留父类则调用super方法。 12345678910class Animal(object): def __init__(self, name): self.name = name def greet(self): print（111）class Dog(Animal): def greet(self): super().greet() print 'WangWang...' 使用元类：元类主要用来拦截类的创建，修改类的定义 1234567891011121314151617class PrefixMetaclass(type): def __new__(cls, name, bases, attrs): # 给所有属性和方法前面加上前缀 my_ _attrs = (('my_' + name, value) for name, value in attrs.items()) _attrs = dict((name, value) for name, value in _attrs) # 转化为字典 _attrs['echo'] = lambda self, phrase: phrase # 增加了一个 echo 方法 return type.__new__(cls, name, bases, _attrs)class Foo(metaclass=PrefixMetaclass): name = 'foo' def bar(self): print 'bar'class Bar(Foo): prop = 'bar' 创建迭代器： 123456789101112class Fib(object): def __init__(self): self.a, self.b = 0, 1 # 返回迭代器对象本身 def __iter__(self): return self # 返回容器下一个元素 def __next__(self): self.a, self.b = self.b, self.a + self.b return self.a __iter__() 创建迭代器，__next__()每次迭代均调用该方法取得迭代值。 创建生成器： 1234567891011&gt;&gt;&gt; def fib():... a, b = 0, 1... while True:... a, b = b, a + b... yield a...&gt;&gt;&gt; f = fib()&gt;&gt;&gt; for item in f: # 每次执行到yield返回一个值并停止，第二次调用f.next()时冲yield处开始执行... if item &gt; 10:... break... print item Python OS模块123456789101112131415import osfor dir in os.listdir('./'): # 当前路径下的所有文件 print(dir)os.path.abspath('.') # 得到绝对路径os.path.dirname('file.txt') # 获取当前文件的父目录os.path.basename('./path/to/file.txt') # 输出file.txt，得到文件名os.path.splitext('afile.txt') # 输出(afile,txt),分离文件名和扩展名os.path.split('/path/file.txt')# (path,file.txt)，分离目录与文件os.path.isfile/os.path.isdir() #判断是否是目录或文件##遍历目录for root, dirs, files in os.walk('/Users/ethan/coding'): print root print dirs print files python zip函数123a = [1,2,3]b = [4,5,6]zipped = zip(a,b)#[(1,4),(2,5),(3,6)] print 重定向12345with open('afile.txt','w') as f: a = 'this is a string' b = 11 print &gt;&gt; a,b## 重定向将a,b输入afile.txt 中 sys.stdout 标准输出12sys.stdout.write('&#123;&#125;/&#123;&#125;\r'.format(step,len(lines)))# 控制台输出sys.stdout.flush() # 将控制台输出的抹掉 xlsx文件读取123456789101112131415161718import xlrdXLSX_PATH = './video_id.xlsx'workbook = xlrd.open_workbook(XLSX_PATH)print(workbook.sheet_names()) #得到所有表的表名id_list = []for sheet in workbook.sheet_names(): booksheet = workbook.sheet_by_name(sheet) # 根据表名得到表 col = booksheet.col_values(0)[1:] # 得到表的第一列 id_list += col print('sheet name: '+ sheet) print(col)print('total account:' +str(len(id_list)))from_slsx_get_video(id_list) progressbar 进度条的使用1234from progress import *progress = ProgressBar()for i in progress(range(1000)): pass python enumerate使用12for i,label in enumerate(labels): print(i,label) python argsort()argsort是numpy的一个函数，这个函数的作用是返回从小到大排序后的元素下标。 1234import numpy as npa = np.array([1,2,4,-1])sort_index = np.argsort(a)a = a[b] # 进行排序 numpy cumsum()cumsum()这个函数用来对数组依次累加。 123import numpy as npa = np.array([1,2,3])b = np.cumsum(a) # b = [1,3,6] numpy maximum()这个函数的输入为两个数组，然后生成一个数组，每个位置上为这两个数组中较大的那个。 1234import numpy as npa = np.array([1,2,3])b = np.array([2,2,2])c = np.maximum(a,b) # c = [2,2,3] python 排序算法123a = [3,2,4]a.sort() # 输出为空，直接改变asorted(a) # 输出排序后的结果，但不改变a Python 中的序列序列是python 中最基本的数据结构。序列对象均可以进行索引，分片，迭代，加，乘操作，可以用in判断元素是否存在序列中。其中list，tuple，str都属于序列。 list 列表list是可修改的一个变量，可以对他进行任意的修改。可以使用list()函数，对str字符串，和tuple进行转化成list。下面对list的各种函数进行讲解： index1234# index 用于从列表中寻找第一个出现元素的下标nums = [1,2,3,4,5,6,7]nums.index(2)nums.index(9) # 如果找不到则抛出异常 count12# 用于计算一个元素出现的个数nums.count(1) append123# 用于在元素末尾增加元素nums.append(8)nums.append([9,8]) # 将[9,8]作为一个整体加入 nums = [1,2,..,[8,9]] extend1234567# 将list进行融合a = [1,2,3]b = [4,5,6]a.extend(b) # a = [1,2,3,4,5,6]## extend 元素不允许直接添加一个元素a.extend(3) # 报错a.extend([3]) insert123#insert(pos,val)a = [1,2,3]a.insert(1,4) # a = [1,4,2,3] pop1234# 用于移除list中的元素，默认是最后一个,返回值为移除的数a = [1,2,3,4]a.pop() # a = [1,2,3]a.pop(1) # a = [1,3] remove1234# remove(val) 移除list中值为val的元素a = [1,2,2,3,3,4]a.remove(2) # 移除第一个相同的，a = [1,2,3,3,4]a.remove(8) # 若不在list中，则抛出异常 reverse123# 反转数组a = [1,2,3]a.reverse() # a = [3,2,1] sort12345678910111213141516# 该方法直接对list进行排序，修改list的值a = [3,1,2]a.sort() # 直接修改a, a = [1,2,3]a.sort(reverse=True) # 反向排序# 此外可以指定key，进行一些多列的排序student_tuples = [ ('john', 'A', 15), ('jane', 'B', 12), ('dave', 'B', 10),]sorted(student_tuples,key=lambda student:student[2])# cmp 指定函数def compare(x,y): return x-ysorted(alist,cmp = compare) sorted123# 该方法不是list的方法，返回值为排序结果，不改变aa = [3,1,2]sorted(a) # 返回值为[1,2,3]，a不变 tuple元组是一种不可变的序列，不可对tuple进行修改，它用()来表示。 1234a = (1,2,3)b = (1,) # 当仅有一个元素的时候，必须叫上一个逗号c =() # 空元组tuple # 可以进行索引分片，与正常的序列相同 字符串字符串是一种序列，满足索引，分片，加法，乘法等操作，并且字符串也是不可变的变量。 find12345# find函数用于找字符串中的子串的位置astr = 'this is a dog'astr.find('is') # 返回第一个子串出现的位置astr.find('is',4) # 指定起始位置astr.find('is',4,7) # 指定起始和结束位置 split123# split 指定一个分割符对字符串进行分割a = 'a,b,c,d'a.split(',') # 返回一个list数组 join123# join 函数类似于split的逆函数','.join(['1','2','3']) # 得到一个字符串：'1,2,3'''.join(['a','b','c']) # 得到一个字符串：‘abc’ strip12345# 用于删除左右两边的空格a = ' sdssd 'a.strip() # a = 'sdssd'a = '##sadsd sasd%%%'a.strip('#%') # 删除左右两边的#与% replace123# 用于体会匹配项a = 'this is a dog'a.replace('is','isnt') # a = 'this isnt a dog' lower/upper123# 返回字符变大或者变小的结果a = 'ABC'a.lower() # 放回abc，但是a仍然不变 dict 字典dict是有key-value组成的一个类型。 创建，遍历字典 1234567adict = &#123;&#125;adict['a'] = 1# 遍历for k in adict: print(k,adict[k])for k in adict.keys(): print(k,adict[k]) 判断元素是否在字典中 12345d = &#123;&#125;d['a'] = 1d['b'] = 2if 'b' in d: print('b is a key') clear1d.clear() # 清空所有项 copy12345# 浅复制d2 = d1.copy() # 对d2的改变同样也会改变d1# 深复制，生成许多独立的样本from copy import deepcopyd2 = deepcopy(d1) # d2与d1无关 get123#访问字典中的元素d.get('key_val') # 返回值，如果没有的话返回Noned.get('key_val'，；'default val') # 如果无，放回default val update1234#将两个字典进行相加a = &#123;'a':1&#125;b = &#123;'b':2&#125;b.update(a) # b = &#123;'a':1,'b':2&#125; Items,keys,values12345678#items将dict项以list的方式返回，keys将key以list的方式返回d = &#123;'a':1,'b':2,'c':3&#125;for k ,v in d.items(): passfor k in d.keys(): passfor v in d.values(): pass pop1234#删除keyd = &#123;'a':1,'b':2&#125;d.pop('a') # 返回a的val 1d.popitem() # 随机删除掉一对键值对 对字典进行排序1234student = [&#123;'name': 'john', 'score': 'B', 'age': 15&#125;, &#123;'name': 'jane', 'score': 'A', 'age': 12&#125;, &#123;'name': 'dave', 'score': 'B', 'age': 10&#125;]sorted(student,key = lambda stu:stu['age']) setset是一个元素不重合的集合。 123456a = set()a.add('0') # 添加元素#遍历集合for e in a: print(e)e.remove('0') # 删除元素 交集，并集，差集1234567s1 = &#123;1,2,3&#125;s2 = &#123;3,4,5&#125;s3 = s1&amp;s2 # 交集，s3 = &#123;3&#125;s4 = s1|s2 # 并集，s4 = &#123;1,2,3,4,5&#125;s5 = s1 - s2 # 差集，s5 = &#123;1,2&#125;# 判断是否是子集s1.issubset(s2) # s1是否是s2的子集 参数组合12345def func(x, y, z=0, *args, **kwargs): pass# 其中x,y为必须传入的参数，z默认参数，# *args 接受无限制的值参数，变为一个list#**kwargs 接受键值参数，最后变成一个dict map1234def square(x): return xa = [1,2,3]map(square,a) # 返回值为[1,4,9] reduce1reduce(lambda x, y: x * y, [1, 2, 3, 4]) # 相当于 ((1 * 2) * 3) * 4 filter1filter(lambda x: x &lt; 'g', 'hijack') # 返回 ac 装饰器12345678def makeitalic(func): def wrapped(): return "&lt;i&gt;" + func() + "&lt;/i&gt;" return wrapped@makeitalicdef hello(): return 'hello world' 即调用hello的时候会提前调用makeitalic，对hello进行装饰。 pdb python调试工具pdb是调试代码的一个工具包，主要特性包括设置断点、单步调试、进入函数调试、查看当前代码、查看栈片段、动态改变变量的值等。 1234567import pdb a = "aaa"pdb.set_trace() b = "bbb"c = "ccc"final = a + b + c print final 代码在set_trace()处进入暂停，输入n + enter进入下一行，下一次敲回车将重复上一个操作。输入q退出程序。在控制台允许执行print等代码来获取结果。 查看当前位置前后11行的代码。 1l 查看当前所有的代码。 1ll 添加断点： 12b line-numbertbreak line-number # 添加临时断点 清除断点： 12cl # 清除所有断点cl line-number # 清除该行断点 打印变量： 1p expression 逐行调试： 123456n 下一条s 下一行，能进入函数题r 跳过函数体c 跳到下一个断点unt line-number 一直执行到line-numbera 查看函数参数 关于python的相对导入问题python包导入的时候不同的层级关系可以使用.. 或. 来表示上一层目录和当前目录。这种层级关系是通过module中__name__字段来定义的，如下： 123456package/ __init__.py subpackage1/ __init__.py moduleX.py moduleA.py 在这个package的同级目录中调用moduleX.py文件时，该文件__name__就.package.subpackage1.moduleX，因此该moduleX反过来去调用moduleA，可以写作from .. import moduleA 。但是如果在同一个文件目录下执行脚本的话，该文件夹下就会变成top-level script，name就变成了__main__，因此层级结构就会失效。 因此含有这些层级结构的脚本，不允许直接运行，而是需要由外层的文件来间接调用。 python 捕获异常1234567from traceback import print_exctry: if something wrongexcept Exception, e: print 'type is:', e.__class__.__name__ print_exc() # print "exception happened!"]]></content>
      <tags>
        <tag>tip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Faster RCNN 复现]]></title>
    <url>%2F2019%2F03%2F16%2FFaster-RCNN-%E5%A4%8D%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[Faster RCNN是目标检测领域的一个benchmark，具有很好的借鉴意义。Faster RCNN详解介绍了Faster RCNN的网络结构，检测流程，以及一些训练过程等，接下来主要想通过github上的repo来复现一下论文，并在自己的数据集上跑一下结果。 准备环节.so文件：.so文件是Linux下共享库文件，也是ELF格式文件。类似于DLL。.so文件能够节约资源，加快代码速度，方便代码的升级。 .o文件：目标文件,相当于windows中的.obj文件 .a文件：静态库,是好多个.o合在一起,用于静态连接 其中这些共享链接文件与操作系统相关，换一个系统时需要重新生成。 pycocotools:这个文件库的作用是操纵coco数据集的一些api。安装方法如下： 1pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI' 环境的配置参考tensorpack的 readme。将所有环境配置好，以及数据集的格式。 COCO train2017数据标注格式：整个json共有一下几个字段： 1234567&#123; "info": info, "licenses": [license], "images": [image], "annotations": [annotation], "categories": [category]&#125; 其中info,licenses字段表示一些数据集以及证书信息。 images字段表示图片路径信息，有以下几个字段： 1234567891011"images": [ &#123; "license": 4, "file_name": "000000397133.jpg", "coco_url": "/val2017/000000397133.jpg", "height": 427, "width": 640, "date_captured": "2013-11-14 17:02:52", "flickr_url": url, "id": 397133 &#125;, 关键字段为coco_url，即为图片的路径名，id:与annotations字段image_id相对应的一个id。 annotation字段包含以下内容： 123456789annotation&#123; "id": int, "image_id": int, "category_id": int, "segmentation": RLE or [polygon], "area": float, "bbox": [x,y,width,height], "iscrowd": 0 or 1,&#125; image_id:与images字段中id对应，找到图片的真实路径 category_id：images box中的类别信息 segmentation：mask的区域，即多边形区域 bbox：目标boundding box[top left x position, top left y position, width, height] iscrowd：0 or 1，0表示segmentation为RLE格式，1表示其为polygon格式。 将原有标准修改为COCO格式由于源码中大量使用道其他字段，因此基本上都需要补充完整。 源码解析对于大部分的源码思路都可以视为： 准备数据，配置网络，设置holder 设置权重和bias 搭建网络 设置损失函数，设置优化器 step by step训练网络 参数配置1234567import argparseparser = argparse.ArgumentParser()parser.add_argument('--logdir', help='log directory', default='train_log/maskrcnn')...args = parser.parse_args()## 使用print(args.logdir) COCO数据集中有81类，我们使用的数据集仅有两类，因此需要对数据集部分进行修改。]]></content>
      <categories>
        <category>论文复现</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk,grep 学习]]></title>
    <url>%2F2019%2F03%2F16%2Fawk%2Cgrep-%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[awk是一个文本解释型语言，在文本处理领域十分的常用。awk的典型用途如： 文本处理 执行算术运算 执行字符串操作 awk的工作流：awk的工作流十分简单：读取-&gt;执行-&gt;重复 read：从标准输入流中读取一行 execute：所有awk执行对文本中每一行都执行处理 repeat：处理过程不断重复，直到文件到达结尾 awk命令行awk的命令行格式为： 1awk [option] afile.py 我们可以使用单引号来指定awk命令，例如： 1awk '&#123;print&#125;' afile.py 以下语句输出数据中第三列和第四列： 1awk -F , '&#123;print $3 "\t" $4&#125;' marks.txt 其中-F设置分割符为,，awk默认分割符号为空格。在使用程序语句如print时，需要加上大括号。其中$3,$4表示数据中的第3列和第4列。$0表示一整行都输出。 以下语句输出匹配字符(不指定输出则输出一整行)： 12awk '/a/' aw.txt # 输出含有a的一整行awk '/a/ &#123;print $1 $2&#125;' aw.txt # 输出含有a的行的1,2列 重定向输出：即将awk的输出，输出到文件中： 1awk '/a/' aw.txt &gt;&gt; new.txt 不显示重复行： 1awk -F , '!seen[$1]++' aw.txt 其中seen可以看成一个字典dict，当没有这个值的时候!seen[$1] == 0,因此允许输出。但当这个值以及存在时，则不输出。 单引号内可以使用各种判断语句： 1awk -F , '$1&gt;$2 &#123;print $0&#125;' aw.txt 例子： 1234567891011121314151617#以空格为分割符，输出log.txt 文件中的每一行的第1和第4个数（从1开始）$ awk '&#123;print $1,$4&#125;' log.txt# 指定 , 为分割符（-F），将字符串分割$ awk -F, '&#123;print $1,$2&#125;' log.txt# 多个分割符，先用 空格后用 ,$ awk -F '[ ,]' '&#123;print $1,$2,$5&#125;' log.txt# 设置变量 -v$ awk -va=1 -vb=s '&#123;print $1,$1+a,$1b&#125;' log.txt# 过滤出第一列大于2的数$ awk '$1&gt;2' log.txt # CSV_PATH为输入，TRAIN_PATH为输出 ,-v 为定义变量awk -v min_area=$&#123;MIN_AREA&#125; -F ',' '&#123; area=(($4-$2)*($5-$3)); if(area&gt;min_area)&#123; print $0; &#125;&#125;'&lt;$&#123;CSV_PATH&#125; &gt;&gt; $&#123;TRAIN_PATH&#125; awk参考链接 grepgrep是类unix系统中执行正则表达式的命令 ,下面是grep使用的15个场景： 下面语句判断文件中是否含有搜索的内容： 12grep 'tf' afile.pycat afile.py |grep 'tf' 从多个文件中查找指定字符： 123# 文件夹有 demo_1.txt,demo_2 文件grep 'this' demo_1.txt demo_2.txtgrep 'this' demo_* 忽略大小写(-i)： 1grep -i 'The' demo.txt 在文件中匹配正则表达式： 1grep 'a*b' demo.txt grep -w 完全匹配 12grep -w 'ab' demo.txtgrep -iw 'ab' demo.txt # 不区分大小写 grep显示匹配出的前后几行 123grep -A 3 'a' demo.txt # 显示a出现的行，以及后三行grep -B 3 'a' demo.txt # 显示a出现的行，以及前三行grep -C 3 'a' demo.txt # 显示a出现的行，以及上下三行 用GREP_OPTIONS来让查找的项醒目 1export GREP_OPTIONS='--color=auto' GREP_COLOR='100;8' 用grep -r来搜索所有的文件及子目录 1grep -r 'file' * 显示不匹配的项 1grep -v 'match' demo.txt 匹配多个项 12grep -e '1' -e 'a' -e 'q' demo.txtgrep -v -e '1' -e 'q' demo.txt # 输出一个都不匹配的项 计算匹配的项 12grep -c 'a' aw.txtgrep -v -c 'a' aw.txt # 不匹配的项 显示匹配的文件名: 1grep -l 'a' a* #输出a开头且匹配的文件名 只显示匹配的字符串： 1grep -o 'a.*b' aw.txt # 而不是显示一行 显示匹配字符的行号: 1grep -n 'a' aw.txt 显示匹配字符的字节位置： 1grep -b 'a' aw.txt]]></content>
      <categories>
        <category>tool</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Vim 学习]]></title>
    <url>%2F2019%2F03%2F13%2FVim-%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[Vim 简介Vim是在Linux环境下的一种强大的文本编辑工具，之所以学习它，是由于在服务器上写代码需要直接在服务器上操作，不像windows上有那种简单课操作的编辑器，如sublime等等。 Vim 模式Vim与大多数文本编辑器不同，它的默认模式为移动光标，删除文本等，而不是大多数编辑器那样直接为插入模式。 普通模式： 普通模式能进行的操作如移动光标，删除文本等。 删除指令： dd ：删除当前行 d+ 上下左右移动指令，分别表示删除上一行，左一个，下一行，或右一个 2dd: 删除两行 插入模式： 在普通模式按i，或a进入插入模式，ESC推出插入模式。 可视模式： 这个模式类似与普通模式，对样本有高亮 命令行模式： 在命令行模式中可以输入会被解释成并执行的文本。例如执行命令（:键），搜索（/和?键）或者过滤命令（!键） 游标使用：h，j，k，l：分别表示上下左右移动 w: 向下一个单词，b: 向上一个单词 进入插入模式i : 当前光标出插入 A: 当前光标所在行最后一个位置插入 o: 当前光标的下一行插入 a: 当前光标的后一个位置 退出Vim模式:x: 保存并退出，等同于:wq :q!: 强制退出，不保存 :q: 退出不保存 shift + zz: 退出并保存 删除文本普通模式下的删除文本操作。 x: 删除当前光标处的一个字符 X:删除光标前一个字符 dd: 删除整行 dw: 删除整个单词 D: 删除至句尾 d^: 删除至句首 dG: 删除至文章末尾 d1G: 删除至文章开头 重复执行上次命令普通模式下.表示重复执行上一次命令。执行相同执行操作代码：N&lt;command&gt;,如10x,20dd,d5w。 显示行号: :set nu 行间跳跃NG: 游标跳到第N行 gg: 游标跳到第一行 G: 游标跳到最后一行 ctrl + o: 回到上一次跳转的位置 行内跳跃w: 跳到下一个单词的开头 e: 当前单词的结尾 0: 跳到行头 $: 跳到行尾 f 字母：向后搜索字母并跳到第一个该字母的位置 F 字母：向前搜索字母，第一个字母位置 t 字母： 向后搜索字母，并跳到这个字母的前一个数 T 字母： 向前搜索字母，并跳到其后一个数 ~: 将字母大小写转换 文本的复制yy: 复制游标所在的整行 3yy：复制3行 y0: 复制到行首 y$: 复制到行尾 字符替换及坐标操作r + &lt;替换字母&gt;: 替换掉光标所在位置的字母 R： 连续替换，知道按下esc cc: 删掉这一行，换为插入模式 cw: 删掉一个词然后进入插入模式 C: 删除光标位置一直到行末，进入插入模式 u: 撤销当前操作 U: 撤销当前所有操作 指令替换1%s/imgs/car_openimg\/imgs # 使用car_openimg/imgs替换imgs 缩进shift + &gt;: 向右缩进 shift + &lt;: 向左缩进 查找/ + word: 表示查找word，输入n或N查找下一个位置 ? + word: 与上相同，只不过查找方向不同 \*: 查找游标所在位置的单词 g\*： 查找部分符合要求的单词 视窗:new: 新建视窗 :close: 关闭视窗 :q: 同上 执行外部shell命令:! command:执行外部shell 命令 :w filename: 将当前编辑的文件另存为filename vim 确认当前的括号： shift + e: 光标跳到当前内容的第一个框 重复上次操作：小数点 . VIM 打开多个文件12vim 1.py 2.py:bn ## 切换 Vim 行移动1dd,k,p k为向上移动，当移动到合适的位置时用p粘贴。 tabe 多标签切换:tabe a.txt： 打开a.txt 文件 gt: 在多标签中切换 :tabc 关闭标签，或:x等 vim 跳转到变量或函数的定义处12[ ,ctrl+i #跳转ctrl + o : #跳转回来 查找鼠标所在位置的字符：gd]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>tip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux trick]]></title>
    <url>%2F2019%2F03%2F13%2Flinux-trick%2F</url>
    <content type="text"><![CDATA[pickle 文件pickle文件的解释如下： It is used for serializing and de-serializing a Python object structure. Any object in python can be pickled so that it can be saved on disk. 即用来将python对象序列化后存放在磁盘上的一个工具包。 Pickling is a way to convert a python object (list, dict, etc.) into a character stream. pickle主要有两个功能，dump 以及load： pickle has two main methods. The first one is dump, which dumps an object to a file object and the second one is load, which loads an object from a file object. dump用来将dict，list等等保存成pickle文件： 12345import picklea = &#123;'a':1,'b':2,'c':3&#125;file = open('pickleObject.pickle','wb')pickle.dump(a,file)file.close() load用来将pickle文件读出来，还原成python object 1234import picklefile = open('pickleObject.pickle','rb')b = pickle.load(file) # b == a is a dictprint(b['a']) linux 操作： 管道命令? 查看文件夹下文件个数： ls -l |grep &quot;^-&quot;|wc -l 写txt文件： 12345a = &#123;'a':1,'b':2,'c':3&#125;f = open('a.txt','w')for key in a.keys(): f.write(key+'\n')f.close() 读txt文件： 123f = open('a.txt','r')for line in f.readlines(): print(line,end = '') assert语句： 在发生错误时让算法崩溃。其用法如下： 1assert expression,'报错语句' 等价于： 12if not expression: raise AssertionError 例子： 1assert type(a_str)== str 读当前文件夹下的文件名： 123import osfor path in os.listdir('./'): print(path) 复制：将文件file1复制到dir1下： 1cp file1 dir1 python 找到最后一个.的位置： 1str.rfind('.', beg=0 end=len(str)) python set操作set 中保存不重复的key。 创建： 1aset = &#123;'apple','orange','pea'&#125; 判断set中是否含有key： 12if 'apple' in aset: pass set集合的交并集操作： 1234a-b # a中包含而b中不包含的元素a|b # a与b的元素并集a&amp;b # a与b中元素的交集a^b # 不同时包含于a与b中元素 Set 删除操作： 1aset.remove('apple') Linux 复制文件夹： 1cp -r dirname . Vim 撤销：u python 在指定文件位置处添加字符后重新保存。 1234567f1.open('a.txt','r')content = f1.read()pos = content.find('word')content = content[:word+4]+'add something'+content[word+4:]f1.close()f2 = open('a.txt','w')f2.write(content) shell 语言用shell写成的文件通常被保存为.sh后缀。被称为脚本Bash的应用程序。 可以理解为在linux电脑上的一系列系统操作，比如下载文件，进入某个文件夹，下载某个文件等等。即可以通过shell程序来指挥kernel，让系统达成我们需要的硬件任务。 示例： 123456789#!/bin/bash# Program:# This program shows "Hello World!" in your screen.# History:# 2015/07/16 VBird First releasePATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin:~/binexport PATHecho -e "Hello World! \a \n"exit 0 第一行：#!/bin/bash 作用为宣告这个档案内的语法使用bash的语法，所有的sh文件必须有这一句。其他的# 则表示注释作用。 # 号注释部分：建议你一定要养成说明该script的习惯：1.内容与功能； 2.版本资讯； 3.作者与联络方式； 4.建档日期；5.历史纪录等等 PATH部分为主要的环境变量，用来保存当前的路径信息。 echo那一句为程序的主要执行部分。 使用sh hello.sh 来执行代码。 shell变量： 变量名不加美元符号，而且变量名和等号之间不能有空格。使用变量是在变量名之前加上美元符： 12your_name='zhouwh'echo $your_name 定义只读变量： 123#!/bin/bashmyUrl="http://www.google.com"readonly myUrl # 之后无法修改 删除变量： 1unset variable_name Shell 字符串： 12str='单引号，双引号，不用都行'str="this is \" $your_name \"" #需要转义的情况，双引号里头允许出现 字符串拼接：直接使用双引号即可： 123# 使用双引号拼接greeting="hello, "$your_name" !"echo $greeting 获取字符串长度： 123string="abcd"echo $&#123;#string&#125; #输出 4echo $&#123;string:1:3&#125; # 输出 bcd 子串 控制语句： 12345678910111213141516171819# if 语句if conditionthen command1 command2 ...else commandN fi# for 语句for loop in 1 2 3 4 5do echo "The value is: $loop"done#while语句while conditiondo commanddone awk 语句：awk是一个强大的文本分析工具，简单来说awk就是把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理。其基本的用法如下： 1awk '&#123;[pattern] action&#125;' &#123;filenames&#125; 例子： 1234567891011121314151617181920212223#以空格为分割符，输出log.txt 文件中的每一行的第1和第4个数（从1开始）$ awk '&#123;print $1,$4&#125;' log.txt# 指定 , 为分割符（-F），将字符串分割$ awk -F, '&#123;print $1,$2&#125;' log.txt# 多个分割符，先用 空格后用 ,$ awk -F '[ ,]' '&#123;print $1,$2,$5&#125;' log.txt# 设置变量 -v$ awk -va=1 -vb=s '&#123;print $1,$1+a,$1b&#125;' log.txt# 过滤出第一列大于2的数$ awk '$1&gt;2' log.txt # CSV_PATH为输入，TRAIN_PATH为输出awk -v min_area=$&#123;MIN_AREA&#125; -F ',' '&#123; area=(($4-$2)*($5-$3)); if(area&gt;min_area)&#123; print $0; &#125;&#125;'&lt;$&#123;CSV_PATH&#125; &gt;&gt; $&#123;TRAIN_PATH&#125;## 删除第一列awk '&#123;$1="";print $0&#125;' file.txt## 删除第一行awk -F '\t' 'NR==1&#123;next&#125; &#123;print $0&#125;' # 当NR==1时跳过 CSV 文件（Comma Separated Values file，即逗号分隔值文件）为一种纯文本文件。 python 读取csv文件： 1234567import csvwith open('stocks.csv') as f: f_csv = csv.reader(f) headers = next(f_csv) for row in f_csv: # Process row ... python保存csv文件： 1234import csvwith open('some.csv', 'w', newline='') as f: writer = csv.writer(f) writer.writerow([1,2,3,4]) python dict合并： 1merge_dict = dict(dict1.items()+dict2.items()) Python virtualenvvirtualenv`创建一个拥有自己安装目录的环境, 这个环境不与其他虚拟环境共享库, 能够方便的管理python版本和管理python库。 安装： 1pip install virtualenv 创建新环境： 1virtualenv zhou_env 激活： 1source ./bin/activate 退出虚拟环境： 1deactivate matplotlib 画图123456789101112131415161718192021222324import matplotlib.pyplot as pltimport numpy as npimport matplotlibdata = np.array([1,2,3,4,5,6,7,8,1,1,1,1,1])print(type(data))"""绘制直方图data:必选参数，绘图数据bins:直方图的长条形数目，可选项，默认为10normed:是否将得到的直方图向量归一化，可选项，默认为0，代表不归一化，显示频数。normed=1，表示归一化，显示频率。facecolor:长条形的颜色edgecolor:长条形边框的颜色alpha:透明度"""plt.hist(data, bins=40, normed=0, facecolor="blue", edgecolor="black", alpha=0.7)# 显示横轴标签plt.xlabel("区间")# 显示纵轴标签plt.ylabel("count")# 显示图标题plt.title("bbox—count")plt.savefig('aimg.jpg')plt.show() 文件传输rz: 在iterm2中输入rz指令，将会出现一个窗口选择文件，开始上传到当前文件夹。 sz filename: iterm2 弹出一个窗口，选择保存文件的地址。 vim 隐藏到后台：ctrl + z 命令行调出vim：fg linux看文件大小： 12ls -lshdu -sh * 打包文件压缩：tar czvf file.tar ./filename,czvf表示create zip view file解压缩：tar xzvf file.tar，xzvf表示extract zip view file 过滤数据集中不合适的记录 找出不合适的记录： 1cat all_box.csv | grep '不合适记录名' # grep拿cat的输出当作输入 维护一个delete_imgs.txt 文件，用来存放不合适记录 使用delete_imgs.txt 文件中不合适记录来查看all_box.csv(数据集)中不合适的记录。 1grep -f delete_imgs.txt all_box.csv 删掉不合适记录，得到合适记录的输出，将输出存成一个新的new_box 1grep -v -f delete_imgs.txt all_box.csv &gt; new_box.csv 查看剩下的合适条数： 1cat new_box.csv |wc -l # -l 行数 -w 字数 命令行输出1cat *.csv &gt; all_box.csv awk判断字段1不重复的记录数： 1awk -F ',' '!seen[$1++]' train_set.csv |wc -l linux 查看显卡运行状态： 1nvidia-smi python shuffle操作shuffle操作将数据集打乱： 123import randomlist = [1,2,3]random.shuffle(list) tmux: tmux 可以在终端软件重启后通过命令行恢复上次的 session ,即当你训练网络中断时，下次开启仍然可以重新连接。 1234567tmux ls # 列出所有的tmux会话tmux new -s zhouwenhui # 创建新的会话tmux -2 attach -t zhouwenhui # 重新进入原来的sessionctrl + b , d # 暂时退出当前会话ctrl + b , c # 新建窗口ctrl + b , w # 切换窗口exit # 退出当前窗口 Linux创建账号123useradd zhouwhpasswd zhouwhuserdel [-r] zhouwh # 删掉用户 Linux 给用户赋予root权限 vim /etc/passwd 文件，找到新创建的用户所在行，把用户ID修改为 0即可，如下。 zhouwh​：x:0:1002::/home/zhouwh:/bin/bash Linux用户切换新建的用户的目录在/home/zhouwh底下，使用如下语句可以实现自由的切换。 12su - zhouwh # 切换到zhouwh 用户su - root # 切换到root 用户 Linux 安装anaconda在anaconda上下载相应版本的安装软件，然后执行以下操作： 1234bash Anaconda3-4.4.0-Linux-x86_64.sh ## 安装echo 'export PATH="~/anaconda3/bin:$PATH"' &gt;&gt; ~/.bashrc# 环境变量conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ # 国内镜像rm -rf anaconda # 卸载 Json 的读写123456# Writing JSON data:&#123;'a': 'Runoob', 'b': 7&#125;with open('data.json', 'w') as f: json.dump(data, f)# Reading data backwith open('data.json', 'r') as f: data = json.load(f) dict to json12with open('data.json','w') as f: json.dump(data,f) 打开文件头几行/末尾几行12cat afile.txt|head -n 100cat afile.txt|tail -n 100 指定运行的GPU1CUDA_VISIBLE_DEVICES=1,2 python train.py 实时显示GPU使用率1nvidia-smi -l 1 执行程序时滚动屏幕1ctrl+b 之后 + [ .bashrc基于linux/unix的系统一般情况下都将 bash 作为默认的终端 shell。因此可以通过修改 bashrc 配置文件对执行的命令进行一些自定义。 .bashrc是一个纯文本文件，用于保存和加载不同用户的终端首选项和环境变量,bash 在每次启动时都会自动载入 bashrc 配置文件中的内容。每次修改.bashrc文件后使用source ~/.bashrc进行环境的激活。 终端首选项：最常用的一种方式为为linux系统命令定义别名，方便定制输入。 环境变量：Linux是一个多用户操作系统，可以在linux中为不同的系统定制不同的环境变量。 环境变量的设置环境变量可以分为系统环境变量和用户环境变量。 系统环境变量：系统变量的设置将对所有的用户均生效。 对/etc/profile文件添加环境变量将对所有用户均有效。例如添加CLASSPATH变量。 12vim /etc/profile export CLASSPATH=./JAVA_HOME/lib;$JAVA_HOME/jre/lib 用户环境变量在用户目录下修改文件.bash_profile,改变的量仅对该用户有效。如下： 12vim ~/.bash.profileexport CLASSPATH=./JAVA_HOME/lib;$JAVA_HOME/jre/lib 直接在命令行运行：export 变量名=变量值仅对当前的shell有效。 .bashrc,profile,.bash.profile 的区别 Linux 中常见的环境变量PATH：指定命令的搜索路径,中间用冒号隔开。 1PATH=&lt;PATH1&gt;:&lt;PATH2&gt;:&lt;PATH3&gt;:&lt;PATH4&gt; 在配置文件中修改PATH： 1export PATH='~/anaconda3/bin/:$PATH' HOME：指定用户的主工作目录 HISTSIZE：指保存历史命令记录的条数 LOGNAME：指当前用户的登录名。 HOSTNAME：指主机的名称 SHELL：指当前用户用的是哪种Shell LANG/LANGUGE：和语言相关的环境变量 Linux 查看环境变量 echo 显示某个环境变量值 echo $PATH export HELLO=”hi” 设置新的环境变量 env 显示所有环境变量 linux 创建快捷键1alias ict="ssh xxx@ictvr.ml -p 11111" Linux 打印出目录下文件决定路径1for f in 'ls cat';do echo '/data/cartoon_detect_data/'$f;done &gt; total.txt python 查看文件大小1ls -lht 下载视频runonce服务器上,sh tmp.sh,/root/cartoon_data_prepare，其中需要视频id，从表格中读取。 linux截取字符串前n个字符1cut -c1-100 file.py # 前100个字符 linux shell判断外部传入的参数12345678910#!/bin/bashif [ ! -n "$1" ] ;then # 判断是否有个参数 echo "you have not input a word!"else echo "the word you input is $1"fi## if [-e "$1"] ; then #判断传入的参数是否是个文件/目录 linux 批量更改文件名1rename 's/search/replace/;' test*.jpg linux awk复制文件1awk -F ',' '&#123;print $1&#125;' ~/zhouwenhui/mAP/test_set.csv| while read day ; do cp "$day" "./aaa"; done 创建软链接1ln -s a/path to/soft_path soft_path为软链接。 linux 批量删除文件1rm -rf PAD8_*.jpg 传输大文件mac上通过rz,sz与服务器之间传输文件，由于文件过大（百兆）容易导致内存不足。因此需要先将文件拆分后一个一个上传（下载）。 12345split -b100k ev.zip ev # 将ev.zip文件分成每个文件100k的小文件，由ev开头，如evab ...md5sum ev.zip # 查看原文件的md5值sz ev* # 将小文件依次下载cat ev* &gt; ev.zip #本地将所有小文件还原md5sum ev.zip #查看还原文件的md5值，是否之前相同 查看后台进程1ps aux 杀死进程当执行一个多线程的任务的时候，使用 ctrl+z停止进程，然后根据进程的id号，依次杀死进程。]]></content>
      <tags>
        <tag>tip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow 笔记（CNN分类器VI）]]></title>
    <url>%2F2019%2F03%2F13%2FTensorflow%20%E7%AC%94%E8%AE%B0%EF%BC%88CNN%E5%88%86%E7%B1%BB%E5%99%A8VI%EF%BC%89%2F</url>
    <content type="text"><![CDATA[CNN分类网络CNN网络在原来网络的基础上加入了卷积层，对特征进行提取后分类，能够提升网络的分类准确率，同时在网络中加入了dropout，缓解网络的过拟合。 写一个分类器的基本思路如下： Assemble graph: read data create placeholder create weight and bias in a layer create a net structure specify loss function create optimizer train model: specify the epochs,iteration,batch-size initial variables step by step run the optimizer（use feed_dict to feed data into x,y placeholder） tf.Session() encapsulates the environment 对于mnist分类来说，需要注意的是，我们将输入向量的大小设置成28*28*1的形式，然后通过卷积进行操作。如下： 12xs = tf.placeholder(tf.float32,[None,784])x_image = tf.reshape(xs,[-1,28,28,1]) # [nsample,width,height,channel] 对于卷积层tensorflow中使用tf.nn.conv2d来创建。该函数的参数如下： 12345tf.nn.conv2d(input,W,stride,padding)# input表示输入的参数# filter:W表示卷积核的参数即：[kernel_w,kernel_h,in_channel,out_channel]# stride表示步长：[1,x_move,y_move,1]# padding = 'SAME' / 'VALID' 卷积后大小不变 / 卷积后大小变小 创建一个卷积层： 1234567def conv2d(x): # conv weight: [kernel_w,kernel_h,in_channel,out_channel] Weights = tf.truncated_normal([5,5,1,32],stddev = 0.1) bias = tf.constant(0.1,[32]) # stride = [1,x_move,y_move,1] output = tf.nn.conv2d(x,Weight,[1,1,1,1],padding = 'SAME') return output+bias 对于池化层，tensorflow中使用的函数tf.nn.max_pool，该函数的参数如下： 12345tf.nn.max_pool(input,ksize,stride,padding)# input为输入的数据# ksize:表示卷积核大小，[1,kernel_w,kernel_h,1]# stride: [1,x_move,y_move,1]# padding = 'SAME' / 'VALID' 创建一个max pooling 层: 12def max_pooling(x): return tf.nn.max_pool(x,ksize=[1,2,2,1],strides = [1,2,2,1],padding = 'SAME') dropout层： 1234tf.nn.dropout(x,keep_prob = 0.5)# x: 输入的向量# keep_prob：dropout的比例# keep_prob通常是一个tf.placeholder,在训练时设为一个小数，在测试时设为1 完整代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import osos.environ['KMP_DUPLICATE_LIB_OK']='True'import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data# load datamnist = input_data.read_data_sets('MNIST',one_hot=True)# create placeholderwith tf.name_scope('input'): xs = tf.placeholder(tf.float32,[None,784])/255. ys = tf.placeholder(tf.float32,[None,10]) keep_prob = tf.placeholder(tf.float32) # dropout rate x_image = tf.reshape(xs,[-1,28,28,1]) # nsample,28,28,channelwith tf.name_scope('conv1'): Weights = tf.Variable(tf.random.truncated_normal([5,5,1,32],stddev=0.1)) bias = tf.Variable(tf.constant(0.1,shape = [32])) out_conv1 = tf.nn.conv2d(x_image,Weights,[1,1,1,1],padding='SAME')+bias # 28,28,32with tf.name_scope('pool1'): out_pool1 = tf.nn.max_pool(out_conv1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME') # 14,14,32with tf.name_scope('conv2'): Weights = tf.Variable(tf.random.truncated_normal([5,5,32,64],stddev=0.1)) bias = tf.Variable(tf.constant(0.1,shape = [64])) out_conv2 = tf.nn.conv2d(out_pool1,Weights,[1,1,1,1],padding='SAME') #[14,14,64]with tf.name_scope('pool2'): out_pool2 = tf.nn.max_pool(out_conv2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')#[7,7,64] out_pool2 = tf.reshape(out_pool2,[-1,7*7*64]) # nsample,7*7*64with tf.name_scope('fc1'): Weights = tf.Variable(tf.random.truncated_normal([7*7*64,1024],stddev=0.1)) bias = tf.Variable(tf.constant(0.1,shape = [1024])) out_fc1 = tf.nn.relu(tf.matmul(out_pool2,Weights)+bias) drop_out_fc1 = tf.nn.dropout(out_fc1,keep_prob) # nsample,1024with tf.name_scope('fc2'): Weights = tf.Variable(tf.random.truncated_normal([1024,10],stddev=0.1)) bias = tf.Variable(tf.constant(0.1,shape=[10])) prediction = tf.matmul(drop_out_fc1,Weights)+bias # nsample,10# create running environmentsess = tf.Session()## compute accuracydef compute_accuracy(X,Y): pred = sess.run(prediction,feed_dict=&#123;xs:X,keep_prob:1&#125;) correct = tf.equal(tf.argmax(pred,1),tf.argmax(Y,1)) accuracy = tf.reduce_mean(tf.cast(correct,tf.float32)) return sess.run(accuracy)# losswith tf.name_scope('loss'): loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction,labels=ys))with tf.name_scope('optimizer'): optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)#initializationinit = tf.global_variables_initializer()sess.run(init)# visualizationwriter = tf.summary.FileWriter('./log',sess.graph)# trainfor step in range(1000): x_batch,y_batch = mnist.train.next_batch(100) sess.run(optimizer,feed_dict=&#123;xs:x_batch,ys:y_batch,keep_prob:0.5&#125;) if step%50 == 0: print(compute_accuracy(mnist.test.images[:1000],mnist.test.labels[:1000])) Tensorflow 保存变量12345678910import tensorflow as tfw = tf.Variable([[1,2],[3,4]],dtype = tf.float32,name = 'weight')b = tf.Variable([[1,2,3]],dtype = tf.float32,name = 'bias')init = tf.global_variables_initializer()saver = tf.train.Saver()with tf.Session() as sess: sess.run(init) save_path = saver.save(sess, "my_net/save_net.ckpt") print("Save to path: ", save_path) 提取变量restore12345678910111213import tensorflow as tf# 先建立 W, b 的容器W = tf.Variable(np.arange(6).reshape((2, 3)), dtype=tf.float32, name="weights")b = tf.Variable(np.arange(3).reshape((1, 3)), dtype=tf.float32, name="biases")# 这里不需要初始化步骤 init= tf.initialize_all_variables()saver = tf.train.Saver()with tf.Session() as sess: # 提取变量 saver.restore(sess, "my_net/save_net.ckpt") print("weights:", sess.run(W)) print("biases:", sess.run(b))]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SSD M2Det 详解]]></title>
    <url>%2F2019%2F03%2F10%2FSSD-M2Det-%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[SSD是经典的one-stage算法，可以认为是关于类别的多尺度分类网络，作为很多one-stage网络的基础框架，有必要阅读一下。 M2Det是今年（2019）CPVR论文，基于SSD框架的扩展，M2Det 若采用 single-scale inference 可达到 11 FPS, AP 41 的准确率，采用 multi-scale inference 可达到 AP 44.2 的准确度。 SSD 详解 SSD: Single Shot MultiBox Detector submit time：2015 arxiv link 网络的背景及作用当前网络通过提取候选框等方式进行目标检测，运行速度对于一些应用场景来说太慢了。 SSD是一种使用单个深度神经网络来检测图像中的目标的方法，SSD 速度的根本改进来自消除边界框proposal和随后的像素或特征重采样阶段。他的主要特点如下： one-stage检测器，用于多个类别目标检测，比先前技术相比（YOLO）速度更快，且更准确。 SSD方法的核心是使用小卷积滤波器来预测特征图上固定的一组默认边界框的类别分数和位置偏移。 为了实现高检测精度，我们从不同尺度的特征图产生不同尺度的预测，并且通过宽高比来明确地分离预测。 这些设计特性得到了简单的端到端训练和高精度，进一步提高速度和精度的权衡，即使输入相对低分辨率图像。 网络结构 SSD的检测过程如下： SSD输入为包含类别以及真实框标记的图片数据。 卷积处理时，我们在具有不同尺度（例如（b）和（c）中的8×8和4×4）的若干特征图中的每个位置处选择不同横宽比的小集合（例如4个）默认框。 对于每个默认框，我们预测对所有对象类别（c 1，c2，…，cp）的形状偏移和置信度。在训练时，我们首先将这些默认框匹配到真实标签框。 例如，两个默认框匹配到猫和狗，这些框为正，其余视为负。 模型损失是位置损失（例如平滑L1 [6]）和置信损失（例如Softmax）之间的加权和。 SSD方法基于前馈卷积网络，采用多尺度特征度检测的方式，产生固定大小的边界框集合和框中对象类别的分数，接着是非最大化抑制步骤以产生最终检测,如下图： 输入：300x300 经过VGG-16（只到conv4_3这一层，由于更深的网络特征难以恢复） 经过几层卷积，得到多层尺寸逐渐减小的feature map 每层feature map分别做3x3卷积，每个feature map cell对应k个类别和4个bounding box offset，同时对应原图中6（或4）个anchor，即每一个位置将会预测4或6个anchor，然后每个anchor有k个类别概率值以及4个位置偏移值。 38x38层, 最后3x3层, 1x1层三个feature map的每个feature map cell只对应4个anchor，分别为宽高比: 1:1两种，1:2, 2:1两种，其他feature map的feature map cell对应6个anchor，分别为宽高比: 1:1两种，1:2, 2:1两种，1:3， 3:1两种。因此总共有$$38* 38*4+19*19*6+10*10*6+5*5*6+3*3*4+1*1*4=8732$$个anchor。 anchor的中心：每层的feature map cell对应的anchor中心的计算方法如下$$(\frac{i+0.5}{|fk|},\frac{j+0.5}{|fk|})$$其中i,j是当前的位置，fk是当前feature map的大小。 anchor缩放因子:$$S_k = S_{min}+\frac{S_{max}-S_{min}}{m-1} (k-1),k\in[1,m]$$ 缩放因子指不同大小的feature map对应不同大小的anchor。m表示最终有m个 feature maps将要作为预测,对每一个k层的feature map计算其anchor的大小，即$ S_k$。 此外$S_{min}$ 为 0.2，$S_{max}$ 为 0.9。因此对于所有层，scale都在[0.2,0.9]之间。 对于每一个尺度，都可以计算其相应的anchor大小，如下：$$\begin{align}w_k^{\alpha} &amp;= S_k \sqrt{\alpha_r} \\h_k^{\alpha} &amp;= S_k \sqrt{\alpha_r}\\\alpha \in &amp;{1,2,3,\frac{1}{2},\frac{1}{3}}\end{align}$$特别的，当 $a_r=1$ 时，增加一种 $s_k = \sqrt{s_{k-1}{s_{k+1}}}$ ，对应6种anchor的长宽比，对于4个anchor来说，不使用3和$\frac{1}{3}$。 网络损失函数SSD的损失函数由两部分组成，分别是置信度损失（softmax loss)以及位置损失（L1 loss），如下：$$L(x,c,l,g)=\frac{1}{N}(L_{conf}(x,c)+\alpha L_{loc}(x,l,g))$$其中N是匹配的GT框个数，当N = 0时loss等于0。$\alpha$是置信度与位置loss之间的一个权衡因子。 对于置信度loss ：$$L_{conf}(x,c) = - \sum_{i\in Pos}^N x_{ij}^p log(\hat{c}_{i}^p) - \sum_{i\in Neg} log(\hat{c}_{i}^0)\quad where \quad\hat{c}_{i}^p=\frac{exp^{c_{i}^p}}{\sum_p exp(c_{i}^p)}$$即softmax的交叉熵loss。 位置损失如下：$$\begin{align}L_{loc}(x,l,g)&amp;=\sum_{i\in Pos}^N \sum_{m \in {cx,cy,w,h}}x_{ij}^k smooth_{L1}(l_i^m-\hat{g}_j^m) \\\hat{g}_j^{cx}&amp;=(g_j^{cx}-d_i^{cx})/d_i^w \quad \hat{g}_j^{cy}=(g_j^{cy}-d_i^{cy})/d_i^h\\\hat{g}_j^{w}&amp;=log(\frac{g_j^w}{d_i^w})\quad \hat{g}_j^{h}=log(\frac{g_j^h}{d_i^h})\end{align}$$其中g表示GT的边框中心，d表示预测的边框的中心，该loss与Faster RCNN的loss 相同。即我们去学习的是边框的偏移量，而不是直接预测边框。当预测值l与g的指标相同时即完成，反向可推导出目标的边框。 样本选择正样本：预测框与GT的重叠程度大于0.5的认为是正样本 副样本：将边框置信度排序，找出置信度高的边框，保持正负比例为1:3. train trick SSD使用了诸如数据增强，空洞卷积等操作，是的进度进一步提升。 数据增强的方式为： 整图输入 截取图上一部分进行输入 随机crop 将上面的图片都resize到一个固定大小，输入网络 MS COCO上的精度SSD在coco上的精度如下： M2Det 详解M2Det是一个one-stage的目标检测网络。基于SSD框架扩展而来。主要的思想是Multi-Level Feature Pyramid Network(MLFPN)，多层次的特征金字塔网络。 网络的背景及应用论文中提出物体分类和物体检测问题上的缩放尺度变化矛盾，即物体分类模型提取高层次的特征，高层次的特征往往更容易学到具有辨别力的信息，模型会专注于一些辨认力强的点，例如鸟🐦，倾向于检测翅膀。 但是由于物体检测问题需要将整个物体框起来，仅仅识别出有辨别力的点无法保证完全把目标框起来，因此作者提出使用浅层的特征来辅助学习目标的检测任务，即确定边框位置。 因此高阶的特征有助于分类，低价的特征有助于物体的检测。 作者提出，通常解决尺度变化的问题采样的方法是从输入图像提取出的特征金字塔，从而克服原始图片的缩放问题。该方法可以同时用于训练和测试阶段中，相对开销较小，易于集成，适合end-to-end。本文的目的即是构造一个更加高效的金字塔模型用于检测不同缩放大小的对象。作者将该结构加入到SSD中去，取得了超过benchmark的成绩。 网络结构作者通过对比多种金字塔特征提取方式，总结了这些模型的确定，并提出自己的特征提取方式。 先前模型的缺点： 先前的模型都是基于分类网络作为特征提取的主干网络，对目标检测任务而言，先前金字塔结构提取的特征表达不足以预测目标位置。即特征太少。 每个feature map仅由主干网络的single level给出，仅含单层信息不够全面。 SSD型：使用了主干网络的最后两层，再加上4个使用stride=2卷积的下采样层构成； FPN型：也称为U型网络，经过上采样操作，然后对应融合相同的scale； STDN型：基于DenseNet的最后一个dense block，通过池化和scale-transfer操作来构建； MLFPN型：Multi-level&amp;Multi-scale MLFPN结构如下，对主干网络(vgg)提取到的特征进行融合；然后通过TUM和FFM提取更有代表性的Multi-level&amp;Mutli-scale特征；最后通过SFAM融合多级特征，得到多级特征金字塔用于最终阶段的预测。 FFMv1FFMv1整合了VGG网络中浅层conv4_3的特征以及深层conv5_3的特征作为base feature（从Figure1中可以看出）。其结构如下所示，先进行一个1*1的卷积压缩channel，然后upsample到相同的大小，进行如何得到base feature。 ###TUM TUM的结构是一个U-Net的结构，如Figure1所示。他的内部结构如下： TUM结构输出的左右feature map均输入SFAM中，同时将最大一个feature map(128,40,40)传入FFMv2中作为下一次TUM的输入。 FFMv2FFMv2输入为base feature与上一层最大的feature map结构如下： 通过堆叠TUM以及FFMv2产生不同层次的feature map，最终分别提取出图片的shallow，medium，deep的特征。每个TUM以及FFMv2的输出特征计算如下： 尺度特征聚合模块SFAMSFAM负责将每个金字塔的输入聚合起来，得到Multi-level feature pyramid。然后输出值prediction layer。 每个TUM都会输出一个六层的特征金字塔，SFAM首先对每一层相同channel的特征进行融合。第二步采用SENet的方法，即是透过 Fully-connected layer 来学习每个 feature 应该给多少权重。最终 prediction layer 会接受的是 i 个不同尺度的 Feature maps。 模块配置 M2Det 网络采用VGG-16和ResNet-101作为特征提取的主干网络。 MLFPN的默认配置包含有8个TUM，每个TUM包含5个跨步卷积核5个上采样操作，输出为6个不同scale的特征。 在检测阶段，为6组金字塔特征每组后面添加两个卷积层，以分别实现位置回归和分类。 后处理阶段，使用soft-NMS来过滤无用的包围框。 网络损失函数网络的顺势函数沿用了SSD的方法，即置信度softmax损失以及边框回归损失。]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[data training tip]]></title>
    <url>%2F2019%2F03%2F08%2Fdata%20training%20tip%2F</url>
    <content type="text"><![CDATA[Json 文件格式切换数据集中Json文件挤在一堆，需要将其格式化输出，Json文件格式化代码如下： 1234567import jsonjson.dumps(&#123;'a': 'Runoob', 'b': 7&#125;, sort_keys=False, indent=4, separators=(',', ': '))# 输出如下：#&#123;# "a": "Runoob",# "b": 7#&#125; 文件改写实现代码如下： 12345import jsonwith open('../dataset/train_round1/train_no_poly.json', 'r') as fin: js = json.load(fin) with open('train_no_poly.json', 'w+') as fout: json.dump(js, fout,sort_keys=False, indent=4, separators=(',', ': '),ensure_ascii=False) Json 处理字符串读写和文件读写： 处理字符串：json.loads(fileDir)得到字符串，json.dumps(dataDict)得到Json文件。 处理文件： 123456# Writing JSON datawith open('data.json', 'w') as f: json.dump(data, f)# Reading data backwith open('data.json', 'r') as f: data = json.load(f) MAC git 使用安装Git：链接 验证是否成功链接远程github：ssh -T git@github.com，如果正确返回 hi wenhui-zhou. 提交本地项目到GitHub上： 在GitHub网站上创建一个仓库 复制其clone 链接，将仓库clone到本地: git clone git@github.com:WenHui-Zhou/learnGit.git 打开learnGit文件夹，将工程文件保存在这个目录下 提交修改，将工程上传到GitHub上 git add fileName：在仓库目录下，将文件添加到本地仓库。 git add . ：将所有文件添加到本地仓库。 git commit -m &quot;some comments&quot;：添加评论。 git push：上传到远端仓库。 github 更新文件 git status：查看仓库状态，如果有所不同的话，会显示不同的文件。 git add file：将更改的文件加入到本地仓库。 git commit -m &quot;comment&quot;：添加评论。 git push：将代码提交到GitHub上。 git getch当与人协作时，远程主机有了更新，可以通过git fetch 来取得更新的内容。 git fetch origin master:tmp，在本地创建一个tmp分支，将远程master 分支的代码下载到tmp分支上。 git diff tmp，比较本地代码与从远端下载下来的代码的区别。 git merge tmp，合并分支到本地的master。 git branch -d tmp，如果不想要tmp分支的话，则可以删除。 git pullgit pull：将远端代码与本地代码直接融合，等于上面的git fetch + git merge。 git push将本地更新的分支推送到远程主机。因此我们每次进行远端数据的更新操作之前需要更新一下本地的分支。即： 123git add .git commit -m "comment" // 在本地分支上添加文件并添加评论git push // 将远端分支进行更新 fork在GitHub上点击fork将其他用户的仓库更新到自己的GitHub下，然后进行clone到本地，进行一些工程上的修改。这个库当前属于你，照样执行上面的git push等操作。完成后在GitHub上发起pull request，然后系统会对比两个工程的修改之处，然后发起request，在其他用户那边将会多一个request操作，可以同意，则进行merge。 同步fork的库与原始的库使用指令如下： 1234git remote add //添加本地库git fetch // 将远端的不同fetch到本地git merge // 融合git push // 更新到自己的GitHub上 iteration、epoch、batchsize的含义 epoch：数据集所有数据训练过一遍为一个epoch，类似于一本书，epoch为几就是要看几遍。 batch-size：一次迭代（更新参数）所使用的数据数量。类似于书中每个章节。 iteration：总共的迭代次数，一次迭代所用的数据为一个batch-size，即一次看一章。因此迭代的次数为dataset/batch-size，每本书看epoch次，因此iteration = epoch * (dataset/batch-size)。 python dict的用法12345678adict = &#123;'a':1,'b':2,'c':3&#125; #创建dict(zip(['a', 'b', 'c'], [1, 2, 3])) print(adict['a']) # 访问adict['a'] = 2 # 修改adict.pop('a') # 删除adict.get('a') # 如果没有这个key返回Nonefor key,values in dict.items(): # 同时获得key和val print(key,values) python 中的类12345678910111213141516class Student(object): # 继承object # 数据成员 def __init__(self,name,score): # 类函数的第一个参数固定为self self.name = name self.score = score ## 私有成员,变量名前加上两个下划线__,只能类函数内部访问 self.__name = name # 方法成员 def getname(self): return self.name def getscore(self): return self.score#创建对象stu = Student('xiaoming',100)print(stu.getname())stu.sex = 'man' # 外部添加数据变量 继承和多态12345678910111213141516171819class Animal(object): def run(): print('animal is runing')class cat(Animal): passacat = cat()acat.run() # 调用父类的run函数class cat(Animal): def run(): print('cat is runing')acat.run() # 执行自己的run函数#多态def runrun(animal): animal.run()# 调用runrun(animal) # animal类runrun(cat) # animal 子类runrun(aman) # 类中含有run()的类也可以 property属性12345678910111213class Student(object): @property def socre(self): # 把socre变成一种数据属性，对象可以直接赋值 return self._score @birth.setter def score(self, value):# 对于score赋值的规则限制，在setter里头 if value&gt;100: raise ValueError('score must between 0 ~ 100!') self._score = value# 调用s = Student()s.score = 100print(s.score) classmethod 类12345678910 class A(object): bar = 1 def func1(self): print ('foo') @classmethod def func2(cls): # 方法类必须使用cls作为参数，不需要初始化 print ('func2') print (cls.bar) cls().func1() # 调用 foo 方法A.func2() # 不需要实例化 staticmethod12345 class A(object): @staticmethod def func2(): # 静态方法，对参数没有要求 print ('func2')A.func2() # 不需要实例化 下载单个文件夹在github上进入该文件夹所在的目录，复制文件夹链接： https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN 随后在服务器上输入： 1svn checkout https://github.com/tensorpack/tensorpack/trunk/examples/FasterRCNN]]></content>
      <categories>
        <category>比赛</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YOLO V2,V3详解]]></title>
    <url>%2F2019%2F03%2F08%2FYOLO-V2-V3%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[YOLO V2,YOLO V3是基于YOLO V1 的基础上，对网络进行改进，使得mAP，检测精度提升，同时仍保持较快的检测速度。本文将详细介绍V2，V3的特点。 YOLO V2 详解 YOLO9000: Better, Faster, Stronger submit time: 2016 arxiv link YOLO V2 在保持与V1基本框架相同的情况下，对网络进行了各种调优，主要做的修改如下： batch normBatch Normalization可以提升模型收敛速度，而且可以起到一定正则化效果，降低模型的过拟合。在YOLOv2中，每个卷积层后面都添加了Batch Normalization层，并且不再使用droput。使用Batch Normalization后，YOLOv2的mAP提升了2.4%。 high resolution classifier当前大部分网络的预训练模型都是在ImageNet上224*224大小的图片上进行fintune的，YOLO V2首次用448*448大小的图片对分类网络进行fintune（10 epoch），使用高分辨率分类器后，YOLOv2的mAP提升了约4%。 Convolutional With Anchor BoxesYOLOv1直接对目标进行边框预测，由于目标的尺度变换范围很大，导致了YOLOv1在精确定位方面表现较差。YOLOv2移除了YOLOv1中的全连接层而采用了卷积和anchor boxes来预测边界框。YOLOv2借鉴了Faster R-CNN中RPN网络的先验框策略。RPN对CNN特征提取器得到的特征图（feature map）进行卷积来预测每个位置的边界框以及置信度（是否含有物体）。 YOLOv2采用 416 * 416大小的图片作为输入。下采样的总步长为 32，对于 416*416大小的图片，最终得到的特征图大小为13*13，维度是奇数，这样特征图恰好只有一个中心位置。对于一些大物体，它们中心点往往落入图片中心位置，此时使用特征图的一个中心点去预测这些物体的边界框相对容易些。YOLO V2每个cell与yolov1类似，都分别去预测目标的IoU，以及每个框的类别预测值。使用anchor之后精度有点下降，但是YOLO V2的召回率（预测为真的占GT中真的比例）大大提升。原因是使用了anchor每张图片预测的边框数大大提升。 Dimension Clusters在预测边框时传统的如Faster RCNN使用的是手工设置边框大小，yolov2中采用kmeans聚类的方法，选用box与聚类中心box之间的IOU值作为距离指标，即离的则认为是那一类，作者选择了五个先验框作为聚类中。 New Network: Darknet-19YOLOv2采用了一个新的基础模型（特征提取器），称为Darknet-19，包括19个卷积层和5个maxpooling层，使用Darknet-19之后，YOLOv2的mAP值没有显著提升，但是计算量却可以减少约33%。 Direct location predictionyolov2 采用不同于RPN的边框回归的方法，yolov2回归的目标是预测边界框中心点相对于对应cell左上角位置的相对偏移值。 yolov2为每个cell预测5个bounding box，为每个bounding box预测五个坐标值，使用如下的公式进行边框的回归。 结合聚类分析得到先验框与这种预测方法，YOLOv2的mAP值提升了约5%。 Multi-Scale Training由于YOLOv2模型中只有卷积层和池化层，为了增强模型的鲁棒性，YOLOv2采用了多尺度输入训练策略，具体来说就是在训练过程中每间隔一定的iterations之后改变模型的输入图片大小。YOLOv2的下采样总步长为32，输入图片大小选择一系列为32倍数的值. YOLO V2 训练YOLOv2的训练主要包括三个阶段。 第一阶段就是先在ImageNet分类数据集上预训练Darknet-19，此时模型输入为 224 * 224 ，共训练160个epochs 第二阶段将网络的输入调整为 448*448 ，继续在ImageNet数据集上finetune分类模型，训练10个epochs 第三个阶段就是修改Darknet-19分类模型为检测模型，并在检测数据集上继续finetune网络 其网络结构为：链接 YOLO V3 YOLOv3: An Incremental Improvement Submit time: 2018 arxiv link YOLO V3在速度和精度上比YOLO V2有了很大的提升，同时网络结构也复杂了不少，通过改变网络来权衡速度和精度。YOLO V3 的主要改进如下： Darknet-53YOLOV3作者使用了Darknet 53作为特征提取网络， Darknet 53是一个在Imagenet.做预训练的网络，YOLOV3 共有106 fully convolutional 。因此在速度上较YOLOV2慢一些。网络结构如下： Detection at three Scales 多尺度预测v3最显着的特点是它可以在三种不同的尺度上进行检测（32，16，8）。YOLO是一个完全卷积网络，通过在网络中的三个不同位置处应用1 x 1内核进行预测，每个尺度均预测三个边框，每一个边框的参数为$N ×N ×[3∗(4+1+80)] $，4表示边框的偏离值，1表示目标预测，80表示共80个类别。 多尺度的检测很好的客服了小物体的预测问题。 No more softmaxing the classes作者使用sigmoid函数代替原来的softmax。由于softmax函数存在一个先验假设，即一个物体只能属于一个类别，这种假设在COCO集合上不成立。例如一个目标同时属于person和women，因此作者选择了sigmoid。]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 笔记（分类器 III）]]></title>
    <url>%2F2019%2F03%2F08%2FTensorFlow-%E7%AC%94%E8%AE%B0%EF%BC%88%E5%88%86%E7%B1%BB%E5%99%A8-III%EF%BC%89%2F</url>
    <content type="text"><![CDATA[本篇文章详细的从头到尾实现一下mnist分类器。 综述建立一个mnist数据集的数字分类器，需要做的主要有， 从数据集中下载数据。 添加网络层（参数为，输入，输入size，输出size，激活函数）， 定义输入数据的placeholder，构建网络结构，定义层。 定义loss，优化器. 定义计算精度的函数 定义train过程，以及精度的输出过程 读取数据123import tensorflow as tffrom tf.examples.tutorials.minst import input_datamnist = input_data.read_data_sets('MNIST',one_hot = True) 添加网络层123456789def add_layer(input,in_size,out_size,activation_Function=None): Weights = tf.Variables(tf.random.normal([in_size,out_size])) bias = tf.Variables(tf.zeros([1,out_size])+0.1) Wx_add_b = tf.matmul(input,Weights)+bias if actication_Function is None: outputs = Wx_add_b else: outputs = activation_Function(Wx_add_b) return outputs 构建网络结构网络为三层网络，一个输入层，一个隐藏层，一个输出层。均为全连接。 123xs = tf.placeholder(tf.float32,[None,784])ys = tf.placeholder(tf.float32,[None,10])prediction = add_layer(xs,784,10,None)#仅有一层 Loss，优化器分类问题的损失通常选用交叉熵，优化器可以选用SGD来优化。 123loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits = prediction,labels = ys)optimizer = tf.train.GradientDescentOptimizer(0.2).minimize(loss)sess = tf.Session() 计算精度这里要算的数据是预测值与GT之间的差。数据格式为one-hot类型，因此计算步骤先判断每一行是否相等，然后去平均即可,传入的数据为测试集数据。 123456def computeAccuracy(xtest,ylabel): pred = sess.run(prediction,feed_dict=&#123;xs = xtest&#125;) correct = tf.equal(tf.argmax(pred,1),tf.argmax(ylabel,1)) accuracy = tf.reduce_mean(tf.cast(correct,tf.float32)) result = sess.run(accuracy,feed_dict=&#123;xs:xtest&#125;) return result train 过程使用batch-size SGD的方式进行训练更新： 1234567init = tf.global_variables_initializer()sess.run(init)for step in range(1000): x_batch,y_batch = mnist.train.next_batch(100) sess.run(opertimizer,feed_dict = &#123;xs:x_batch,ys:y_batch&#125;) if step%50 == 0: print(computeAccuracy(mnist.test.images,mnist.test.labels)) 整个过程完成，可以通过增加网络层，修改激活函数，learning rate等方式来测试结果。 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import osos.environ['KMP_DUPLICATE_LIB_OK']='True'"""写一个分类器，首先定义数据然后定义判别层，层包括输入，输入维度，输出维度，激活函数,权重然后是构造结构写loss写opertimizer然后开始训练"""import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data#get the datamnist = input_data.read_data_sets('MNIST',one_hot=True)# 添加层def add_layer(input,in_size,out_size,activation_Fcuntion = None): Weights = tf.Variable(tf.random.normal([in_size,out_size])) bias = tf.Variable(tf.zeros([1,out_size])+0.1) Wx_add_b = tf.matmul(input,Weights)+bias if activation_Fcuntion is None: outputs = Wx_add_b else: outputs = activation_Fcuntion(Wx_add_b) return outputs# 构造一个三层的神经网络用于mnist 的分类，分别是输入层，输出层，隐藏层#定义输入xs = tf.placeholder(tf.float32,[None,784])ys = tf.placeholder(tf.float32,[None,10])#构造层次prediction = add_layer(xs,784,10,tf.nn.leaky_relu)# lossloss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction,labels=ys)optimizer = tf.train.GradientDescentOptimizer(0.2).minimize(loss)sess = tf.Session()# 计算精度def computeLoss(xtest,ylabel): pre = sess.run(prediction,feed_dict=&#123;xs:xtest&#125;) correct_rate = tf.equal(tf.argmax(pre,1),tf.argmax(ylabel,1)) accuracy = tf.reduce_mean(tf.cast(correct_rate,tf.float32)) result = sess.run(accuracy,feed_dict=&#123;xs:xtest&#125;) return result# traininit = tf.global_variables_initializer()sess.run(init)for step in range(1000): x_batch,y_batch = mnist.train.next_batch(100) sess.run(optimizer,feed_dict=&#123;xs:x_batch,ys:y_batch&#125;) if step%50 == 0: print(computeLoss(mnist.test.images,mnist.test.labels))]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[YOLO V1 详解]]></title>
    <url>%2F2019%2F03%2F07%2FYOLO-V1-%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[YOLO 系列检测方法是不同于RCNN系列检测方法的另一种思路，其速度相比于Faster RCNN要快很多，但是精度上基于Faster RCNN框架的算法表现要更好一些，下面介绍YOLO V1. You Only Look Once:Unified, Real-Time Object Detection Submit time: 2016.5 arxiv link 网络的作用及背景YOLO在做目标检测时将任务作为一个空间上分开的目标框及其类别置信度的回归问题。单个神经网络一次计算就能够直接从整幅图象预测目标框和类别置信度（one-stage）。 YOLO的训练基于整幅图像，而且能够直接对检测任务进行优化。这个统一的模型在目标检测方面相比传统方法有多个好处。 YOLO极其之快，YOLO是一个回归问题，省去了复杂的pineline YOLO是在整幅图像上全局检测。检测过程中能够包含全局的上下文信息 YOLO学习到的是物体更加泛化的表示。在如艺术画像上，性能较优 精度上，YOLO的精度略差于最优的精度 YOLO V1的网络结构yolo的训练思路为对一张图片划分成S*S大小的网格，然后每个网格预测B的检测框，以及这些检测框的置信度（与GT的IoU程度）。每个边框包含5个变量，分别是(x,y,width,height,confidence)。其中x,y指边框的中心，confidence指置信度。 对于每一个网络，我们同时计算一下其内含每种物品类别的条件概率：$Pr(Class_i|Object)$，条件概率不受检测的边框数影响，因此对于每个网格还需要预测C个类别的条件概率（是否包含该类别的概率）。当我们测试时，将C个类别的条件概率与边框预测值执行度相乘，得到：$$Class-confidence = Pr(Class_i|Object) * IOU_{truth}^{predict}$$从而得到每个检测框各个类别的分类置信得分。这些分数就同时包含了检测框中出现某类的概率以及检测框和目标的匹配程度。 网络结构：网络一共有24个卷积层和两个全连接层。模型初始的卷积层从图像中提取特征，而全连接层则预测输出概率和坐标。 训练过程以及Loss计算YOLO中每个网格可以预测多个检测框。在训练阶段对于一个物体的预测，只分配一个预测框，这种分配是基于与GT当前的IOU最大的预测。这会导致不同检测框之间的特殊化。每一个预测都会在预测特定的尺寸、长宽比、物体种类方面有更好的表现，从而提高整体的召回率。 网络优化的loss 如下： 其中$\mathbb I_{i}^{obj}$表示网格i中出现了物体 ，$\mathbb I_{ij}^{obj}$ 表示网格i中第j个框负责预测。loss中第一项表示预测边框与GT边框中心的MSE loss，第二项表示预测边框与GT边框长宽的MSE loss，第三项表示对含有物体的网格的每个预测边框置信度的MSE loss，第四项是对不含有物体的网格的每个边框置信度的MSE loss，第五项是对每个网格含有C个类别的条件概率的MSE loss。 为了训练过程更加的稳定，使用loss从小到大，同时使用dropout和数据增强，防止过拟合。 YOLO V1 特点 YOLO 在推理时，即图片预测时预测速度非常快，只需要一次网络评估。在Pascal VOC上，每张图像上网络预测98个边界框和每个框的类别概率。 YOLO对相邻物体检测效果不好：由于每个网格只预测两个检测框并且只能用有一个类别。这种设置限制了小目标，以及密集目标的检测效果。 很难泛化到一些新的或者不寻常的长宽比的检测目标：由于模型是直接从数据中学习边框预测，因此对于一些边框不规则的情形难以检测。 损失函数对大检测框和小检测框的误差是相同对待的。一个小的误差对于一个大的检测框通常都是比较温和可以接受的，但是一个小的误差对一个小的检测框的IOU有着较大的影响。主要的误差来源就是不准确的坐标定位。 总结YOLO V1在先对图片划分S*S个网格，然后对每个网格均做2个边框的预测，以及对所有C个类别计算每个类别对象存在网格内部的条件概率。因此对每个网格检测的变量如下： 对于每一个对象，用对象中心的落在的网格来预测这个对象的边框，如下图： 最终通过最小化loss，对边框进行预测。 最终得到预测结果。]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bag of Freebies for Training Object Detection Neural Networks]]></title>
    <url>%2F2019%2F03%2F05%2FBag-of-Freebies-for-Training-Object-Detection-Neural-Networks%2F</url>
    <content type="text"><![CDATA[这是一篇关于目标检测，语义分割领域，数据预处理以及网络调参的技巧文章。这些技巧对一些强大的算法，如Faster-RCNN，YOLO的性能有很大的提升。 Bag of Freebies for Training Object Detection Neural Networkssubmit time: 2019.2arxiv link 这篇文章在没有损失网络速度的前提下，介绍了一些通用的微调方法，使得网络的性能得到了大大的提升，网络的精度得到大幅提升。 作者首先探讨数据增强方面，图像mixup的方法。随后作者探讨了在目标检测训练的pipeline，例如 learning rate scheduling, weight decay ，synchronized BatchNorm. 第三，作者探讨了将上面这些方法共同作用在一个两步或一步检测网络中所带来的性能提升。 mixup本文的研究者认识到了多目标检测任务的特殊性质有利于实现空间不变的变换，因此提出了一种用于目标检测任务的视觉相干（visually coherent）图像混合方法。使用mixup，但是beta分布选择较大a&gt;=1,b&gt;=1(而不是传统的0.2)，融合后的图片显得和现实一致。同时没有对mixup进行空间上的扭曲，使用几何形状保持的对齐方式对图片进行融合。如上图第一中传统mixup的方式作者认为仅仅是引入了一些noise。第二种mixup的方式不对图片进行distort，同时与视觉一致，数据增强效果更好。 Classification Head Label Smoothing大部分的目标检测或语义分割网络中使用的loss 是基于softmax的交叉熵loss，这种loss 鼓励检测到的目标类别为正类别为1，其他为0。softmax函数如下：$$p_i = \frac{e^{z_i}}{\sum_j e^{z_j}}$$因此loss鼓励$e^{z_i} &gt;&gt; e^{z_j},i != j$这种极端情形，十分容易发生过拟合现象。因此使用label smoothing 来缓解这一现象。具体做法如下：我们对groundtruth q进行smoothing操作，q在变换前是one hot编码形式，通过如下变换：$$q_i = (1 - \epsilon )q_i + \frac{\epsilon}{K}$$其中$\epsilon$ 是一个很小的数，完成smoothing 操作。 Data Pre-processing作者采用了一下的数据增强方式： 随机几何变换. 包括随机裁剪, 随机扩张, 随机水平翻转，随机缩放等等。 随机颜色抖动：包括亮度，色调，饱和度，对比度。 cosine learning rate decay and Warm up learning rate通常在训练过程中，学习率都是从一个较大的值开始然后在训练过程中不断减少，最常用的是 Step schedule（阶梯式衰减）。例如，训练一定的Epoch之后，学习率衰减为原来的 0.1。Step schedule 使得急剧学习率的急剧下降，造成训练不稳定的问题。因此作者选择更为平滑的 Cosine 学习率衰减策略。 Synchronized Batch Normalization在多GPU环境下，对于一些batch size很小网络，在训练的时候BN会导致一些性能的下降。这个问题可以通过同时进行BN来解决。]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MobileNet V2 详解]]></title>
    <url>%2F2019%2F03%2F04%2FMobileNet-V2-%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[MobileNet V2 是在V1的基础上做了一些结构上的调整，主要有inverted residual 以及Linear Bottlenecks的改进。使得mobileNet v2 的精度进一步提高，结构进一步合理。 MobileNetV2: Inverted Residuals and Linear Bottleneckssubmit time: 2018arxiv link mobileNets的背景及作用mobileNet V1在设计的时候使用deepwise separable conv代替传统的卷积，大大降低了模型的计算量和复杂度，但是其仍然存在以下两个缺陷： 直筒型的结构影响网络性能，后续的网络如ResNet等，在网络中重复使用图像特征能够提高网络的性能。（引入inverted residual） depthwise Convolution 导致特征退化问题：由于depthwise conv使用很小的卷积核（1x1），经过BN归一化，以及relu激活之后很容易变为0，即变成死节点,导致特征退化。（我的理解是，对于一个1x1的kernel来说，归一化过程可能会把它变成负数，然后relu激活后就会变成死节点。但是对于kernel size比较大的卷积，要使整个卷积核上的数都变成负数要难很多，因此不会有很严重的特征退化问题。）（引入linear bottlenecks）. mobileNet v2 通过引入inverted residual，将图像中的特征反复使用，提高网络的性能。对于特征退化的问题，通过linear bottleneck，去掉网络中的relu等步骤，能够缓解特征的退化。 网络结构MobileNetV2架构基于倒置的残差结构，其中快捷连接位于窄的瓶颈层之间。中间展开层使用轻量级的深度卷积作为非线性源来过滤特征。此外，我们发现为了保持表示能力，去除窄层中的非线性激活函数。 线性瓶颈的倒置残差结构：模块的输入为一个低维的压缩表示特征，首先将其扩展到高维并用轻量级depthwise conv 进行卷积。随后用线性卷积（linear conv）将特征投影回低维表示。 MobileNet v2 模型的特点： 如上图，mobileNet v2在V1基础上进行了改进。 相同点：mobileNet v2由v1发展而来，继承了深度可分卷积（depthwise seperable conv），采用深度卷积和逐点卷积来代替传统的卷积操作，使得计算量大大减小。参考链接 不同点：V2在每个DW卷积之前加入了一层PW的卷积，主要作用是用于提升特征的channel数。由于DW层无法提升feature map的通道数，于是先通过PW提升feature map的通道数，PW卷积的大小为：Mx1x1，卷积核的个数可以控制，也即为卷积后得到feature map的通道数。至于提升channel的具体原因如下： 当我们查看深层卷积层所有的d通道像素时，在这些值中编码的信息实际上位于某个流形中，这些流形结构可以嵌入到低维子空间中。ReLu在高层空间中的变换有助于增加网络的非线性。对于ReLU（Bx）激活后的非0部分，输入空间与输出空间之间的特征映射是线性变换。另一方面，当ReLU破坏通道时（relu小于0的部分），它会丢失该通道的信息。但是，如果我们有很多通道，并且激活流形中有一个结构，信息可能仍然保留在其它通道中。总而言之，以下两个特性表明感兴趣的流形区域位于较高维激活空间的低维子空间中： 如果感兴趣的流形在ReLU转换后保持非零体积，则其对应于线性转换。 只有当输入流形位于输入空间的低维子空间时，ReLU才能保留有关输入流形的完整信息。 因此我们需要先对channel通道进行升维。假设感兴趣流形是低维的，我们可以通过将线性瓶颈层插入到卷积模块中来捕获这一点。线性可以防止非线性破坏太多的信息。 Linear Bottleneck：V2 去掉了第二个 PW 的激活函数。论文作者称其为 Linear Bottleneck。这么做的原因，是因为作者认为激活函数在高维空间能够有效的增加非线性，而在低维空间时则会破坏特征，不如线性的效果好。由于第二个 PW 的主要功能就是降维，降维之后使用线性瓶颈层来获取低秩信息，防止非线性破坏太多信息。 倒置残差：V2的 shortcut 设计与ResNet相反，呈一个纺锥型，中间大两头小，因此称为倒置残差。使用倒置设计是由于其内存效率要高得多。网络将PW层得到的feature map先扩展6倍，然后通过DW卷积，与一个shortcut上来的feature map融合之后再输入PW卷积。 mobileNet的结构单元如下： 网络结构参数如下： 整体的结构如下：参考链接]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow 笔记（搭建网络-II）]]></title>
    <url>%2F2019%2F03%2F04%2FTensorflow%20%E7%AC%94%E8%AE%B0%EF%BC%88%E6%90%AD%E5%BB%BA%E7%BD%91%E7%BB%9C-II%EF%BC%89%2F</url>
    <content type="text"><![CDATA[本篇文章主要讲网络搭建过程中的代码以及注意要点。 添加网络层定义网络结构，然后将网络层添加到神经网络中。定义网络层的主要步骤有： 确定网络的参数：输入，输入的size，输出的size，激励函数 定义weight，biases 计算output 123456789def add_layer(input,in_size,out_size,activation_function = None): Weights = tf.Variable(tf.random.normal([in_size,out_size])) biases = tf.Variable(tf.zeros([1,out_size])+0.01) Wx_plus_b = tf.matmul(input,Weights)+biases if activation_function is None: output = Wx_plus_b else: output = activation_function(Wx_bias_b) return output 可以看出来，网络层神经元的个数即为输出的outsize的大小。 搭建神经网络以下为搭建一个三层神经网络，其中输入层为1个神经元，输出层为1个神经元，隐藏层为10个神经元。搭建网络是需要完成的事情为： 定义数据，网络层中的参数维度 定义传入的参数placeholder，loss，optimizer等 值得注意的是，数据的维度变化需要十分注意 1234567891011121314151617181920212223242526import tensorflow as tfimport numpy as np# create datax_data = np.linspace(-1,1,300)[:,np.newaxis]noise = np.random.rand(x_data.shape[0],x_data.shape[1])y_GT = np.square(x_data)+0.5+noise# placeholderxs = tf.placeholder(tf.float32,[None,1]) #表示样本数，和每个样本的维度为1ys = tf.placeholder(tf.float32,[None,1])# structurel1 = add_layer(ms,1,10,tf.nn.relu)output = add_layer(l2,10,1,None)# lossloss = tf.reduce_mean(tf.reduce_sum(tf.square(output-ys),1))optimizer = tf.train.GrandientDescentOptimizer(0.1).minimize(loss)#traininit = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) for step in range(1000): sess.run(optimizer,feed_dict=&#123;xs:x_data,ys:y_GT&#125;) if step%20 == 0: print(loss,feed_dict=&#123;xs:x_data,ys:y_GT&#125;) 代码详解如下： x_data = np.linspace(-1,1,300)[:,np.newaxis]：其中np.linspace(-1,1)生成-1，到1之间的300个数。[:,np.newaxis]指将生成的数据维度提升。原来是1x300，现在是300x1，由1为变为300维，每个数据占一个维度。 xs = tf.placeholder(tf.float32,[None,1]) #表示样本数，和每个样本的维度为1：其中[None,1]有一种含义为，当你不知道样本数的时候，抓住每个样本的维度即可。 tf.reduce_sum(tf.square(output-ys),1): 其中tf.reduce_sum()这个函数为求和函数，第一个参数是一个数组，第二个参数默认则为所有数之和。第二个参数为0，则为列之和（0），第二个参数为1则为行之和（1）。tf.reduce_mean()参数含义与求和函数一致。 在写网络结构的时候，用placeholder，暂时忘记掉真实的数据，先构建好框架后，然后传入参数。 结果可视化可视化模块一般使用matplotlib.plot as plt 来绘图。 matplotlib的层次结构：matplotlib的结构类似与一个树状结构。Figure : 为层次结构中的最外层，内部可包含多张plot图像。plot图层次结构可包含的对象例如刻度线，单独的线条，图例和文本框。几乎每个“元素”都是一个Python对象。具体代码实现如下：12345678910111213import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-np.pi,np.pi,300)xsin = np.sin(x)xcos = np.cos(x)plt.subplot(221) # 表明共有2列，2行的图片要画，现在拿到第一个来画plt.plot(x,xsin) # 要画折线图，如果点很密集，就是曲线图plt.xlabel('x轴') # 所有属于这个子图的小对象，如颜色，图例，都可以修改plt.subplot(222) # 表明2列2行，现在要画第二个plt.plot(x,xcos)plt.subplot(223) # 表明2行2列，现在要画第三个plt.scatter(x,xsin) # 要画散点图plt.show() 如上，每次使用plt.subplot(xxx)交换控制的子图，非常好懂哈哈哈。 下面是搭建网络，绘制拟合图的完整代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltimport osos.environ['KMP_DUPLICATE_LIB_OK']='True'"""添加层需要考虑的因素有几个，首先输入的数据，输入的数据尺度，输出的尺度（神经元个数），激活函数"""def add_layer(input,in_size,out_size,activation_function = None): #定义权重 Weights = tf.Variable(tf.random.normal([in_size,out_size])) bias = tf.Variable(tf.zeros([1,out_size])+ 0.01) Wx_plus_b = tf.matmul(input,Weights) + bias if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) return outputs"""构建神经网络：1. 搭建一个输入层仅有一个神经元，隐藏层10个神经元，输出层1个神经元的网络2. 需要定义数据，网络层，输入输出，placeholder，loss ，optimizer"""# np.linspace(-1,1,10)[:,np.newaxis],引入新维度# create datax_data = np.linspace(-1,1,300)[:,np.newaxis]noise = np.random.rand(300,1)y_GT = np.square(x_data) + 0.5+noise# create networkxs = tf.placeholder(tf.float32,x_data.shape)ys = tf.placeholder(tf.float32,y_GT.shape)l1 = add_layer(xs,1,10,activation_function=tf.nn.relu)output = add_layer(l1,10,1,activation_function=None)# lossloss = tf.reduce_mean(tf.reduce_sum(tf.square(output-ys),1))optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)#illustrationax = plt.subplot(111)plt.scatter(x_data,y_GT)plt.ion() # 动态画图，不停止plt.show()# traininit = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) for step in range(500): sess.run(optimizer,feed_dict=&#123;xs:x_data,ys:y_GT&#125;) if step%20 == 0: print(sess.run(loss,feed_dict=&#123;xs:x_data,ys:y_GT&#125;)) predict = sess.run(output,feed_dict=&#123;xs:x_data&#125;) # plt.plot(x_data,predict) try: ax.lines.remove(lines[0]) except Exception: pass # plot the prediction lines = plt.plot(x_data, predict, 'r-', lw=5) plt.pause(0.5) TensorBoard 可视化Tensorboard 作为tensorflow网络结果可视化的一个比较好的工具，他使用tf.name_scope(&quot;name&quot;):的方式对部分元件进行整体的命名。并且支持多层的嵌套。如下例子： 123with tf.name_scope("layer"): with tf.name_scope("Weight"): Weights = tf.Variable(tf.random.normal([300,1]),name = 'W') tensorboard工作的思路是将文件写入磁盘，然后由浏览器进行访问，写入磁盘的语句如下： 12sess = tf.Session()writer = tf.summary.FileWriter('./log',sess.graph) 最后在命令行中，进入文件目录输入指令： 1tensorboard --logdir = 'log/' 随后在浏览器中输入：0.0.0.0:6006即可预览。 tensorboard还可以监控单个变量的变化情况，使用histogram直方图来显示。tf.summary.histogram代码如下： 123with tf.name_scope("Weight"): Weights = tf.Variable(tf.random.normal([300,1]),name = 'W') tf.summary.histogram(name,Weight) tensorboard看一个一维的变量，如loss，使用tf.summary.scalar 123with tf.name_scope('loss'): loss = tf.reduce.mean(tf.reduce.sum(tf.square(y-p_pred),1),1) tf.summary.scalar('loss',loss) 最后需要对所有的summary进行融合： 1merged = tf.summary.merge_all() 接下来在训练的时候更新参数,然后使用writer.add_summary(result,step)来将summary写入文件中。 12345678init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) for step in range(1000): sess.run(optimizer,feed_dict=&#123;xs:x_data,ys:y_GT&#125;) if step%20 == 0: result = sess.run(merged,feed_dict=&#123;xs:x_data,ys:y_GT&#125;) writer.add_summary(result,step) 接着使用命令行运行即可.x]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MobileNets 详解]]></title>
    <url>%2F2019%2F03%2F03%2FMobileNets-%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[mobileNets为谷歌开发的，为移动或嵌入式端视觉应用开发的一个轻量级高效模型。 MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applicationssubmit time: 2017arxiv link mobileNets的背景及作用背景：在很多CNN的是被问题中，总体趋势是使用更深层次更加复杂的模型来实现更高的精度。然后提高精度的代价往往是网络在尺度和速度的性能变差，对于一些要求时效且计算资源有限的任务这些网络难以完成。 mobileNets：本文介绍的moblieNets具有高效的网络结构和两个超参数，以便构建非常小的，快速度的模型，可以轻松匹配移动和嵌入式视觉应用的设计要求。 本文提出了一类网络结构，允许模型开发人员选择与其应用程序的资源限制（延迟，大小）相匹配的小型网络。MobileNets主要侧重于优化速度，但也能够产生小型网络，我们介绍了两个简单的全局超参数，可以在时间和准确性之间进行有效折中。 深度可分离卷积： 一个传统的大小为$M*N*D_k*D_k$的卷积核，对一个大小为$D_F*D_F$的features map进行卷积，他的计算量为：$N*M*D_k*D_k*D_F*D_F$.MobileNets基于深度可分离卷积构建，深度可分离卷积由两层构成：depthwise convolutions和pointwise convolutions，分别对将$M*N*D_k*D_k$的大小的卷积核进行深度（channel）和尺寸（$D_k$）上的分割：其中M为通道数，N为卷积核个数，$D_k$为卷积核大小。 depthwise convolution：将$M*D_k*D_k$的卷积核分解为$1*D_k*D_k$，一共M组卷积核（不使用N），即每个卷积核仅对一个通道进行处理。对于一个大小为$D_F*D_F$的features map他的计算量为：$M*D_k*D_k*D_F*D_F$. Pointwise convolution（1x1卷积）：即将$M*D_k*D_k$的卷积核分解为$M*1*1$大小的卷积核共有N个，用来创建depthwise层的线性叠加。该层的计算量为$N*M*D_F*D_F$. Deep-wise 分离卷积相比于传统卷积的计算量减少如下：$$\frac{M*D_k*D_k*D_F*D_F+N*M*D_F*D_F}{N*M*D_k*D_k*D_F*D_F} = \frac{1}{N} + \frac{1}{D_K^2}$$计算量得到了显著的下降，而模型准确率仅下降了一点点，MobileNets对两层卷积层都使用了BatchNormalization和ReLU非线性激活。 一个转换的例子如下： MobileNets 网络结构网络共28层，大大量重叠的deepwise结构组成，最后接一个argpooling送入全连接层进行softmax分类。网络中大部分参数及计算来自1*1的卷积层，以及最后的全连接层。网络很少使用BN以及数据增强技术，因为小网络不易发生过拟合现象。 超参 Width multiplier（更小的模型）我们使用一个参数$\alpha$，称为width multiplier。它的作用是在每层均匀地减负网络。对于一个给定的层和$\alpha$，输入通道的数量从M变成$\alpha M$，输出通道的数量从N变成$\alpha$N。深度可分离卷积的计算复杂度变为原来的$\alpha$倍。α在(0,1]之间，通常设为1，0.75，0.5和0.25。Width multiplier有减少计算复杂度和参数数量（大概α二次方）的作用。用于定义新的简化结构，但需要重新进行训练。计算复杂度如下：$$\alpha M*D_k*D_k*D_F*D_F+\alpha N* \alpha M*D_F*D_F$$ 超参 Resolution Multiplier （Reduced Representation）使用超参数$\rho$用于减小图片的尺度，$\rho$的范围在(0,1]之间，用于缩减图片的大小，计算复杂度如下：$$\alpha M*D_k*D_k* \rho D_F* \rho D_F+\alpha N* \alpha M*\rho D_F* \rho D_F$$ 通过调整$\alpha,\rho$来使得模型在资源使用和精确度上执行折中。 网络的损失函数网络损失函数较为简单，即为feature map接一个全连接层，然后连上softmax loss。$$Loss = \sum_I y_i \log p_i$$ 总结MobileNets 是一个目标识别网络，即用来判断一张图片的类别。它在原有CNN的基础上，将卷积层进行了deepWise 和pointWise上的分解，参数量大大减少，精度仅下降一点点。缩减结构的同时，使用两个超参数控制参数的大小以及图片的大小，在精度和资源上进行权衡。此外，MobileNets可以作为许多网络的特征提取部分，例如Faster RCNN的特征提取部分等等，精度在可以满足的情况下，大大降低了网络的参数量。]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 笔记（基础部分-I）]]></title>
    <url>%2F2019%2F03%2F03%2FTensorFlow-%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[TensorFlow是一个开源的软件包，用于各种感知以及语言理解的机器学习，深度学习任务。 简单例子：使用MSE loss去拟合一条二维的直线，优化方式选择SGD。步骤如下： 定义训练数据，以及GroundTruth 搭建tensorflow的结构，包括变量的定义(weight,bias)，损失函数的定义，优化器的定义 执行tensorflow，使用tf.Session()定义回话，用于执行tensorflow计算图。设置epoch的次数（执行次数） 1234567891011121314151617181920212223import tensorflow as tfimport numpy as np#create datax_data = np.random.rand(100) # 100个 0～1之间的数y_data = x_data*0.3 + 0.9# create tensorflow structureWeights = tf.Variable(tf.random.uniform([1],-1.0,1.0))Bias = tf.Variable(tf.zeros([1]))y = Weights*x_data + Biasloss = tf.reduce_mean(tf.square(y - y_data))optimizer = tf.train.GradientDescentOptimizer(0.5)train = optimizer.minimize(loss)# executeinit = tf.global_variables_initializer()sess = tf.Session()sess.run(init)for step in range(500): sess.run(train) if step%20 == 0: print(sess.run(Weights),sess.run(Bias)) 这里头可说的东西有很多，首先是：np.random.rand(100),即： numpy产生随机数的方式：为什么重要，因为很多神经网络中参数的初始化，都是使用numpy来完成的，以前没仔细记录导致一知半解，自己写不出来。详细链接 np.random.rand(4,2): 表示产生（0，1）之间的float随机数，维度为4x2. np.random.rand(4,2,3):维度为4x2x3. np.random.randn(4,2): 表示产生一组符合正态分布的数 N ( 0,1 )，维度是4x2. np.random.randint(low,high,size = (4,2)): 表示产生一组整数，维度为4x2，大小在[low,high)之间。 np.random.seed(1) np.random.rand(5):表示指定了seed，该seed下产生的随机数是相同的。 tensorflow中表示变量的函数：tf.Variable()tensorflow中所有的变量使用函数定义，tf.Variable 类用于操纵变量，该变量可以通过op运算来更改他的值。定义变量：weights = tf.Variable(&lt;initial-value&gt;,name = &lt;optional&gt;)变量的初始化：与其他语言不同，tensorflow在使用变量的时候需要先进行初始化操作。可以这么理解，tensorflow内部是以执行Graph的形式进行计算的，之前的所有操作，如定义变量，仅仅是构建Graph的结构，但是并没有真正的将值传入Graph节点中，因此需要tf.Session()来执行初始化操作，为变量节点赋值。初始化如下：init = tf.global_variables_initializer()sess = tf.Session()sess.run(init) tensorflow 产生随机数 tf.random.uniform([2,3],minval = -1,maxval = 1,seed = None)：表示产生均匀分布的随机数，大小在[minval,maxval]之间。 tf.random.normal([2,3],mean = 0,stddev = 1)： 表示产生正态分布的随机数，服从N（0，1）。 tf.truncated.normal([2,3],mean = 0,stddev = 1)：表示生成范围在[mean-2stddev,mean+2stddev]范围内的正态分布随机数。 tf.random.shuffle([1,2,3,4])：表示沿着第一维，对数组进行重新排列。 此外初始化为0: tf.zeros([2,3]) tensorflow 中的LossMSE Loss：(L2)mse = tf.reduce_mean(tf.square(y_pre,y))MAE Loss: (L1)mae = tf.losses.absolute_difference(y_pre,y)mae_loss = tf.reduce_sum(mae) 处理分类问题交叉熵Loss：softmax_sparse = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y,logits = y_pred)loss = tf.reduce_mean(softmax_sparse)其中不要求y-true 是one-hot 格式。 优化器：tensorflow中的优化器共有其中，均在tf.train 这个类中，使用的时候看具体的应用。optimizer = tf.train.GradientDescentOptimizer(0.5)train = optimizer.minimize(optimizer) tf.Session() 会话控制：Session 用于执行计算图中的节点，因此获取一个值，或者是最小化loss等操作，都需要使用Session来激活部分计算图。使用如下：123with tf.Session() as sess: sess.run(init) sess.run(train) tf.constant() 常量：tensorflow 用 tf.constant() 来申请一个常量，常量指不能被修改的数。matrix1 = tf.constant([[1,2],[3,4]]) tf.placeholdertf.placeholder(tf.float32,[3,2]):表示数据类型为tf.float32，大小为3x2。使用placeholder的目的是： placeholder 可以作为一个参数，专门用来将数据传入函数中 由于tensorflow是计算图模型，如果使用变量传参数的话，计算图将会变得很大，不便与计算，因此使用placeholder来代替 123456import tensorflow as tfinput1 = tf.placeholder(tf.float32,[2,2])input2 = tf.placeholder(tf.float32,[2,2])ouput = tf.multiply(input1,input2)with tf.Session() as sess: print(sess.run(output,feed_dict=&#123;input1:[[1,2],[2,2]],input2:[[1,2],[3,4]]&#125;)) 激活函数1234567891011tf.nn.relu(features,name = None) # 下面均相同tf.nn.relu6tf.nn.crelutf.nn.elutf.nn.selutf.nn.softplustf.nn.softsigntf.nn.dropouttf.nn.bias_addtf.sigmoidtf.tanh]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ESRGAN 详解]]></title>
    <url>%2F2019%2F03%2F02%2FESRGAN-%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[ESRGAN 详解ESRGAN网络是在SRGAN的基础上，对对抗损失以及感知损失进一步的改善，引入residual-in residual Dense Block(残差密集块)来组建网络而取代了网络中的BN。并且借鉴了相对GAN的思想，让给判别器预测相对的真实性，而不是完全相同。ESRGAN网络的作者是香港中文大学的学生，他对学习的建议是多看论文多实验，自己push自己！显然能力越大舞台越大！ ESRGAN: Enhanced Super-Resolution Generative Adversarial Networkssubmit time:2018 ECCVarxiv link ESRGAN 的作用传统提升SR（super resolution）的方法是使用Peak Signal-to-Noise Ratio(PSNR)峰值信噪比，即最小化生成图片与GT之间的MSE loss，但是这种优化策略倾向于输出平滑的结果而没有足够多的具体细节。这篇文章在SRGAN的基础上进行改进，提升了图片超分辨率的精度，作者从三个方面提升修改模型： 引入密集残差块（RDDB）去除了BN，节省内存空间提升模型的结构，使之具有更大的容量和更易于训练。 辨别器使用相对平均GAN（RaGAN），即判断“是否一个图像相比于另一个更真实”而不是“是否一个图像是真或假”。这个改进有助于生成器恢复更真实的纹理细节。 SRGAN感知损失部分，使用激活函数之前的VGG features map，而不是SRGAN激活之后的feature map，调整后的感知损失提供了清晰的边缘和更具有视觉体验的结果 网络结构生成器部分：生成器结构上的改进： 去除所有的BN层 用残差基础块代替原始基础块 BN在训练期间需要计算整个训练集的均值和方差，当训练集和测试集差异很大的时候会引入伪影，造成图像的模糊，通过在残差块中去除BN层，有助于提高泛化能力，能够减少空间和计算复杂度。 生成器训练过程的tip：1）残差缩放，例如将残差乘以0和1之间的常数（图中$\beta$），然后将它们添加到主路径以防止不稳定。2）较小的初始化参数，当初始参数方差变小时，残差结构更容易训练。 判别器部分：（相对判别器）判别器部分使用相对判别，也即是说真实图像与生成图像哪个更加真实一些。如上图，真实判别器为$D(x_r) = \sigma (C(x))$,其中$\sigma$ 为sigmoid函数，$C(x)$为sigmoid转换前判别器的输出。相对判别器在则判断是的，真实图像是否比生成图像更加的真实：$$D_{ra}(x_r,x_f) = \sigma(C(x_r) - E[C(x_f)] )$$其中$E[C(x_f)]$为在mini-batch中所有的生成图片取均值。 损失函数生成器部分：生成器的loss由三部分组成： $$L_G = L_{percep}+ \lambda L_G^{Ra} + \eta L_1$$其中$L_{percep}$为vgg中激活函数之前的features map与GT的features map的MSE loss（同SRGAN），$L_{G}^{Ra}$损失为对抗损失（与判别器对称）：$$L_G^{Ra} = -E_{x_r}[log(1-D_{Ra}(x_r,x_f))] - E_{x_f}[log(D_{Ra}(x_f,x_r))]$$即生成器的目标是另判别器将生成图片判断成比原始图像真实。$L_1$ loss 表示恢复图像与真实图像之间的L1 距离：$$L_1 = E_{x_a} || G(x_i)-y||_1$$ 判别器部分：判别器loss与生成器对抗loss对称，如下：$$L_D^{Ra} = -E_{x_r}[log(D_{Ra}(x_r,x_f))] - E_{x_f}[log(1 - D_{Ra}(x_f,x_r))]$$判别器的目标是将原始图片判别成更加的真实。 网络特点 用密集残缺块来代替原有的基础块，去除了BN 对GAN进行修改，进而去判断相对真实感 对激活前的features map做MSE提升恢复精度 网络插值：为了去除PSNR导致的像素平滑，同时保证感知质量。可以通过训练一个PSNR 的生成器$G_{PSNR}$，然后基于GAN网络的$G_{GAN}$进行fine tune,然后利用插值模型得到一个插值网络。$$\theta_{G}^{INTERP} = (1-\alpha) \theta_G^{PSNR}+\alpha \theta_G^{GAN}$$因此可以通过调整$\alpha$的大小来调整网络输出PSNR指标与视觉效果。 总结网络在SRGAN的基础上进行了大量的改进，包括在训练方法上，loss的设计上等等，最终取得了较好的恢复结果。]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微电阻成像]]></title>
    <url>%2F2019%2F03%2F02%2F%E5%BE%AE%E7%94%B5%E9%98%BB%E6%88%90%E5%83%8F%2F</url>
    <content type="text"><![CDATA[微电阻成像原理： 微电阻率扫描成像测井采用多个有序排列、间距几毫米的钮扣电极测量井壁地层电阻率，并形成分辨率很高的井壁图像，从而对地层进行细微分析的电阻率测井方法。它采用多个压向井壁的极板，每个极板上的多排钮扣状小电极向井壁地层发射电流，由于电极所接触的井壁岩石的结构、成分及所含流体的不同引起电流变化，电流的变化反映了正对电极处井壁地层电阻率的变化。经过适当的处理，可以描绘为彩色或灰度等级的井壁电阻率图像，对地层岩性、沉积特征、构造特征、裂缝及洞穴等进行分析。 微电阻图像： 将电阻率数据进行处理，然后进行颜色的映射，得到的结果如下：上图宽表示井口的周长，长表示测井的深度（图中仅为部分长度）。可以看出来，途中存在倾斜的黑色条道，只是由于探测的时候设备仅仅有六个探测口，然后每次探测完一个深度探测口发生旋转，继续进行探测。黑色条道即为探测器之间的距离。 任务： 根据已有的数据，恢复出黑色条道部分的数据。 workFlow 2019.3.5沟通需求之后发现暂时需要实现有数据部分的数据恢复工作，接下来用网络跑一下看看效果。如果效果好的话，改写到tensorflow的版本。 下图是使用ESRGAN恢复得到超分辨后的结果。右图是原图放大到像素级别的效果，左图是图片恢复后的效果图。 2019.3.3do something in this place 2019.3.2看了一些对微电阻成像图像的应用，发现人们会根据有数据部分的图像来推测没有数据的部分，通常使用曲线的先验来判断的，如下：也就是说我们可以对数据进行标注，然后加入一下曲线先验信息等，然后采用深度学习的方法来做。 专业人员可以从电阻率的分布曲线看出岩石的类型，如下图，因此可以认为电阻率在空间分布上是存在一定的规律的。是不是可以找一个指标来表示这种分布？]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux操作及远程服务器访问]]></title>
    <url>%2F2019%2F03%2F02%2Flinux%E6%93%8D%E4%BD%9C%E5%8F%8A%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%AE%BF%E9%97%AE%2F</url>
    <content type="text"><![CDATA[welcome to my blog,enter password to read. Incorrect Password! No content to display! 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]]></content>
      <tags>
        <tag>tip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SRGAN 详解]]></title>
    <url>%2F2019%2F03%2F01%2FSRGAN-%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[SRGANSRGAN是一个这篇文章将生成对抗学习用于基于单幅图像的高分辨重建，不同于传统的CNN的方法，SRGAN得到的超分辨率的图片放大四倍之后还是能够体现细节感。 Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Networksubmit time:2017arxiv link SRGAN的作用SRGAN目标从一个低分辨率的图片中生成它的高分辨率版本。 传统CNN方法：基于深度学习的高分辨率图像重建已经取得了很好的效果，其方法是通过一系列低分辨率图像和与之对应的高分辨率图像作为训练数据，学习一个从低分辨率图像到高分辨率图像的映射函数。但是当图像的放大倍数在4以上时，很容易使得到的结果显得过于平滑，而缺少一些细节上的真实感。这是因为传统的方法使用的代价函数一般是最小均方差（MSE），使得生成的图像有较高的信噪比，但是缺少了高频信息，出现过度平滑的纹理。作者还做了实验，证明并不是信噪比越高超分辨率效果越好。本文的做法：应当使重建的高分辨率图像与真实的高分辨率图像无论是低层次的像素值上，还是高层次的抽象特征上，和整体概念和风格上，都应当接近。因此在loss部分，SRGAN加上了feature map部分的MSE loss。 网络结构 生成网络部分：SRResnet，由残差结构，BN，PReLU组成，用于实现高分辨率的生成。判别器部分：由大量卷积层，Leaky ReLU,BN等结构组成，用于判别图像的真实性。 损失函数SGGAN的损失函数由两部分组成：content loss，以及adversarial loss组成。content loss：传统算法使用的是还原图像与GT图像之间的MSE损失，作者为了避免放大后特征过于平滑，认为高层次（features map）也应当相似。因此定义了VGG feature map loss。其中$\phi_{i,j}$表示feature map的位置在j-th conv 与i-th Max pooling 中间的部分。即同时对GT与生成的图片提取feature map，然后最小化这两种features map的MSE loss。 adversarial loss：对抗网络部分的loss为判别器判别loss，即当生成器生成的图片，判别器认为为真实的图片时，该loss取得最小。 SRGAN输入输出以及亮点SRGAN的训练数据：GT：为原始高分辨率的图片train data：原始图片经过高斯滤波得到的图片输出：即为最终恢复高分辨率之后的图片 亮点： 训练了一个SRResnet，由Resnet生成的一张恢复高分辨率的图片，然后将这张图片与GT传入Vgg网络中，训练一个MSE loss 最小。 重新设计了Loss，将features map的MSE Loss，与对抗网络的Loss结合。 总结这篇文章可以比较好的恢复分辨率低的问题，结合了高层特征Loss以及对抗网络的loss共同作用，得到比较好的还原结果。 看这篇文章的本意是想要对电阻成像数据进行恢复，这么看来，恢复的前提需要GT，但是数据集中并不存在这部分数据，因此这种方法可能需要进行修改。我觉得一个思路可能可以行得通，首先对电阻成像数据进行切割，然后将切割后的小batch图像作为GT，进行训练，可能可行。]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interview Summary]]></title>
    <url>%2F2019%2F02%2F26%2FInterview-Summary%2F</url>
    <content type="text"><![CDATA[welcome to my blog,enter password to read. Incorrect Password! No content to display! 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]]></content>
      <tags>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[project two]]></title>
    <url>%2F2019%2F02%2F25%2Fproject-two%2F</url>
    <content type="text"><![CDATA[welcome to my blog,enter password to read. Incorrect Password! No content to display! U2FsdGVkX19napMBuPrWAl5IBDJeQYvbtZfvTruDMdvTa3TfnkeWjI56o9AqCrLOf2C/qgBOaNew3vb0phBmdmfIJL+7NbdH8nUEESLmt+RBegsprhA8J1qoGWXZy94LPPJc/uVEA3QRTVA3jmN8QKNt+ICenyEl13i9TZZon9zAzwsHrayodEmPxWIOyVqzcALesjVcvPV1TkdVLdzkNPrscIHz/5aO4GEvoKKbUnuNst/suRQ8tq3nbbtL4njFPji2TIa5G9jDEpF6VDEL6iT9Z4nNPiOCZJF/KoRrnHjKow4sgYAWoIklsLxh6tTraeNHJTNL1kW1+XBD6o01fvRuMTs1EH88arqwfx+s0X+vhLFnz0ZPfk+WlPbPccrXyoYJetf8cG5U0Flb1G6d4tgafTLJq9Xgo5eHOt9U+U+EFRNvU+55NsHILFccQ9BmSlWo9l9xuGUuk3rPrJ3T71MCc7l6xcVdqC+AQHgI7L5tSxZ53lcXBscyJPe5yh2kBCP2Z8cIo3tQWXYELvUmbUDvDCkvcjZu+YdlKQqMHnKh5aUEZHmoJWLexNFLzCX/0WsmaEkyXGa4dqvifFOO6dVbWTxKJwwSSRXLetxcrQq0ConttQtABwb9ITqjB4gZ51ScqfRVXD8Y8RsXNa4n7kgVi7f7kT+PM9vQAD/cJtGLANQYDx5KOROKNyC/s/N3EFhDQhmOr7Q3/6yyR4dCQhHAMF1RwwwmpGnUBi2tyIylLtyBZB/LfFiNdIGM8M+ZJ2gckXSR2b3njuOUCAIM7rtTfleWlSgoxhVsWmQ211rcojdhrfvTFSqIYbHsVlUgEqOe4ktBZibCDGGb3+DgePE//jYpZuIn42P9dGiKbJ29v2kArMIjEOz/kzSnSmBzKSNfglyX3RCBX7Ct8Jjjwv0wtppsdx6AlyrFIVa7IvsL6SH6O4k5xn79O9ueCdHRHt+BeuRf9SE7cJvI6781trXyE5gR0GgOrvlqoikq6SaIBa2gqTOKrCWMGdUuCOfzQF2BEYtv10jejOBNy1ndi5B4tD6MUpRuJMJC+/6pf6bKbbulPGjeoWgSbynttzj3SrtSN+1qjuCHZ6s92lh+XPM6IpNLSxYSfV17TlXKNrGr+meZUd9I8Iv0uJsV6zWGinSxNX3+EpZaKoDq41HniyHPfq8LuTJtIch1ZJ2mWPe02MMYp7ssiS8Me7hKLeAb4R5ks4+PDehrfyQkTz1yu9J3c4pgFabbRg4/2g9L5WBYduHMU1W6UBFgCSLb6R41iJfQ2khilDbjCdGpViXShSo4FvlzcFMwMqxE8QjB4qbhxg5lebQ6gdqgd+x9gKz73Kgw0t+ROyj9xMKPwRPVBccDrWE8HR5+YqIBQjuV7cmFPDbg13oN4i0s8mBotsYUYatz18Pk/v9K1yDTUGbQnt4Ypx0y7Jtxlf66mRyhsRqn4IxFDnC5TL182zVtpSRz+F5mV84D3Ag07D836sPo7xf+ja2/ocC3G+FGKMHGxWgWq0wgva0/6LHl/GRFMq736oDTxP5uMS3byQfjk2VUCRzoFveMwYiyg/BQU350hZ08AnrgnSny7qM7vqZzphGkcx7EPGYMmqN9K6Bd5EYmEIvc1qynyHmn8M3Ei+CRLlFVP65saIsqvUSa0s4KD3XJkx9HQobwkFOXBn68fAQQyW4EDkdYxkaxrEShe3zRjqcRrkxcCXrUhwMpgffuwRlBvStCZ9znIKBfvAsid732uJt/EqElo3lYYOZjyzfPLUmqXNmrW53IP61h83IbypnkqB+RQmSt7ijkoLiH7hwGKtuA84oyHZkIxxfGtcX1RLaDDPpnlUs2TuSfGZyYMLa9782hjfgnYl7vp6hPSbbSabZ17wxHz/yRiDeBVKm58p41uGE4dzQxfGAmrZuLAFUAdbMsm1K+TuQqMuKaXCxApgCjPHHbyPT8Uj1amOEcp7KPVjR894vMfkoaZpi+CA6ztN5/rGWP2Uh4ZphsvrVrL0WzQlfH9Kwa/Hy+6dQQx0vsn+ayMCOXZI9IzngNLKFsPFpVPJiWz5fCRjTJk8eEATKdVETU18L6l9W8M4zfrR43BXfZNtHMnRqxyEAg41cSWcTaRZ//ii4n1LSY7Rym2dt9FDH4Ff6y/0PBJ+aHDRTq71H+iyx8E1UHXMyTedJLPk57wqWAZRv5CKbHEuZlmxt2Uwm9XZ6tGoJxH5ZW87Fa/ufdWWpMO9tu1Nv/JZTnnUP0yIQJ6c6rFXgn2nUErmqK92NNE9q9eaXVrOJ2xcbezL3lvmqdLUScwVhiBH6yb98+kCTzwEIfpCF1v344tpEihRdeaZNIo1z4F7PsTLnOHZStLTgyzk9HH2SwExtXQXyLBP2EphH8e9zfo+tY5Uo1rHEw6Cc3lp92oEyY0lagK6xrINXIRguBZj9QANB+rae5Hs30bQLQpr+YIblxuoBsaEALxaZCfrYU3I4cW7/DXCyGHwVhVkmPza1CkdLY+Xh9TKCXnLg3Bvbyda5VZ+jHieJw8KH7eiBcSbCzV4ZhpC1ClbCr1bAoN6SFfODnSwCAqZk6LhDPIreOWJ/zlme3oglbeG0NWL6kwSHRc8WsTq1t5vMQGEwnJ+7cAqIJ+Cp49RbBkolJGD10n4e+X9hsxhXsK57jpBMVZjGjyzNNI10RalcChWPXRtUX5fClDOqfi7R/4b9cNSohjnpqh7WbJjgEVxwulL1OzFiRO6zxH1HOzhHi15Dy8s2gXawJ0bNz3MI+46+RDwbrd5S99JnPMniBJ8obIHfrZcMLxsfCyUmwiaPjUeRTMMw1tU67HqNfABYVn3kpJG2cHKWmrccz1QrnwraenOx4U5+FG4LK+/aOWTceK0hGYAyT+PlftllSjxB9C94Yl2l/HnNhtVPm4YBtsRJ/tJPzUnFCEhAOf9f1nrMuvkH1mY4OH0ePXBIxYLjEW1yNK7moa2xxEPdSTSOwaqbShDyqbMndmRL8tcyFDqMvUOu7lKt2adrnj18+KFm8O0PXDHbsvV9V0gB/uVNfPibm0JULWG6ZJDJzpYoVrx4TEuoW5oLRsXfxKHQQT7Wl+mPKgeZoQhkOYCTUyQhXp/QgrQBP/tBgEKUbpoAZ2UtEvPLWjAQaEwpLoOJRoLlMfA1KG6/GM0mRYLDSV/mDuRm9TgOVYvAOaEWmr26QgWuouyUfJTMQiQYyW/RbF00fGIjbto+GCvgk+ZVceerIcNGysLRmI4LSSAAFpOUrsU65fNA4CKKWGzOS31HlKwzkhiX4zTrv6i0rxdQ60mA1wIaXIoW3R8qIwGoxJuYoWhPMg+hCFcpxUbdIARS3HMfJVDNg7ZHdcugUiT/URNDvxn/7kL5QhW3g78aiI+3Q0D+ajbudIjN1lPyiohqLSZfdb0QuTF/hHV8HerUccE7o2xuDwleOT7bAZXl5gOURddo0OSV3AuYtUwfG5QK8iLeRji7zbYDbfpX61CWujhvysYXZaqvLmoJh4H5jGUGo4A+3gGuEuquk1bTIyMemD0S88gqtL7LtD0cFm+Vt3DWZZBoPu6zE+rj7KHzyLi9cu7RJrHRGTrLpBRYaFTzIpkM1fVVxj+Y/wsYCbJsCX+yoOH/DCy//qyh9B+ymhI7DoJcyDGmbgC7X7pUW1XMX5EL6qRO7Qoni6Q2j6v11XoPrMsES00crYi492sccxfM8QfvemSGcomGYvFsmhs5mS6hYwHdQO0Iv0q2IchHCHFWohWK76G5UJ5jTtdHJikWkbjLuPMeDEnI4NGATrMNQXNR3ZruB1gGKr2hlmQMccs9Y98HHl8COr53s+a4kQOdA4l/+JAvNUtQ4HwdXqR7YjR4ibyx6tA+P56N9Z9Mptlx6k7YqQmqn/NEI5x0pEp8Mcs0djpv/fy1lGFuaO01B0efnQWnjcrj3BWtZPgwHFb+yTNACm0Opjvyn3mbaVTdobuvlqqGvjwizurw3WEeTQoBZIsHWVJ+1lPBvaIhR6bWbNK8RfaX5FoJvi6x4A6DirDsB/wHC+wHXb9CXsH7sHbAQ7qTZUfbRVbPhyVHMCV+Jh9sVVAAtGI97RawrLaRCFz7xSuU9EIqzvDuPwXGk4+J1QLKHs4iPF2+kkmzFmnxh3ok8afSfe7n1WmB4tZxWeN21dYjQIMKRHmeUQjbvhY+9f3+z2Lxc89adM41ICanij5EljkuvNEvsPOXL0ZvI20WdxEjPjUceJA43czzk1pwLKlxuZ7ce3b4NudkgDzVAVjZd0z9qge9vC0VdqswUASUWaiJ4yJOSu/XeYL6eebUwWbiZpEHBXZibVXEUzFs2Pjzi0OpQs1/rvcxgh0go73eMIGKIjRIZRqPr8adOYjDclfjsXjmZc0ya5OIbqEE2X2UXH2+B/1gDjzjjZ/Dut7VF/iBm02hNN6GclYuCswUhYhXKlbaU9OSzRuEf3j8+hO0VFXOxkQDkz+MbwHMxgeLLas3JJvcs9rNTTYfqLA+/TXq38vX4VB7/MpoPKNNrcwetmobkbH3WTAaiDOKvF2oHMfdxobrFFvte7UALDHTtwrousq3m1UHHsgBFEyT+e5rV/pK0lXNgmHjb/cuHjOyEL9+U3JFiI7KurL8AzQUATrJ90LBWEaDjGPpBigtfck21a8pLddrdvnxIP8m1ORzljKfWCdWF0xet4UJuVH2slbk/2PTfjSbz9NJ+HtF6EJ+1CwqiMgHg4eIQ/bCbqX1Hdu09wypTyZA08OfxR5JxVc45WXQJ7gyj1B+SMs+JGwQoTmwLFrjmLJ7NfJHz3TLAeQvMpNum2PU7u7PxGs2KZ7l7ATfl8U5Fo6HUvGQ/YX/y3CLVAPFzGlJ3wPE1+6/ZvkGEwbMpzSK9M1uEB8wW95gYTkIkPf5JxlnK+CoEbvq1CXI3QPFHTUBfceZC+rbNvKFab3lTS6DtqSQusI2xoS20jtBJ8PAED1Ij1e+IUGjLKKOVLzwIGowFAYCc4GDSs5zZrATc9VgevEIIY8Nt31Vf2+cO9sJZB6kXdNiUKzaoeAnokLfInvIjql/R75UNIZ5VXlR6Gnwa/jhk9bAIURNvWEpf8qpdVZNI3yzpUbJY9+vXVRlNlWJrTT6fmnEtg64HqvqzNG0MEq0BuKF96wuKVbVvxTnWFbLBxAs6uFnLl1zu2IL6LNj5wQIP8+T2/GSkKuGTkpeA2D7/XobNla/TVHS9n/WFEgPUKOF6m7XKL8sgHNRrFSI0L+nekCKh3dH3PYFkbbCnVHGxKjTC9LGRN1k02AX3+k6LAuBgylOuJP3mT+YeW/fh1LNPpE2tj2IVPzOr6TGaR09a8hojTYU5mFpIWzy3l5GHnFjG3vFuBQF2zg7K0O7IA5cdmyOqbCCBmIqnhmG1azepiFPEHb5jVkaOhDGGryHr1LFCyPtD2uIZvQmKS19bakHHqa7rCH7rM4FakLTOE8R8O2NZp5bRhNEIb0e8JgGCy4q4T+W0iS7PldvtAmqcNfSoKhg96ZZrqfacTbufxrxNTcjgI617m2iPidSCrZ5jBHkPs1Qr1coHg3Yy1Qs4j3SzLFF/uEyz+IpozviidpRL0tSPt4ao+9UurCOw2eF6QPEr3aulO9FpH/XL66KXYWMhL/xK9xObvdNQE8iBddMjj++iirxPsf6rG7jPkh6xwM0BZWDN6GkFxhs/wmbmv0lN9sJpqoGYeClIVIx5jqT6ChiqKC4K0F9J2oHLnd9nQzhQr7kIMvO+ZIKnMTPzC0iN/Sqrn/SG433oZyuNcjwxbI4JG4Albbtv7x7NCp4u2BnCvgJAUi6lunNp5t3gMFnNLjFRmvhe7j6k97iIGqrz6iQSNcp/BjBTi6MYf4xA4CaA7Cl4VdOr/uL9TyQeUUDOSt7WiazD3YCLnmS0h0ZY3J9O1FZ5B6DKVrLkzzvd2NGMq64w0kQrG6Z6l0HIshxVz9m6TW607xb5GCNE97zqpTQ6ZBrygQ+OTU/IM5iNoV8YCfO/2iICedIjVmbgApzWEBahiqRyUvdYUc8skQuuQLP6GKcgSQY56ycm5DP7XcjIpYAbsR8NHkUOHVM8ATsff7cbRF49rOJqA7DqTfCidFBkuoCQ6srUdbUsBasHtD1x/+hVAjbkOtGzAVxS6O5XltzqAJLs6AxfNecb979Xvewyqp1wQMS9JZFYaqwl4Y/V92nJO5346YiVliRafir0SfP9Ow3ReM+Y4oFXQuX5tGu6V/UyRGe5pOwPoumeRY6EdQKjDkFVUbxqeC1o0a7L0xjcLkzpkMhB0TS8HjIdEeG9O02vn1uwcp3CTOJcAiNUKAQU7vO3dXckawkoDD1jCAwmmZwKOSCcwkItmeHsJn7YDhOsKh8pyXWvtuaYd1cWQWSBKX5TFwXAVVRpaC7O0Cm81C8Wn0Fx+/ZZfIvR7UnJom9x/Mi5nwotHZYCca2uqcrms6JnBu9xOW2ar5PD0vtIt5nWCbIDPyu2qnHboo0ctA8vgxSCdOu7DuaZk5txWRhQK7J9sbYZMKmdhG2oLZC+09VOyk0sslq6i9es1vInnrqfJ4Tt/zxNftIEsVyKUM3cb7QKkfij6U07PkS+aPp5MktYKMtsR2dKdhxHzqFTDXQOq5wShwmxQUddR6XQMOdjTH1BVn5soxLXtkpwODgZY4tX+lsK70Uap6sHGNhmIewZ+IThxAJJ1jwT2fD2R0r2WxnXFqSetCfA/+ncYXCtCXsFcDyEg93nfLHgvjRcVpF3k0QCdjiHvqRFNCtQNMuHhwWo1OCveb1k4TSo+iv7iIbOVMdivIgXMACAOKmFZerrazsDt7sRhOZunGN+5opUr1boNw8pcNI7JFKtxjBtaXwPnMk1GczFOaiLdxxo/loQIiQ9jhSEXwT12GnO6nJ5O9LkKK407VvaO/JQA84SLKNlD1WPYdeeWtyPbhHJQ/rd1g/WoqE3LHSi841jTO0sXYuxG2js+dO7DUKBnUB4Swlafce3z29dRzhSWNIha9JbBHXpcrDb/7AcDL3/uiKKTMiqg0zpjyAoToeRd8hElrhvD+vvEGtYN068I0rC3IaD5lvhJudQozxUPVUFj0VyPFB2Cu1rXq1iue2ucXOKMzQ93iGcgulYbtioePfViStw0d9oWwH88e/J5rX4SBPeo/YvMihT2ov4I3VTM6Zmm1SpJGhD8FjNVwqub5vCI9Jme0aaSfCZqAgKoovpev/LilOWJKjMoCUe44bbg2/mHRVqYft67kw1erIh8ceTWNpEf92cRemgYGxYqJcYrLnodouoX8HEZ1OqHwMthoCtz77un24CRL27fUOw10aq/uD4P6edRGVK45d+F6tPIHWKlhB/5zkaqU0sL/XdOMH/oqhNHM/KSdv7BhJ6eOu/LKfmO3NZ9CWRneYdJhtcZCONL3QQAuLKF1abpiUhyJUXR+pZR3TCOZaMccl63xHPmKCUxgvWAdpk5v0wXRZLofyvbQDk8ud8lfQcm6/jbpMC9M0g/FJoTG6xtolbLANXFT/UCXs9ow7sNV2ZtmClRxMVIIHfscZrT5N4jBfMLrVpA181Hj3mH1k0d99gnyPtnfTigCBVAgLr7ux55jpCbRnIErm82MNw1qFfdwCDBl5gmysbtRMi6RK7ReI4FIxKEP/CZVZEy5BL3ncgO0c/olOXIawX1+KTjlMUdV+A3NLq7wT6oMIFLxqCh2XnG6qZ/2Y07MyZZ3WJfkGzYI0uGaQ0kS0xyfbEgjqq3tZu3HxO6dRC9Fqtw4D75nqPKnassTiZn9hHu3ZTQmqzl58BxDpBDWyjC0QUeKQhaPEfMylSSEgdyaxl3tzYpdBVuOj3XinNaJ0mB+S4okyRweYdUCRwpkSmq+QinLwgqI5+m01k6UXTxLx7kEEErpZWs2X1jb8L5v/GK2HFi3j9lh7Z6bkSBcyrebVco8hhN3Ver7Qep2MR6hlgDmlZHhv37YEoeJKWZr3ThyEcEPRgKck03uLGpMATlboEQeXR3vNqCgDjTQRH8skQbFpcAWYeaB9c73GrU2zMfaqmVYz3hKMmEtGV0XINDaqp/FhUY132iQlzMtDILpEumeX5ljYLX8fkeqY7q6Sgsf2pT00BeFx/Qe6T66Kc6vjv4cF87lb1aUIZTylcLG6VhNq5M7TSaGL+ZWiZDWu/ujjMUopKO8FdXGUkcfdwb7bpxtYzEWN3nv4yPN9JtQUcJ+ctEbPbss7kgTw1XENVNhPtWugUQ/dsCIRS2akI3RthUJuUBsqKWA1ToQWWFeaE22fYUeOhfZok2/+5lQ1P9ABJ/muzTfr3YwvSjCEusRFnSTQ3qEWGMVRYULmdKfvmAvEkvsZuuQ3B4ovdWBIm0+rU3XWKPI2ofu3V4ajcokUL4ravPvqJrQt4royp3LO+17ZOtvd97qs30smMrAbJ2udmm1sbb4HOtwELW4VkAho6bm0E2JaiEFuJVjjPlPt2Rbfhsk55iT0ptcL8+JqKb/LwkcolPSMiEREwJuJSgTpsaryTVtibYpXFzZAUp+dxwHO7ncxeNG85yplwGKaKRDZsFYRBXBNPBfY07iBixjgAEr8B9s6SAYVvBk4g/h/rj8A62YD1YnDL3URPIvI2fiQB1lgNZaE4pFbQ1gRm1bn51OI5aJAlzi9sZDT8dW6wdIXvWCkUokW4ttaOBmsulRBzRpwBHhCh4Ery4Sy6nBeQi6GLrW4uo4gLsON8bTKUIMgAzgFdf31cgEGWbZJBq1M4YQk+/66Yp2p2wqnIIRQxz6uakUMw2P4rlCx3NeP1jyzSwrse6QnwA2yblOSKnpsOT/QxcpJ98pKjyE5AFc0Z0Y0tZJylKoyoSFdf3KNNBrnUhpk35TE123ioFkICK3s91R4IyerajK/HHc/0kC5bcv8sVJ82KonAZmnwq1nHvyZ6WR8z2/cAeaFTz64Ufflh03CWwrv9mhKuVGZELWY14uAWozXaJFo43lgHjQ6vJR6KjA4/6a1ULETjJn6H2MsTvjEqDQYj1cF84EZsdfPnDDZe84gijpE5XYfAzpnOut3zCP7aYj7clCZEUJkaOXNpiCZ4t1d+X9DESn4W3ITosG/vLMpkIDz7HND35ZXssKwQk1QqprbGOEX06VpcpUOKk4Z19bpoNQwdmVH9tab6IPMwV1xCtJ2fBn1IoPx7VTSvOt2HOwyOl3WdSnaxbLZGaZhkMsdhspyb1Gb09xAU2VXhDmxt/sUQHoqoiW/1PUnZe/U1Up7ToaGbzp1oP4wXDR1hPCPKAv1pTpjZzNYDC82Kec9mBeaIXFbnMn7FsMFLpsBE8JwBlFHSC2GZ4hzyOS3k30KIwoYcNHtSNo0v/M0idmySjJzpdKxBxD7bupveTfvrDCjU5CI2DFvbR/vpmRkv4OC5vBuuB0aSLSChsOfpvga2mnDkE9ardfA1wNOGNqbxVzgIr9DekGi+pMt4tB1w5LrfGN2eN/f8ktnmiTv6/x3QPrmHHzQZcq+Cuz9i6XafaxHp00Z3Vw31qk9SVDBNqWZdomBcYzCyiWgDqd6kbihiA++W2PrhbUxejuaQ5aj1QXUBz93p+PgDkbYljUVTdU9nt6LiHisOY6O8DopfoLtOgySm3Bp9O2W3lBVzUw9F32U0KqDDnJGHr9mYHkAPbGNd3LgpEHd0rkM3VzjSN4I9RK481cCMRwcSOtL6pAnDEoytFTB7WqPFhD6q5H1nuhrBEfejwQzB+gFXmAMLFDJX1C2pr1po8qupObCfqtRUcGFzAkT1RC24xY67qocjt3RSVI1U7YT7MGSibv6H0s/iYDUoAb8uHPQAhRWlpRvmOubS9dxSlPzsAslQ9A+nRKJKWd/TAPYaBrilEBiCiPQsLbpK7/hmggb6WL6AIv9zZBuSoLDGMkmR7QVK6m74NJ1WHA+zgLCF19aRBYV+vEylpjRawuwGYD1bk5TZ7f5K0wVEMkBkp0uuybZjEjAviA8g25MqD/KVYmbJclB7geAVearwSY7scjFGC+couCgdGWoihp9aqV+1QkcCwtxXXPSLo5S6CD5AsUBVAmxFFmJmzSs9x35emshvd1rnPBqS4e6qxCTg3E849VRKL3vosLu3rLdB3YOYTB0jRzs+OFNoymC6cuDfgJidqKniL+co4MSStKA0gT1ocwSLHbYBUIQgdR4t/I0Zb+4MnCll+xC0P0jS4DRaxczJm8s9WakK8Z5hJ0hfOoRGwQDu7mBhnyKZeOYsrhzhLasd5eRQOztPlqsSG0o+XQyrfPgw+bUX+A1BFnX/F8IN+FjeblvD7Hymn7eSKq5Wybn9FezsmR/oqwPNwD0gxKBRIBjUtw54IHeCPViaQsnX4Kf3c5MroXB6s/Ov+34JwtN7Afxstu1uLnBoXcwmvWmI4s9hoG5eCo7nlnYt/PIvEFUcdmiWoJ3Zc4JZsbKtyOQON4M/hFT3ZOVnfUnrZmGn4+FRMdSH3fJ7lniCgeNiHLlspPSId1ch4hSfbamRUEuN91coM2/AZAZ5pQddS72BP7Zx/zg5D7HqdSwqgwFqRjLkXg/PuHtSg+YvaElCOlua0UXqBXsDFc8X2aEBv0GgKyUYgR53XDV4FGMOuPbYmJ2T+5CamEw6gGwpPC/fzeQkP0juFPv8Lw2U2l8RXu79Ap6s1+fdqv7u0w0JIXh2fy5AlUPX1wOGQLk7fo82pL7kGw6pE6mfx5Bbqet8EhfY6MUqd9iHNoVrGw9jCcRfGaw2Ik88L1PKyzBNFZgNQDeeqTPPoPLtjMk7CnrmSPHVQVEi0Fqh5m+f2A2zgDM0vO6fKIxwHWX3JX74AqL71J2pL6hxaQBj6uBOAUPEKkK4SFVnJ6YqTiPma2CtW2PHsAy7jDi20Clg5FS+B9ZjHSncI0KzfIp+LSMmgEqOF+OPzDhQjQiWQ0RcSKig3yWC//qs5cMe7dC2QxzHZhfh5+0SHiQxxfBhhNE+PlUx/FE7jbVK/1/x62tiGOY6XTb4kDm1tdsCLeC9M8qlfBt/U6rbdGvaIn4jfW9ZH9aObxOR7vSgFp4cB768QE5fs4zYdmkuZaBlm9tBArmaR3W/AikY7YVxS9BBUxvEHduCFPxESbfAC2cFgJFSYJ2eTWKrupsQ4qFtThNvMi3FDsqi8f5fB24GWSiy+fL0dpwWcalJvuXq8eARRBsz75LUx9mj3bDKQcsvPq57Gvittf3FOCurUzj0Be4wzkso1rhwYGmeM2fEbDAHWx+4m9CkBVjYDdBrTBMIRmaf9IAY58LaVHX6IA1cBcJaAfAgETsnkI6IBSx1TXUppCH7eCRPWxYcTvqaM24idpDhJYUfeJx46A4MMzJx4LYABDvRwmX0OpzjrTb6xmV5ZdDI+5X+QA/cmnBv0m/5ukMNjb/idSp+b+ddqRMBMZvTu4S/YY8AJpxw1sRCMbBPKd4HcTZoWNIaUNZ8uNr9B4ajqnvHgphc1ZAFW9ZEimrXNMQmKJ0PhtglNH/OFnym7wQZbr3w6bwQ4o5kLQXqRzxrqQX6dNqTLljm5/7Mfawm+e+vLvWmNkeM+pVu0/PoQX+EVPUQp8qS7oQDw/qTP5VuyZlRdG8FlTHuoK5IdHdUbAB4PpRPsk/S7NfGnUmgXrjV5hDOBpI/v9jwi1D5mHwZPton361chDpI35nInzdO6DUjNcotCyXsLtMAOsKerqlkZuONffVKWg1LPBN1hO2CbcYh9/1uXIeAegy5r9pd4AY/DPOou7sLyrq76+tvE/JQMWTD95pLySXCNozXLinv1SWEvcMuU1dHP+k+pMEQi+E/kc0mIb/siIvBYgBWTyAEQlV6CUVoGlOrGNvamUhyiH/ToHBBarR1W9D5urQ8SMxCAlLUjLxI28ONwW1HCYvD6u+ICUYkNR0ZF9Mg4EJTL8W0ih+eXXBqPDL+zTee+A/3bQsra1aMJTn7o2WrktMXRPoRz0d/d2uwuZtcPUULyhG+Y0TbNhZ269dbMIbxrSrZ2ZGkLXKalLJ+eujxXrPnaQUMbloq98GohL4ctoY6JGv98CUH//gL83OJyA0jHGw/YLsvfD8lSOy8Cv2JNb5WPLE1lyCkduF80oj1iIhRejSmStwsHWKa9O1oRtUdC74YuFMM11vH+bQefLN4Z9Flw+vDvOCglL0/cS23YBbd0/j/oY8uygWc/Nh65jiYX1SOhwl2FFknlY3g/PsukuHGkYAXw18Ocm2i+9gyNOzVyFul/6JT4xwKvtuGDz1lp0ntJCVsRyZBhoxFN4TY+hbVPTVBXqRqk8WhhXuMAVaob3MSwFSogtni3mHwtT994361w/99B3Uewl/3MOW+tbWRyVJ0t1EBKrDLTQJgQuvkMvCZh5zfTzW1hvYCqqSOSahSqFWJimWzOwJM2c0KS6qZfaeqYtGWOl0+hguG8LWmrFHlvf9FNBULn8K4ieMh5y5UyyCY0fCvW8SfluFI72CQ7J+2lKq+zbjOGLxVUsMUBuZnzX8n4X/IkPTwbAqx4kkAPhl0y7tvS5PXgN782ZWCmWTTBiPTOztoICCILkpnkSMczDJLzGDdRI/b3xjRj3ADUASM3P3cZpMywTBdDQRUm93dJ+irxCql02kHft9i0KFanaHBV25hXph3/KeMxYrckjDa2RuHzE06+rsv0KRosLD2/hiwLkPCU/LutR2NTP/tZDEVJwr6F7q3yUhRvLUziKova9cuUWObhrl3JFhCOmD9xZfAQPCSwRDwmtst0swQMltatzy+iNPLIxOAOE2UGOtu+PNQ7MhH3Lla3mspo+1KAPINg7FKPdiJhBS3/PvMNqdfZba+WG+nRjKDafOvllkuTcpfImBFAS0FwJ3TMO/w331JIN1GeaFehYa0X4fqJLqFrmyDiJLnIlYNRmJapjnRwi9iWbGtM4pvhucBLY3OB0Fma2kPd21yqWALVjA3uzBDGC8nKhVvuM/azgv1YG080n71Qp8WE4h4kxrsZww+qNnshWSicDSFEb+HTXBkajltktwjKB3N7MN0oNeX12INxU/n8VPJ5rdhxhWI4MA/LQqVkMz5ckaVJtPJA+vqNHzB7sKLBnRkEi4uSlR9++DGjj7+yG9DKZlXeCa+TSqUqxboAInqweViKfeLlcf+txKU+3sF3K4XL0l4ufn0isIVN5W9uZ1wUpGzlr5eVI2Cg/glLgdBqc6pxfdfhcvLw807Hb5IJMvh9WbUgodAJnC+/1v4n5FN3iP5fF3rfFJbxV3eUT4GLGH/lZGhorDqoKWjMfBRPhdgYs8cJaJTyU4lmlyorXdWsiK0oxRSH8pl1Znr85+VR9zVk+GYMZLbe5mpqpunzoXv+uPx0N+Zd6LpH6/jGZ/vRWQ2kVLwSbqFTGFB3IJAuchsYsQzSRUxFW3UFHILXiA1/iB7RtFMqDTfik7reTuDjQtXeCkRdQ3E4erYQMJEvmlvfbC7iyPfTF8NxYShWgc2+gSkLGy/kNS1bRnpTnKNkuVSbNn1Dm15nWOApnIHqo1I13hEZ0hZGu6nWT3Z2eVRREDs57old0QQuK9H4ydVZxaf0souF5aB0owBM5/399z1jhaWk9YGSlNPbyJsl3yluLXkBHnUmnt3oWpb3yg0jKU3v9JzW9vceVzqjOeudmZavdz7Au+YpEKTWaRBNUCjLSCTfQacCUATwrQ89B0yxtjVn8UPfbbMHM/IB8a7/HLED6EpyHHr/gfx3xdMZtgBBQU+0Qy3WDVe/TnlEDMPlHMgHyBFK4eHRymE6tfJvQ7kKgxgCNdBbJ4w2kQtggL8Cu/mDRwanSCqMJrcqmKhWJnv5w06imFEzPKdTj6r79c8CrZfzfT4yNlLz5p1ehbjQWFi0D84OJYihW78AbxrG7IPL+h5JSa00IrIKzz27DVNNBIGSmrWP0OT/7pjKIDa4jzYrP0yJmPRSbfW3+G42vk+rpdyrBucP6tsHJVmZTTapGToUINZT80Ue0gAHC83/aMJ2EY+UgEikWWXtZ/5hT/4NH/rENRzGBMQtDHlnecd9BQIjo0/Py9ETyRmyOZU8o6uycfb/hk3FKwZRWnzapWrnBmcw/IvWZFUJfmGoXbh5irvaC9SHh5QVSX5pKmCGzUm5RqR4xn5sIKpCjf0ZBDSPywAJtTnRjYW92oYaYJkfEptRC/sq2txcWnoyK10AaehStix9MC5N4ChYyT9Lml99YlRRFnxQYeEi82DJBOR5SyJl2CcStBD7jc3usIJyp8l975FLm4jNrmS9KPtmPVU3N20D+2ky8MncWHR/v+K4qVGiOwm6LnQSn53QoKJ3VoX8dP2xrctgBeErFfFGATEE3HXNo+niOOVgS5h/95659mhyPYybPR+S0RplMXIpzMSpjh2grAxzWfmpCVz9nAPITtXxS5dsWI11sMdtHDjc6NeHN1QkF3E1hsm70Aa1MQcYsSNQL3FP/iB0lHFepaUJxHOVpKAGbdC7rkwh0/x3lVzxfIGlR4oCRPjDglAxrJwoZmKMw93RaxwOhScErcR7m4tGCir9QuqtNaHiAcbO6cj5qFaMRp2YYpBq3c85BSDIO+ziJkJ4wxdZs6mggFUg6cQ37UlCjoY/fy9aOYjyW5WMAEhrZesxM088HK396p4domP2lKTyZQHVejkCtS3dP/3cvYZ5cz+8wE1YNg/4MItTH5/azlBTYX44y7kVZKFMydUj6aGL+mgwWBvGAFZFJfNpz9n1P+9oiP0QTqBkjoZoLwghuHT00OXsOZp8OnRCOox6w4335ae4Bthpv3xALxpa+PjY3/6UNr3oVxwf41UWKc/oOEPkeV4IOuRdw8GiKAADltyHuvtrkmJEncqr3nQJpC6g0np/ctSQRVgX49k2zgibCVxOY0Moc5lu3m/DejRgMRNoIWyQo02MoBX+0IlQgvw+Hu7j3z+E47iDTU5GjBxTsNX4W5M/yTG+XM/VtkrJhIvw29UIeTTBTQDpAqid9wLkbPssT6G/0fynvUFlvs7NdYQOzh+4kzRO9uvYtH4GI7G8+Agu4M+Sd9IVnFCpe5NBBSVaLNN1rIqyPDVEyMCXjXE/9ODsY85bCQixLwVCZG2iUAFe5SAX8rS8UChXqSjHExqk5zB8i2yl//XJDCGrgVW4SeBJQQ+ZFbAvzLDIBnUcEf+Fi0G/d0G2FtSn/0rQHUMaUKJ39X+5CQiQsB4iJlbEAWC2xfnP4t7S/rXwF1+Z6/+f3MF5eDzQSaylauh+jPckzEYo4gPs/zqPPuTMfPtMNZvC5d/u5RYtgyGCZMZ1N1D324qFZM30Deo4XcgxvtmFZ47LHPp5VFTmwwp14eNP9AvPwbDjBRvKeyGKtaGx2E38vHmu3aLDlwWi8xx8+D1HJ0DKazkkPjOlUdqCepUovD6dW7gqiDkwLVeYcpJ5ZKjJOpe6ft2mT8f9qz6WHYuBAlTj12c2E5LciTEcy7Fo0n4xUEw2osnO7g3K5IP08uKLOL+mbuy5+haJ63gLnZWEXm6NWmPpXR9Grofyd4KupwAisswiEHHDdUWdiZ6EETb5q/PcPnb7y04EZAwgIAHvnTpkHD/5cwIrEZph0OCEyDpBUWXD/Usq/TGDjcAyt6GZPdIJGs6EBWYOFcTC9DKT9qGhHl4b0xQlSMnPh4Z3TN/jZpwr2p5aBE+eOO6Md8tWt2ot+GvuGJ6l6tQVXkM+IXwhnPkJwzWIELilxnVbn9J7JyFKfLptL6PoGbtroKGA8AceNh4jQ2eP7OBMwSl7phzfAPOCZs/NtTUnj8FCQpamvNvE+Ac7S3BIKtO3lFObIZ5S5JBboCEBJGaczfz6Hazu+pHsYnrv23euwwQx2X7PLzlQ5p/yJbKfpbPJjpPhEVxVZZkOWXtYhoA1HHM9d/IhVzGGqQyS9wP3YXAgCWrfHy9CZlvyII/SftLbcC1Jhx+qnboFerJCbPdJw9iioDDEBAaAjPyCnFpWi47HpcHfU6RM5dK1iQlbKHzTh+c1We+zaRBe1pJANbmo4M1WYKUeLYu2/3Oaiyf+1HoZcoUZAIm0/m14qrTKEidMDsXjzn8y8/7RDoFxcAaQJPmKewlpeW+17w7FAFrKWAI7eI9PxznITWxzGh0ryv7EfBo4geAqIOUUWr16BLYs0r7lI6SNhFlvi4UoXA9PYRyXnLE+k0zorbJIbLheyASg4gbfpy3mkFInHSycsyVG/RTxU3EIbvb6VNQhGPPERu3xyGPTfx6J4bP8PirIoAym/mTDLiRcZZ/FmSzj9r9MYlm45637VxIW/LHKUo2GjBtOVFjYXSYv3TC3az8CYmuQFqb4sraefS9LpFdBISLTpTNiovCl6ymo0gDZy6chr8p/QyD8eSgsD+zQn5wqi9RyDaQyPAvDqf/BsNEB230c98GfiN3yuI1TluDVitQV//drRaDGOBdDdCSeCqd/RJ4uY6VwUQ1WthQGl5huHKz8ZDkD+rUC1G95On2emlntnnUZOCpIa6GwRqFrBtwcPEzkAC9kcwgRsUkmRQa6eqgLYfj2t/WO2lvOYHnK88Y098JKX1HF8/2tntnGtYF6Ilx7zhhcPLoVnFnU/WVFALj4EW7u/hatHyHQKp5zZtn8aYG3zkyrGmEDZy4ul1LcKojDmofGerClge2wyvfzGh3DYjITbJtOCx1LGB5gN9Be5Jm7a874ZLsSyzLEVArUBLPNnZrgT5Gm0tA7zEiSwpdxZA+mgczkh+cCNQcTF8AQDgT9Xa/CAIXBllFCclHb52fPbcK/h67LkzxybNyip2sNdYoLov0e2pXXwpMLffUHkgWt6LleEaxTFVpJH+fsTulJ+KZJjU8iRxFy/M5T8OsBhfrb6JruwN3k4DGYKj7/dvYH9XFTG9fj2NC6x/Kz6//fsJ2pEWoRX2niNEIOZKr+99159cZXw4Fk6onWucy4LzkD/FUzylS6wXoRPL9KeXFhCkRsoX6Ayr1OpeIAn6l7m6ato+SBdHrE/HLF6hdkZNtCnOVBSmUXWeWOj2hvxPcuUoprVWkjV+uBZkH6mpBPMT4tyR0/hVrGCG8j3He4v5Zvae1KCFW9bKIWwElF+KyELyLkCnpyWeKJm14OEeyQQNY/vx+Az8FPOwK/SUOfj6tBR5tFWqfEWzQ2AY5woKbGhKvXV/3lkglrQtRfoZX75j4SctSA17Z992hvkahknnawN0DsW3wblc+WPWe/BC/cnVtllFcn/UyAVgXqH2I3fgdAD7V+b41DZhbSt9erhKnP6jNR928+ULkyJCsmfDSOij7Egjcez4bqu5I45k2ndJxwOMGkd2VQNApm8t1DhRqaVYFGI/G21CrEPISiQSRB6PgZNA+xyW1n0GlOcyre0kCyYu7e7Rb6/QegsgH3z5GBlRu2krrlLMrLY0xXEG26RSEWKip/MeGZ3mas78tyFgZWe0UL+lwaeYmewlQrnS/PEE0OBpHE2YE/qrWyr/Fwimw22t65OOhN4l1Vc1z12z20yzynD+szXIjhXxDESrRTI5R+xcNN8ywH+/1FhAJ/pJe6q1RJAgTrVfsGT3APbD9b+m/4td7boCerOmVUPYkm5f77D/mdhAJND8Q7cisMDp8rsO11hX+nPHwHcOAWnGhhduJHqhT+xnXEAt6wdqzEaJFGkUCDdRD+fS6rkKd5hUE/I2oHzuh1stCP2uoS+EoUGSQVvlZPYYbOsLraSSpBfW7+VSEN8LC1bG8mkL0a/9DMc+hk7gsJyO01I9b20pg4w6COzBH2j2rdEEQvCiunBMAnJRZQOxhGSe88/hEN+CceC8n2fs3S0dvSm04DDPwkmxIl1nf2iqZzOycByZvsm/7BnCnGu7M/x4og0y5TxznbyVSftPiB3Nkmk9gtCPHjsekbkrXV15Uhwu1Gx/LD0AJTBSHolGwE3JI+QyBR4e2hJewggxtx7CVKCYzAiZ4tHQX8dL9jQQQGW/F0hcZ4AT60SB/tI/99ju/1L1ehtxwDCMG4w+iPcmwcDlGcerC3FP0BzqEqeOcKdxd5ktLCWFA5G2XGE0LqHncV+GqhCiGZvXUNZ5WJKPBZVFiEEs8Kfz/0Wzro6HZlARiHU7RDZ8nbSRYjbCLrad43/DVeiBcwi1QFOcVALGMQv9cR/gbs5yjN/I3htg+uSjEJBGdlWEbuKvYJ1tgY5zkN2o2ZLhnkeSQ9Dp04JZAuXdbIQjEUPmP/90Q7PAqYGS2mxECCQzbzUOQwrhHS/kNgTTAPAo/UdyFyIWd1yQsVK92vKJ6XlZiCFVHNFVQBhzg+HRywD2/iQb+7opb3ZbMewUY/1zlBbrPBmT01ahJMx38xpFsweOu8dOp6lreSWv++0c3Vas4MCNhUu3BRkSDzeswvm2ppVT3i5hNmCziRKRk1P5Xrv3sLzfXubvOUXA9w/SeCqCtztnose9oSwohDgEHdO3V+PEHzTaMhxglXOg/wMouuPiMPwpBJ3XVGdw1pVLf/eyfT0Lkmpjf+W9eQvuOsA3bHQvHf0AmbEc/W488UI+/oqUbITcdN2cvnIcsYF7DrZrRbOh08+s8bcIP6FqPU7aapeS35z49Fbomy1TPQJMCwgXGRCAbgoy5AsPuqWTzg68aLYYYybtZshcx4MXGnmYiJxjJbos5JzqumgXKHsElKG4tf00wwCuglgO6OhT0oUay69J4tbAhhtM24AagTDea4awx2FithsHoR8L/wxhGMPl9JS+t4blGYS2w0IgSagHT+NWPdKiLrVeCv8ikPX1CTnSIPVqwZj9Gc+ekyZtfueIM0i0ZTu6/kQ2wExsVM7tvsXrmaflW7Z7RpIJHGqpf4N/UVG9ljnGEOxECkmKCbTzglzOEbiqSGYFhW3pomlZcWe5FO78Hn2dcaAe9Bl0k3IvPIp4rN21lEjX1NhAmWFT7U5iLiFE4R/4b7BJBpeFCnkE7njQ1gOiND/jUWhudZQyjNv7zjwBLHwh9UbdcZ3hlx9OIWsyQSJMayQxF8z8UQgVnJ2V93ZBJyQ93w+LxSK2t7BOagVz75x9ISc5kVCTMBxVPldXiA1r9oC3D8HXBxBM/cuG8RKsZL4GwJUagIp6QMb17P4KWRW33UJ0wE49S+XJqlM9VSCoxOdJQn6vm1b2yPaVuHurIgjCdFPkEYUsRuQNdz6/6SOxR0qKvKLVxXftSbV3ZLrgI7O0S9u/NzWw13GUMIzswSFSL5D22HZH1RbruSf2hq18vTRaBX0ff9k9Z5LU4c9JuZqk6O8RqbxfVkoyJog7ZxmO2biSKm0cdLgko84+Q2Tcu861YbBDO39RM1PtlXAXt9Egb236hN7CzsxeJb7PMpnLFN8nY6qHR2AXwHRrYqScws74EDWogOwJ6t45ZrX6zmeBU8XkY0j6ton16e86TNCNzLbBG+NN3CnAEZeFpMPJpEjQyYOtbCC05tTuOsWDCvbtYxD1v8w7aki4Ih7RNccNVVOY0sa5+EB4U7DvYHGWWrYjKW7vdT4rCz2ZsBjCzOki8V66fJ0P3aq13kwuuY55IAnWaMvi9cw/cVpl1JcI1OtYyPwCHwFj+Uo1/40oD/oqZ7OJIlSMM8yywXBxExmpdFwkHb2urPghnxo/vVCpXo72R39ouN9McwxwyRjbmXBr4MxfTiI2NhOv+kUpz4PSGOiQrl+Bc+qCy0dPuFto+Ugh1aAQBYcmHxmsv4nPaMUD/MZ+AbAXDN6UhcD7XQnj6qWqbYKuNZi5sFD19O1dRNzDOcBE4ceEw1XjKxjRv19mzDOMazcPKhSNpk9yyb8Tq0QqQ5WMUb4A70aT/Ho3UWQrWI8gWTah9N7gN3O7hx9MqrA9AffwycQlaZY7n4iQiYdbKNTLrm/OXXEQf7nkbudsvow8T5yheYXz4MkPd8pQLSYdz0d1EqIETdTAaOGsvUAmITJ5LVmZ3xAgbCtfQtOFUxUSUaKvSw1gh2plP0LTi/dlfoV1fm4KY6t3ErVSOrxUniJKnDn2KgMbOPeUiacVATH+0wtUi3lPrU78/DeqShx+tXXV6SJTXUfh3mnOGWsm8Lhs5DBYxGNNS1iOrX+u+BHLrGl+G7A93qhYr1DuTar3td6fszPgP72/GFMRMPy1QVd1bSDW4q74K82DiCD/SltqrunuXI4lFDyzjkBCh0fKu6Oadv0e6PhQ3xlPI0XkzC9Ts1FHQApX7a5huRltrxdTYeI8cobnDrQG9S6nSzwzAlg+rvXNraN8XDgKayVTAgmpfjqPKm+Z4vLOCZmB/M8x/wu6CSRr1EobWAvpIkHFB3m347lXV9/CtE+RfG2M4yOPlM1kHRG7T7MMIgrwJcdgewpSjBgk0hgxdoHRibaWXfuWyxuXnT4SYYavvl4kthnLKI4UzhIZCZSykNGdz0jnDWd5IhQ1eSArksUIPH3E/uFlvmcioNDTxM+yTQmK85wVJVhbb5//DLqlfwZpJ59eU5ux8jplD9jD3JE7RDYp0dJSxLJgjwiy+ofT5v2wXsUQ/iDd/iZAnolU09za+MTlCt+3fF64IR49Kxv+vgpJGLK7CMTAffSu/JHEBNy4lEBES3dvTWvUoG6N8KK/RMCEQUPlCnyqaAbtW4eyAyHlLayJkOz3EQRj0IlLlUHqgD9vicsBxOAvviObpQVngf/p/UNsj59yC/WdJI/PQ3sQ5G9ed9o1OYzNCSy333fuVXS/JLaUTXWDgufBPNYnnQEbQdynoFrfVsJqUvFS9P7/UOh2g0rUjTeB9SA7hz7NLsr0cN3yyG2/TFuOvfRf6T1DyEsXs9J579ekcoO9haYukQeSK3Zh0/jefmYKqe3a/MmtZsMrpouVFnSZHdRCxGdbnEDPY/RFDyW85dyLFGJMfLA3GM3tkiJUGFnQsA+gNH6UPE0BGGn1WWRpTu/YiJcoAFlLaA2j3EWEDS6fr0zw88VHNH4JW3GNJqzD8t4btst8yFMd/cjdqUb/VO36rZiNfTXigrXFB4unjlJNjX8Gaq36lFRMmDsBFtsSGsJ9ZvORTLiIIvE5PDESfvlXWaCgAA7fp+9mYn4gtXbNiL85OzX4NXO9KVLpQ0CYNpFvRFsqVZReSVaeDNjnEJMzArpQ2sySwigyDo7wQV0bcF7hSxo2yr06lOp2x6Ma1g+dJlRYf1n2QqwCn+SYWTqOucvRWssdQJNtWdhbNc8T3IIOoJXKNWfFElPWOCkKXkfM5LFHlgm6FOZUEEKaOMg5yNdD77DIzd+kdCKqnfYbCiF80PdPyU9tbCgrhR7E70O+3QCa/cgudj0Ym+M6fXcRQq0kenr9jQoBjJ8nGFf+SMyASm63eHlcyT9WKKTU9mBP6uwOdVPJKwY5R7QI9XLJKxuAVfVSV8Wc8SFh3PemWm5yehdY28FbMearSRu0tRi559E0d7NRPAi5hx+FNVD3a+ENM7TtzyWQ5/xyObivVpyHTGQehMiQPXSU8YZ06SUlzx+4zDDUWVUy1gDHKIdR/Z8RIWRydd0BdaDHp3aduYnb7uYg2bGfJJMkLIfO3CaE5ObePgKNVqI3QUJK5tUGWewSnWDJ/mZ4BwMCPWlP8U6DimJESOPdvZY7N3zeoOX9D27KyjoBBmyEze423cZz66AiZhZmCmei/PwbEWL9voUY8lf4D87KVTv3tOwv2k2G5SQadMPtk5R8oKMikvG+ww+G3tx8ZaY15dKU2ioDi144Of1Q/tHWJEm0/uycHyL2ZIKpNmjucMpIzaH/p+N4KZTpYlM0fR2X+9pyBRicWgvL2wXSvrex+Ad7b0n4EMV1LQbJzUTFkepxo5el9TH+zawvZQA4UnDptR7eoVPoWO5OKB4KY65aRrW2WDcGWqK+C0Ng1f+km+LCKxY0F3wIPmiOu6obbHRKyLy6Q4A70ERDpk6rb4aCLf1G3X0SHKAXuvhdjiYtO5OHdl56siU8jWj0O9lNL3KrVQu+CLGD35Wk2vLD2VnoiF9bOMyzo+SyZemTVDifcDywZizqy99yseKazUSQ6k4XjfoyGwIhbEq7r7amDg3/j0aSRkZZJRQj2uNQgqVDjG3kHIQMj3WdipOtsib8qrUdOPxa4FJNDWdQ9JEizHwAwiTo1+fNr2l77xceBDVX5R+4NG3DrYH1YUq1tXIvDlXHhDimze68hkZNTcCPUXn/Xu4SD+NKTt+jEvCZVWKLfkEIQUuTHs9mv5K75RJFK3vTHEbtqy1YfMC4VNZbSFJsZOdUXXl8+41JSud4pvhjO96MXG4dkFaN8btCvjCE4FEDwMjPN0HPTp0/r+8l7Qhy8hz0UZfQaUn2QzDG2WbgKjZiFaifQib7DTnresr8vRwSVy2PJB6glp447ozwwe5Ykceku1752mbUstvrwKMrhMSVBge1YNhWXrs3drhRDNlNQUtV34aHNgBJcM4KFU1CRFwkDtp52UP77rPrmeG+s6uik7x/3+3fJQF2cbILNtQX2HshHjw0pWSkPt9PaQzxCeCJYgG1RbLIFzcVZ4hh6isoZ64Ily+AtJIaB0w9otUutPsmjVKPL15yXGeMhElzu4JEhsZWpPg2/yLxtC1Kgb2k6uS5RC11BQa292zhqHpo8v0IwkJlJd03lTtdy9QY+Dtvs7hICE7nGsr3Z8D3pFOWniptSWDQ6GtmfvDnxnalDs22mLuMMkphkgpYx1lSzYljzqv3khGsWuD36PbYv5upTNTZq0vLdAaFiWbaLDh4Olk2r6DWx1WhF+iCFBI6aWpioKJO7xAU/KSy7z5uElzMnyTxXhtITQlZgstfxPqL3ZdM8+eJ1smvQVTTiFqcino3yhbCgcu0eaBMjTWRZZmPRvdFNIHuh0QLOfLhSVNoaVKBvE3RjTxdzGc1cfgKgicJtipwLarRqFcCjjydlo5M98j93bz5OavN986UeGiwV1m3rh52GT3sVZVqcfu6+F+e3J66X1l2AB10slRlYKTkgeHz7bn4fOfkAt/XoCDVOnUvdF+hovl7AJMlDqtdTdiX8e72/jlUtR8shI3FD54IJ3OkKkPQIE5/vEF3A69roICr+guM5qNB4o++rsB8aDcE9Kp5zkJTUFarq/NfIxcvGmQN6qM9seTRn/DiD1/sAFkCUz0MlHTgCRMcjGjdtoj4abbzoarbEXAidAosgXREALWLT7vyyOg3WygklIf3VnCKNuE4w6kSX8XxcX0ma5rvsn5fnHApO3lG1zzqHK5zy7dsUVnR6GDL7dX+KW6w3WWpdRaQ1irZYfviEgQDiqp26nXsoUzmhg/nWXCDal1XZYsz3iwJ5GgI/2LqAjDJVE8FGoW4aQL89OteXblsar6drQzTpdJKU7VEK8wJ+XYEIeZiC5JDlOsFDzz7FXvS0SzytnFsq8n7Ae4CMOgfc98ajdEzPFx+py+9iv3HtVb7u1/5ruzhsKyukdbwJEZE6TNsoMhwEs6EVzy9247ZIZCzTDIjcx/+APKur7jhVXEkJcOjFTZwP9eDZbx0b9CmUrUrXnHBZsynctosRTBp0A1JUWGt+vWdEXyhBYo10OewX4ISNFfXtvsLLlm8zEskzP67uBsJ/Bpk9QKs16DIEodfmRubX8diFOl8YtnzAoySM6gpreT8ovmT59r9ohjJp+F80jMEQmk77NFhfXfxqPzbLktMGgMajgAeDncs6rz0DVDFhcrDA3pcyPZP2F56X0/hKIL9QJYWdNQIldyeobSlE8ArjDw2UoQHtiHi3WMN4hfCZhGCVetmzhainge6elKs9I/FsCSoTYH+C/5UYSBVMPp9b/GASM2VjV7jPh+8rrMxOrLPl6k6tXjfxM03zGt3dWg29/Bv9XH2+0+y9F/Pz/PnHQdcazb1LSPL4AX4+uPguMRDezJhb/XS7tz55wcMnetOqfQBr9Fe00m5BFOdj8+Fs6OU169MWowwSJ52QLdJJJfUh35R3wJFIqQEZaFMnqnrj9gdedFrat31uN2BoFGOy7EYbhtJH7vuTn53TSYkamr630v7mJQKldCgxeJLsWdu0woPfiE/TQkZ2KDDXiPog/he1w/GLtcPAbRekTpoIHZLzc3bj9epELwPsGC11f6E/rrjpPpZ4XBYWmhHy3fL8SnUO8ZcJY3hf8RrOQrdbw4+Pe3O16F7xDyQfOw188cPnNb0rhJs3LFChGfXLCCrL08ZxDqq03zSpQs16vFUOigYBQb4D+SvVZC1/rpL6iApBt5+b9LCan4zeHJ8xCCeZ1uAeKFWeMwyLv6sODaNJrhgBdqr9J3wvKMrDmD8a1K2rvkiDwwIX01Tbi+Nb0lpOUWwMiWXo1BuDO4IcpAId5M6kX/Tzjb97JZ9lgIVkHoC8X1g12xqxm40AJtYgrxiytRIRt4XxrSmK5XAQs9kx5jdDPVqr2WsZi6O9AAo0bp54kuHylPM5dULuaSEzc0l7olVD9ywFjbxCFkSBVQczezaGNw8sova4e/tmn88vtgGBa5kBYDHXg+pP5NHviTZrCyT+a08PniJ76fhqDPBIa7+HJ4ppnHeiUwsWeU6UcunAt+yhpz70d9WY8mks2wcXSOwhJoaEy1zkQgYHBRnVdOvkJQ92d6p1dKs2GqSrxNrGaGzsnanUQaGK1V6XBglowjcNB8mRUf/89MdAc7GIMIFIzbjM+nZj+RrcFuE0imJXhJwaT2XLKlIp3lSLHqKFVD3oS+ZUsMYvMRY8e6CL4ah1KAClYgkVZLAl0PVdzwrbn4XMBNWiyYKmD/QVCq2ku/QCKLI+GajWj22PStJppMZV6mM312atjxXq3CX8z75RUR+5FaLBETWdyCR+Gfg7803VkMrVGOzZ9EGI1Fan+w04zy1r7zPPcqnm0+Ac1BuTyFJMh4j2f5yvq3B4+F84GpKCPP/G0jTu2xsv+r7CBU7EiyMlrv+tQwxrHX+ACx1FbgybGWTSf1VgloRHcGbOSLBYwPcE0F/vKn4t3qn+3lXXgDFVGlkm8X71LAaYo78Vb+EnZVxzdg8S5FABjgDwoiNXD8sLHNkul/fUOmiSocRJHpoZ7LCXgOxnoxgGzsBhG82xmwGYpGQ/9qzPuNSdxOldqRs27Yr0+cB+207NT8nUEj/0yAibaqfEpPWqBzMaoKhukOI4mMgYBZR1ij5itwcimbI2/8xp4hb9j3awYwQ0XH2cgxRnLMruNKLZnH/pBRW7agBRp+6bIGmdjkcVedwqVB62OxHF/bel/eULfH3qNzgRykEbMVzg3zFqOgQ8rmsZoDPFTEOMxnmuDWt21R6KK3eulTvxybskz1IVRqbjcZcJLCPNon8qaVt0mmIcE/fwvdCYUa+L9L8Zupbm5EzRoyW3Zbi0MBfRU/v2H4+5FO7UmuekHA44rCPt1lpH3YeCmRxj6qCqqZ69E2Yx9YDbtapy/7JadYoV9m2jAZDUVl2pmUNKc9mZP4oTl6c3/T9xYk+dNzkyPzEtGJS/DUMmyNp5BdownbdwfqvcajRS08pbx1bDmTSqXCBQ/nR6uxJbvo6WA+2iq9aBPkPZg6MkHOxJUXBhtzOiLM2XPjcm253lhYP53l7xsuOfX7Ql9b90cU15VMnLg8rmAln6pVAWEXXTKs8qrVjw+2wQ++8Y+EuqJQ2YPZy68pUNLNfQFjv46znOKWGV1kwEVI95da9nmrDO30QIkED0AyBG8qFjm43q4zoYTlK0HfyGuLQj2lpgMWh5BCaY4/x/oByXxk6xLecLAe/BAQqa+3XcSM30FrWYGZYaMj8p0hIdznnbQZezAQm8bl72VTWibSCKPgLffn/+4xutZx5XyRoLS51puMoKbRVM7tfT/VNduM5fxRBrDtD1OmX16ZzwB2Be7v9gDcDymkaL7p/4cZWLgfcRjN5MEaEbJ3Ul6f2ejcOkRWFwwuM8ihEhC6xW4DFFVKOQjltKhxK0/eQ4tQdDYuVbsBUe9FpZN6CpmEla/fGVNxnuewqp6Vi4yC3Bxq4zpfGU3HvdSyygMJ6UX9CJn7qz7CUbBf40exZ8d21PnUCLKtxNW+yjMoHXU1VuW+tjRqne8r1jcPNYNvFNzbt7KafXt9ZqR29RR/fRq64HIb/x5KNj3DPYhx0uuqN83V2h0YWm076fnuRG55SfDTqUzJYtBgTHnVl5704kVAQUPJeaFqBaKtg7O4vvya0tontxjKVDBonPO2J44meBZDdJahlP36G3WS3F6tLpLppZKAGW9uHvo1AZiS6V5fPY+zIYqKMa49hVbQSdaQ18xDkO+ipiRajuyMqcgXX6axZFLJ0RUXb2r6N/Su9wkq90oXH5ls9Qh+d+1khJxsy4lncX6bcCFaAf8ImdEqetuw9DcEK9RP9TIPy6F3sjNk7gJWQBb6LnJ5HLgjpzM8RWfd46ZlPNXfgfMo3yc6CnjKJS02x/ipnOQvxmDtEYI/3k2FSVsH0RwcWcUJ7IrSJcYG9lbv9VYQdTt/EEoQlls322J9rmVizdAkJ1EKdL8Ah7d2QEvmEk62h97gppmA9N2NA46IkQEXUUU7Aq1QRuTf9mrAl1OMvo9241q3GuiG3GoBtiDvIWd6RACwLTs4Yr/nYCzaZC8JHaUfv8IEeCpE/v3EdksC9gC3ztFnlaInf3V92IvTPZZrWZSX1OoDXJWmvxZEAT9Ez6aK4fdX+09hXbQd/j+rsi77zOWo4rpZ9CZ2OgvwluagZglbWaLdtu8VqxkvaYh7ga8cxLCBKZZ9keJn+rGySwPL2/zV70BixFyoo2Ab960+/c+37KHo4gU12A3fFZAm8w92DP5MRpFQEbHCLVsWavEu8Jetgl2QGhd1UD6UZqk7uEIlFqU4IbUk6cYRcNX7X5td8+lGnuKaUwwiFu8GEDdbjSHZxUyIt49CKYG8yuUVTVQFcEAYHIIJjDz2oXY30LYWBw7Y5X6gBWWBO/NjYVRIkyhIMlprY5EcMbUa3vOhZf5QCOPvqvy67VyR7dSWD4fLLaEp+vG/PV1PODDL5hQc1sl7fYys34aP29MxqCS6sEIg0GqsD1O33byfvPJ+MODrJrNzf5zlDu6Owr3n/ZFFU0+KkLDcwQNvwOHrvNfFmWg31jsHdJlJoK6AsCMxkY1S9CqD56lPyUpjSBDsK6VInLcOe2PDLFjvXFUq0ZdustGezgMKfAJ+JeLCUgt93Yu8y+uDDEs/0cDoL1LYx76IZVqh/whXxRkiMkNK1Z6KBKmphcIu2xfjQIbMNFXzzmW2uFyz+is+CRfFFDlfeoczDa5wRFQKoXaIMJZttV/Fbm/QZJPCI6Xo3sK+cZQADwj99bN2cBk+2g6iWvy4YYI9DBNAJvXnujnp0QyX3hnOn12UyLa60bjkNnE6+rrbNv6KDY/X2Zix+yIszrbem4yPzDZS6CagYTvKJDsRtQ6VYagzzAoFwCifUxXQARhjAUxSt7mLo8G0QvB+ScVLCo/nEGisMDdcyNAok2RupjcCGPmdzGWNuhn1TZ1HV9fgblKRRFUp/zfJQ37d9s/kSNUZ9R8/UeX2Zbrd6TO6iIs5oCMIBefJEUwkpvQiP//Ko3rEBX0yN0MFfaG6I47WLAWFQ/v3omAXpnWEM3ywv8EFAnT5NK/GkCKYj6hzRFfpO8EpFO9hIqb74Fd4lXUito9YCg2nKPb2pKvx1E3z1MQBzyWzKX592C0OrmjgedqingSLm+LZwA+pDWfRC8h/7mGdLl2WS0Dg+K6zqc30yNHKqCCt4IySXPPL3c1ph0LTUGiHJv0BJ8gDPRfJDqoYOxk356m5t4u/ZTqBvwFGHjHzjEmL6kVtNteW8oUSskQXKKeX235ChZOEZ+k7MQbaswotvZ1/wIcmWiE3g0KEFXkSrQqe5n1nQZ/19TcwQJW1HfahU3eyzhr9lJYJnnSpZmoAjxoi/4guk7lK/ViO0L9VHxVtmnkuai4yJfcVxRzFVLcSuAOSrCKHpongwDSu4vA8abMaMPn+yhD9xAgg3/+abxXfoyZZdXcuu0TDABzty3fFBLp8JLAa3PPs2bjFLBElpTcXx5kgpKeHxBR3fFJ11Cwd82FBZzdpL3/xTgn3vpP6JIblr8ECLF3fLQ2nGeVOueINjLb8+TI53uFzGBFy/AJP8jMWqia1eFpuLstlRjujCsTzuFza8rhcJwNumgvM2gysgfTdVEX7raAzogxPZjvFQ7Iyqlk0eB79oNIqkcqZC6+TXOymS+xCDbQfz3TE/VDgQq8ZtrzX+BWw9auGoe3f3LwnNmz03CgYKgxjC753XQG9HcwoDJldV6WjfaL10uFdHEdfMwtZd6vSFU6LWn/psQI1vU20S7Apr0s3QEr0qlwy6adWG+RjBtRaO44Zer1TCMMwUZmDFdVHaZqhiXMhlEAWnD+lMI1f/CCwJikkoCVSFKWqxFFmYCZFrX708Y08NO6UoRLdVTxIFOurS551d1TwuQC9mbthr8AcwsYy8aQV/2ey+q5+sxxDRg6Cf3z13nsbIiZO7b9SoAKqnxHJ98hbOb6759AGi6qBzUiOxM47zqzOiwuSI3AlUuG83bEkmecXBYNK/Zjj7B0BIdpqCUd6J8rskbNTBwqYrElBPdnD/SUSAl4+vA04Cz5BkwCx/aFHe5FmeveXB7R7vqQ9r2OaPP1/wZjNtyfDb/n6xMaiqs8zycoFHsDgbQHuHS0e9pe9bGzOATl7i1NRRrRumh2PN6IdT9iVCpuFokMp/XOSOi+7da0uo3TO4teE9czrpvdJrNgOaT5UdUF26dWt19OhqUYE0VG0EflHD0bB4G1ZPEzN10WKuuNpP17sSChJVNEHAxTxs9niXHg3Y7SHEe/ijQxmuwEwxPPE9/7RUQWFv5sJvkYKEAwJFpFT8dq//VDmbprxELWIScpZzAf36CWq4SIH2RwhPB2M5ShkO4BrkBQoMK5MmL8xKO/gUeoPF0SeFRaQQKr7Q+dtwwso4XoFiyleCdyLvYj6Z1z8uUSRoG/nmnZe1uTlQnHHjcu3zXQ+SgVCHtz6qTkidwB5qkyVk7cBpJbz75O//kYA9NpbuAj1yK/cDygMXYu/swyC9mJe++hcmTi5kCsS4Y018Ih2FTRHgLNG0k7OK7w+aF0nC6D4HzWmQ1NxGBQ72uDZU2qXP0L61CkxP26tQXNmslcmD2IKQi98catgg78vDAKHiDEjTjNlnnPcgJZmX3phs9hRmy5jmhzulQzfguxvKKW0M+H/2VlJNHwN7mjKU+WQYQZ9SMJqAZKxdhjceb349c8CT1ZvDBGOgEjnffVCs2OGhS/Qmbgk3wQ2HyCoebAtdwa1kzhwioBhnDe2Lm5hMKNlfTfbApnU7JFYCiY+WBOPbnqK69RADkzyMp7sGG5UJb7hm69tDOvKz78Q7CsprInGWoObMRZOzXJkS6KLnyT9ck4fBHM7n1Ri2JZ9yH3PmZfPAvG6/VtoW7XHCDZOZgh7K50HgQvgHp3kdSbbGR6g5YVMuMe2qIzWliBnUmsaF/JxPau5lJqVQWUhSso5bx9yR7aJhBBthChjQUICWbqm9l3777AsSkkt7GBFgzZ/nKsTrq6Hqu5E3szuQSLImDqTvuKcgcrsdGLG0vPq+cpY4t/+Dljbg5JtQA5vG+A1Ykn1vdXn1x2RtQuqF2Tnk8jGzY1ATyazIahMbbvKvxarRDY+KqNVEf92G+Rn1eOSS4K/rNXndN0QDylzTc6nCAapvCUlHwHa2IqbWX/fgfX0OkITQq6v7kOwxy/rmLzLulae4GpPGkZSuYcFlR+kFG5ZycOkwfgUJWhueXnSOzKBRLSop3yFeM/Kiwp+MYyfodiW8M61oMufhjU5Bi74EAMuf8a/R2kPCln1hl0hdRWVRBJDT/HUSYo685QL4BQl9g+RfK0IsU33DE3dZ6yNqp3atbRjrof0QRBZ1YltgoTakSGU6Fsng99K84+PkZt+j0GALst1Vr4UNlqxLc+9noDrUt5GvE6uVrlsCLFLOUAxpbF5//WQXYGGKfd03FWi0ARbbfWHINzTdqukgeRwpiB7+FkTqPVGgFYhOSyNEuhtnqvTPh65dSA28s1LR1O+s6w1Oxx9NcTH+rzze0B8vLe9jGZKBm7TV2DF7p6SQ2l0rzfark1lpwiD2buOD9vt8O7T9ctEhvZbHY+Q1junRO16IA6JiXpy0heF2JrxOg0NB23bmO8NZ2vvC9ORLZFCkmZb071H4u5TPEO4bwU9os9TURhXRocRuWedO7VT00JOHt9W2c4S8Z1rZeTlQCS1g0WL66qqbgNmpWYcTaXFOlKX9ZJ3vzwL8q1lLU17/eeL6O8L2707Ue23oSJiw2RLq+YNvzhQPJ9CdCktRZ401Yn2OVISDKGuk3xN7/g3Dx5jkLQBp9/JiytMEhicEOTNS2TIdw46lVeBwEzgjEyUGGuKsLlddQFR+IkVNXNqzxy30N0LdfNZTomnZt3R/Bp/jzX6jAPJQ9/vtjCXk9jUd7CqrIQB9XpybfbWR/fskxR/9s2WRO/AvYxqOc7TNdhgWO7HbST8iaRQ9b5QpY9EHa3lMINUCNquQRKgESXfa9ilPfsNklR798MDebRJ/zPz81CwU6dtkGBavC1IpLLUy3DlWT5qLNICzRqCfOaGkOi5WgtQC76M6KGDmP77z9AWdg9lW8PiqHtfCoGQinRc8w+y9tbjphoWuUKTQLYQ+ZtrBF98q409ZN5QmgPcq7FrnDC4jgMerkfOmWdBbKBgDcAGron/A59WfcN1s+F4jYb0tEHPh1TMQxjw6ebPdPZq8XANV4fAF8cifHngh4EeQIrg6ol3iM6bbqJbAZydBh7rL2mqJElfBspwoOgrQ1+lIyearUgZS9xaHnfWpteNTTdMciVWUW9uufn1U3fAK/IXDTZoq79d5SRyoTWoTL3hX+imOhtT60WBHpo4jkmqrGlgzKENKm86HeHZY+yRA6yluH7UIlw0f56KxM8pu6uZiVnnFgjnsw8EXJbHw+1+Vh5LcPCxSXDdro4eTMzjuH3R+zw9QPutNZRRdu+Ma6dsGuGElI6HmqG129PmJkbXgLzzmChHVZMihTlCFHAt22pmz+DuZxyWgU9n4gjfcV3UhGG1cyHPNmPw/tq2NXn+2dskuXYOEYgsQBn+rmlqZ0MyC8fjeAlmveHIhZ8Mrktk7ThrkWKh4aQ9lMrF8AJCUAf2nQHoeGLsMMLxK9hBau3O97eyzd78ynPuYubRP+BnwgfZORLtaorqy+dt/Pn44/Z10eDbs/zPCXUCfl01Yv1pluVR+kGmSNkn8NDS6SjUvD6K2q9cFp6v5/EG1Z+WM4C4RqiBlOfYHgg7mf/akW92DVqElReb7C3aMff2rnx7cklL1f7Ph8mcfT1da3dMTDHnQp5nPfi6fviutLr6bzyK1vcbpWPa906d/3dly6wU+J0Mrn8Nmc8bbeeYA5wqY88nidjyJrZtMs6sN9HkvSgEXI5JvfiadRxvRb3TNbpOUfCr7XkfFtdiPuSXzIKA9KrqGR8JdUv8x0CMXL/26uWP0++lHQaDMEqEcZFh+xV96AGxprui0o5gLhTCmPE9JPWoP/LchiGYAzt7/QIfoZHjHgHyDmaiBI4kYrRiSEzahXEHWNagLC7GKHLDIhPkdDG+JwGx86OmQtpSXmvJMPI2jYIlSclnVPVeTns5w3yTln4eLcXJMhk+AgU3TWfO0baaK8WvyBPiDT3V77fJ2dGD8OvMSYErYbdnDgx+cdKG+Sa8OJEZ0H2HL9j0mPeowGjK+sGxQ2woymGO4Aaimgkmb/Y+OvTksqNZUohMsm3+no82/z4s+ndF9aBqXlfAnXT7FvRhP5xVukIgdo/IKzVwRqt33oKrW+evVROEyy338+qDvegw/AYY+VVOyOXwMfLYXpqLi7baj+gatdR4LlZs6BHIlrF4U53/f65Pjx3v4mvi9HlOWOn97Z9JxP2mWJOyFgGDjzsiyROOgaThUyqYaXbtzV3LGVz8yxUMEP6SVU3IbgGqKlgNzbUrC9mKmwr+tTORV+jMwuqa/r2ItwcoNQPJThsXqG4zuowYg4vxlHGkG4QZk4TNuaGMHg7pI+DnIPcI70QikdY8e147GGpoO+oyrcgziM/GJqamWGPaSZrCAFkmvocQ2j/HfPbJOIPE5uvGgLo09vAEFQdkCBQiTlcNi8jhscwvaJ4Pe7PC6jICzDqW6jgVdTvrJX6DmhQuG7k5tTVD9qSiaNcuTXTKzkFgckB7IJSsWktXDFLBxD4vkuIpZC8RO5f027ZekihXJVhx3CmcOTLvgWY35CoPz7ofj5yYCxP5aJbp2WiQXC28KYupvv4MwjTKXPn33FmqvecC31I5b7gVupVz/oYOXjPRLH8Ev1qAJ8hLQGznLuvQQCgnXFsfs7gQi0EqvALIx7AfNY4voxsb58u93fhKYpQ4PTolDOE3WQwRvXv4SEOCx5HZKpjHgYj4Q4xInGjF+TFUUoAalWc5MMHnqGLbsb6dOMZv5tUCWSvkKRSAyOGuAzk3msof2h1OV1zBoJRFqJeTib5BcpHrrw7GHiSmkvcOKVxALXnuBeI6/4LSl0NwgiYteJpVbSL1v9f4z90OmoihHvwQgqblyFI4+H50W/VdQ85SyTogxyr+DXq/C+Qgl6G62EE3qYYFLzOZAVFXb14CuuGnYeiimI3IQpRxaWaa/nsymjguhLUf/XRIwKdX6Im82zq61r4I/cbEhy1bse8m8+BPZcyzByec8v3dytK7auUugibZE0W3RFSc34e01AOMXWGZ1D4K3FXc/LpeyKeZTtN0Yxtel6CDc0HeomiP2SKCQ270lM3KkXSU8SD+774ksCZq+Rg9akftW6RMcsMJjv7UggHxqU/WO3+yTXT6MNapvCrLOQdKi/DEabh7QYWrrinpUXw0c07vcjcac88JjMM9EZPHZ+UcMX83EnBzrxIgN2s1zpgZrNbtAu3H/VkhIlONlJCKfMBqbpjeL/UFNTTUQf7qx914eIY5DAxLgEAt4OGeBHhDDTuNS03YnhdUPXGL8aMZ/z+aQjKec1/sBFy4qhwsC9x1dy39y6nZZk5azFG5jxVbfT6M+6OujnxmV83RBtVIQJHkNLlGmEzAycNxmU2kA4ztal2ESraAkJ+xpoqmd58rGIHQMewVMeRoaieCmmvLsDSRwTwUR5+ovFIQH21Ogptigu6dR1fhqgqT8qN1niYckGZ474jd0n/nXeCjmlniuuWec2Y4u6P3D0vn7FLORQnmSg7Xq/80z58l3HqQtLL2YEJxdYdvxxhrWqoL9rmkWLQbeEfFo79gfpBqAVq44FTcrSs/86hs8UFxdKVIs3/QCEGLJs7A8K9BHi/2Ga5UVYxB6R/ry9EuFiyw51cYV0gAfI2/7ic+AY/82JoofmCQP1/lyC2Sw5t6usY22r9mbd+8XvXqmC28grQCLlGBBbkrIv57SqpPi+R3Hd+8Qgc+hacgNYT18MM/HEV4OHQGk78mn2VJwo5E5sqCfTYaO1Ai0fGReoL26hrCgL8SUs7+zVPiiKQZcGJHseYWxYPI2Uq1r8rnfmbNXBr9+RNTXtGm/SwS7ERaYbG6fiYEToQ4snDNT7wPuJJNwc+6qNwPhOB5n2h2NssYRbqpGqr3uTuQgC7s+N0/TU1GXELeblzEnv76p6zbrJWYvCfi6PWtup6MziszsU/RXYLYf/a0CGinR+wFqzDDtlf7BFyMWRYQMNampFG4f0ScWFqgC+BkLjJCNhTJMHY+kTq/DUzG2+O000bzIBXetvDdLhJtuOVFfiNQmZUbAgStQi13SfIGk//opgPpsiv3qL8fbdj+CXKl+5csDrKeBSCKQpcaiiFh/xXcjhqzK+XjSfp9cm166l8OuhpNpNBTihpzF+iTXXsVZJ9IZ1ZoXj1Ku+xw7ofVQay640asUhvy2xLdkmjSL8fBiXCmGdysQYUn/DyoeAB1DT3vpoKZ9KoXTiI6Iu+TpwmtyPv75EClGA0fKxtvTacLNMuXt+SV91JsPB9/dCC5Z59Xjn3aAipRU8twIMyBfIxa4y9erpDvqUdzdXP5L+5imwthu3usOJjDxxNGiBD08+Dqlab3eRNKSmgS1Iu6LMFzCMEFoGt1IomREI7fBGNqJDIUrrT6YF1DGs6hWSIGzl1YpU6EsZBNl7byyKaa52IoEx0avJWteNOPcYHxF6GkiTAfXmMZANBBb/RYbuzUp/yqgqgrH9Xj0UmD63b1Kr4c3RDn5/KpyVfnbybHEEVJw+7KNe6Sif6bVhHH7xogUGHLuidw31WZdqCN4/VUGNbEvldrw7Gp345Mp0Y/EsnECvyXJfgROIZwvzthvlRQMSU3qckdYnTyxVy1URR5TL3kC1Cu4FvqY5KBxkhxMlqLIS+EqZ25/jkxcf7UYWKbDXLVLTkN7xj4H9Xw9ZYhQxdefnTFOLCIa86W/mvMss9yWyK9qO++KnrmLpnbDN6uX+qMLXrAjgDYylRwJXeK91FKdyh9MPaX6XAveUGE+v/a5b3XdJS0KsCIAjDVeIujizHlwrWJY+6XyCTqeV8viWS/BC/b3Bf9MFP2A0B+CRoC/7ev+1sA2buBXaGXbJ0gph+cqjwU4xbUQU6Ku9Z1iS+C2WHCSYbRZCqJ9ZIcZT5DfNAFGNWd9mGt0ZhhUoXDWA4saGeF9SNKJ7wZFHlbtY7IDx/xAonLdDX14wJSQlZYPL/GgnNDVpEgUJEeSraYogGrW6ko9nGH9eiJ6MUhfBJEK+MjN71eI174k1x4K3679wWxybEyXSOHUb4JupCOioDUEbn7P8sdW9lpUCkBTCVG+N+h8vB2xqZL8MJVZhoTApVt6ISdhfN5SmXVazv3YBWFIFw+Neyx32KmoTR193Kn6VxoWZTq8+CxurnBwV7xnIukkjnQ2wmDul6cgPMtfclrRvKybBdW3dFHpvncnjZnW/NDSX44/kFlLPC4axKBkPow8mhIUf2y/O073AO+6F4I6i4nm0nm/fSNcrByXQRinPBbhnPRIVUQ9jNXnxCKHXVxW13i74m11dwDzLOg969i/8dTEVqgrZgZYSgXQhGtapSCsaJPtx99OmxYH9g19QSJJEPuA3NEdPdccV58DmTLYJiy1o8hkwfZms+LU3XZZrpq4eMK3pZwBylpes5lkfczY5wzO5usa4ajfnO6q9CVgV6HKjejmmeWOZTn2Pj936xRIbwFYrYrIouUrOW+guEXXzXh20nMQqcOJRRCbWiZR0h45r+EVlwyzDvW/2u8iLzgeOcUYHRCaQe/VfIIJJRr1lzCd6sUgaVdNu1RQQaMSt+xKdf1nyWRqghG401BKrz79uCSyAPcLD472Re6a4KchHkbANfDpWa1cmxrTV08MiWRUHzSW2GMSSMSDZPQLEYdSDTkfSTiwlCGGZ/AlzmIgZREK+ksSRVrq8Mx7drLEzAPfU4gkCr3N0G9SmioJyqpOORuDNbrbuyXIeMu6JVz68xUoBGWRtZ/svrlDSffrl/JfjhH2Atheg0durxoWMlKqZmEN/Oc36An8ILGRt4vT8IBWKjxWFBD76GXEjue2QDxSBvATvYrjzuqZNwNFlAXOzyBMqR18liuJn/QJX9CRWb8heG6e8l3OLeubKCB84T5+W7Tw3Z2UJUHjkwOlx0wXBnaazR8U9gSZOpIhd9Lkp4iV4rEWt0sgE4MCtPBmsp87GOItk/iHlsofXBLqWIC3tX+GiRp/KVcE+TOmN0lZrGFt7RZNbn3LDKRx6VJXJlY1T6yqxLGYeHwW2c6taq4lZda8sO4qWwpSTQF6jU+ujfH7iSyOqPwM7emBFDhMVwi+ZLnUtMJgzWNCKOvrMUrgt43k4VTSiMjxhhYQxbGAU/pnn2nYCk7C/OmHS1QD3k1kpt146Dt9sZo8CFRl6hz/o22U5U8g/NKcY6G9M7ageVD8dkXh5hczkK+J4EYaRpoEjOBSesaRX2Q1zpMZzGb+2O9ROvFJZ+Y+c8Z2QZejDaEbGOEb/O14Eo7IEv8CHNMVqvJtfwV7VzNIU0BPF18RqZWqfTMZ5APjXbpSDD5s2clya8pBgjgCwO40OD/NQtSl4kGLHFDXWajZ4dNRTI+wujYM1R44REJ6NesZUMlktpHInYZVqtYl8D7GBR0vVteEm9amL6LJWv5es48lRfY0hyu9TO/9m0D/MkKu5Ngi/vjPWH+m9EuF6mebufZ1Jlfj/YQYhr4Qxung9RTFVhpooE8XT+kY3s0M1Kg5LPVWjqAmvZly3BzEhrsKfs8Us1FTxQmTNd5uvZgHBESqSo5HKApF/MBUl/yTEaQrZBpulEifq5r1LUDt1QdUcxxiZfegeWz/F7Bnw+u/Aw7EvCZOa7bIjgy7ce7Srcm12tzCqeCG/A1QzMnHsjAACR9mQQFj82ongsLgl1Z7b3DysWHhlQCX9hjx5MaJMGf6xCBR2Bf8tTvsDge1s937H9Sr6N/JwLz2QU8sMTznWwew97cQIPd0sokkBbBZDdGiAYMQsF0hNqZD5XjKzuab1xE8OIFiFIMUXhF/q1g+fiqyvqzgZ79qG0Mz3M2mtQ2SRXf3Q0kQFxiVLD5PTHTj2yxi59bxxvRjoMt37NF59unw2ecDm+WFlfrMENliLdAPHar2a/pA+QcZ2J6tmz4bpWgiqsZTWB+MjPYoHmg9OGQdCJk0sgSfR+miqzXWzhj2FmFi0O7QHxSJQndd6VZELzyZbpULYOT6ydKL0TLvV9SVoFTcAGnIvv+va6RxQBUnT/lI+UI3U6TtFl38GOCuLJCN+FQTIrvpmXgkbjpZ51LLFrhXsN75EiIq9O1Xv1pVjHm8DxMfAZ8xq9WsYzt+8ZCc09er5IQzTptMjk+yQ86DMOFhsJzJeQFEtDcprsa5p8Xz5W2owvF3NTUmehT3djM0pmIbS6JBJGAPPqTrE/v0uMjOuMvBU96ZsgDvzCyie7xlTdAvemMIRtuX+G9UpCaZNXxHWIsbWLPwQEmb/KJ5ssfXYWmjAGV0O4ZtVs3d5aOviAD7jNnh2vs3AlnuAwEO23qxdEFHh8C+BNPhMq+3htq1PNwtrc5xfTFuPPPDbZ0vUYEUnogWk+AFNDd/rAUNkYDQ/ftKQtYuPm/C8i2CZFPVJNTENp8LheJ2qU3gxZd+JaROrbHPTzAiNs4oI/znP+2KNAzYZekdU/sikQSKmgWp0ZraP8mVapmQDRwvAZcj3AuQMZkO7MX3LxWBXBBfhTEUFXKnJ22KmHG+aKs69He6cHEaY0qDxSuCZLXZg6IbbDthe1OkIFTExZhPAPxyXxlcf3hTMxpbmO+SaRbVuJgEnsR1NxENUgk/TA2bW0l6Wj64ySefaol33/Cqac2wB72fOhF1cpTo1hD0xDiplLB1IunYpVZYhTbqzCy5Bzcuunt7KWI6RULGhDhqPdScOEBMNVM/1Ywu31myVfa1R2OnYiVTVhzB91fSw4Vd7N3kUhGJLjD3PltpoELkk2OZMF6B/V0WiA6XHyY48UYd9H4BN0VzsmvEntZPY9QMqxU5SQ+ZfO6JqA1mE3q2QKG6Y42V1bSVC1Yl7ioNz1N+M88I7a+yzs9mrOLyJxSESM4BtJaAqsrBCh+NBR2kAJJvyUUJ8wSffSFYiOlepKXO1E/T6YVgHbdFUETCalfBU7i9dP3nyc+fo+sFq]]></content>
      <categories>
        <category>项目总结</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈看论文和记笔记]]></title>
    <url>%2F2019%2F02%2F23%2F%E8%B0%88%E7%9C%8B%E8%AE%BA%E6%96%87%E5%92%8C%E8%AE%B0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[本篇纯属个人体会之谈。 如何看论文这几天看了些论文，遂总结一下看论文的方法。我尝试了很多方法之后觉得下面这种方式比较合适：找论文找论文一般的的途径就是找综述文章，找博客总结类的文章，里头一般就按时间顺序排好列出若干重要的论文，打好基础后专门去看与(🌧️这个表情还蛮可爱)问题相关的文章。看文章 下载文章后，网上找翻译，对照着翻译大致通读一遍文章 通读完文章之后，上网找对该文章总结的博客，越多越好 结合自己的认识，参考博客总结一下文章内容，尝试复述 如何总结文章上次看到一个博客里头关于文章的总结结构十分的好，值得借鉴一下： 这个网络是用来干什么用的，有什么好的特点, 网络的背景及作用 然后直接搬出代码，讲这个网络的网络结构，简直一目了然，网络结构 讲一下结构中特殊的部分，以及好处，网络的亮点 第三部分讲损失函数，这个也很精彩，因为一个网络知道他的网络结构和损失函数就了解的差不多了，损失函数 第四部分讲测试的输入输出，讲清楚就知道怎么用的，网络输入与输出 最后做了一下总结，清晰易懂 ，总结]]></content>
      <tags>
        <tag>tip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GAN系列论文]]></title>
    <url>%2F2019%2F02%2F23%2FGAN%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%2F</url>
    <content type="text"><![CDATA[GAN系列论文Generative Adversarial Networks(GAN) Generative Adversarial Networkssubmit time: 2014arxiv link 生成对抗网络中含有两个模型（均由多层感知机实现）： 生成模型G：用来将随机样本映射到真实数据分布 判别模型D用来估计样本来自真实样本的概率 G的训练目标是最大化D产生错误的概率，D训练的目标是最大化真实样本的判别概率。相当于一个极小化极大的双方博弈过程。以上是模型的结构图，生成器的输入一个随机分布的数据，判别器输入的是真实样本和生成的数据，做一个二分类，最后通过一个sigmoid函数输出样本的概率。 训练D来最大化分配正确标签的概率即$\log(D(x))$.我们同时训练G来最小化$\log(1−D(G(z)))$。换句话说，D和G的训练是关于值函数$V(G,D)$的极小化极大的二人博弈问题：$$\min_G \max_D V(D,G)=E_{x∼pdata(x)}[\log D(x)]+E_{z∼pz(z)}[log(1−D(G(z)))].$$ $Pdata(x)$：真实数据的分布；$x$：真实数据；$P(Z)$：生成数据的分布；$Z$：生成的数据；$G(Z)$：生成器网络；$D(X)$：判别器网络。 GAN的训练过程首先对具体的问题定义出生成器和判别器（多层的感知机）。1. 训练判别器（discriminator）如上图，判别器的优化目标是上式两项期望和最大。输入为真实数据和生成的数据。由于log函数是一个增函数，他的形状如下：因此当判别器将真实数据判错时，第一项得到的D(x)的值将小于1，当判别器将生成数据判断成真实数据时，第二项的1 - D(G(x))将小于1。由上式可知由于错判将会导致上式的期望接近无穷小。因此最大化这个式子（等于0表示完美的判别），可以使得判别器尽可能对数据正确判别。1. 训练生成器（generator）生成器的目标是最小化上图中的式子。由于第二个式子在优化的时候能够提供很大的梯度，使得算法快速收敛，因此通常使用第二个式子作为生成器的目标函数。式子含义十分明显，就是使得G(x)生成的数据分布与真实数据分布无限接近，D(x) = 1时，函数取得最大值。整个训练过程如下图所示，判别器和生成器交替优化训练：全局最优解：不断迭代上式，上诉方程最终会收敛到一个全局最优解。首先固定G，优化D，得到D的全局最有解为：$$D(x) = \frac{P_{data}(x)}{P_{data}(x) + P_g(x)}$$上式可以通过对目标方程求导得到。将上式带入到目标方程里去最优化G的结果，最终我们将得到如下方程：即两个KL散度减去log(4)，由于KL散度的值大于等于0，当且仅当：$P_z(z) = P_{g}(x)$时，该方程取得最小值-log(4)。我们知道，当固定D去优化G时，优化的最优结果应为生成的数据分布与真实分布相同：$$P_z(z) = P_{g}(x)$$上式恰好等于D(x)最优条件下，G(x)的最优结果，因此通过迭代的方式去优化GAN的目标函数能够同时得到最优的生成模型和判别模型，并使得生成的数据与原始数据尽可能的相似。（具体的式子推导过程可参考论文）。因此判别器D(x)的全局最优解为：$$D(x) = \frac{1}{2}$$例子： Conditional GAN由于单纯的GAN的生成器太过自由了，对于较大的图片，较多的pixel的情形，基于简单 GAN 的方式就不太可控。于是Conditional Generative Adversarial Networks提出了条件型的生成对抗网络，通过给GAN中的G和D增加一些条件性的约束，来解决训练太自由的问题。 在生成模型（G）和判别模型（D）的建模中均引入了条件变量y，这里y可以是label，可以是tags，可以是来自不同模态是数据，甚至可以是一张图片。 $$\min_G \max_D V(D,G)=E_{x∼pdata(x)}[\log D(x|y)]+E_{z∼pz(z)}[log(1−D(G(z|y)))].$$ 在生成器模型中，条件变量y实际上是作为一个额外的输入层（additional input layer），它与生成器的噪声输入p(z)组合形成了一个联合的隐层表达； 在判别器模型中，y与真实数据x共同作为输入，并输入到一个判别函数当中。 常见的输入结构如下： 模型的训练过程： 论文中作者使用的例子：在MNIST数据集的实验中，对于生成器模型，将label的one-hot编码与100维的均匀分布的噪声输入融合起来作为输入，输出是784维的生成数据，与数据集28*28的维度一致。对于判别器模型，作者使用了一个maxout的激活层连接输入数据（与生成器输入相同），随后将maxout与判别器相连。 pix2pix Image-to-Image Translation with Conditional Adversarial Networkssubmit time: 2016arxiv link pix2pix网络的主要作用及特点pix2pix是一个基于CGAN改造的一个做图像变换的网络，在CGAN的基础上修改了生成器G的网络结构，及判别器D网络的网络结构。同时引入一个生成图片与样本间的L1 loss增强生成图片的低频信息。 pix2pix 网络结构以下网络结构都是用了conv-BatchNorm-ReLu的单元结构。生成器G(X)的网络结构：生成器的网络结构是U-Net结构，即encoder-decoder加上skip layer的类型这种结构在输入与输出之间共享图片底层的信息，有利于图片细节的还原。具体每一层的尺寸看代码注释：1234567891011121314151617encoder_1: [batch, 256, 256, in_channels] =&gt; [batch, 128, 128, ngf]encoder_2: [batch, 128, 128, ngf] =&gt; [batch, 64, 64, ngf * 2]encoder_3: [batch, 64, 64, ngf * 2] =&gt; [batch, 32, 32, ngf * 4]encoder_4: [batch, 32, 32, ngf * 4] =&gt; [batch, 16, 16, ngf * 8]encoder_5: [batch, 16, 16, ngf * 8] =&gt; [batch, 8, 8, ngf * 8]encoder_6: [batch, 8, 8, ngf * 8] =&gt; [batch, 4, 4, ngf * 8]encoder_7: [batch, 4, 4, ngf * 8] =&gt; [batch, 2, 2, ngf * 8]encoder_8: [batch, 2, 2, ngf * 8] =&gt; [batch, 1, 1, ngf * 8]decoder_8: [batch, 1, 1, ngf * 8] =&gt; [batch, 2, 2, ngf * 8 * 2]decoder_7: [batch, 2, 2, ngf * 8 * 2] =&gt; [batch, 4, 4, ngf * 8 * 2]decoder_6: [batch, 4, 4, ngf * 8 * 2] =&gt; [batch, 8, 8, ngf * 8 * 2]decoder_5: [batch, 8, 8, ngf * 8 * 2] =&gt; [batch, 16, 16, ngf * 8 * 2]decoder_4: [batch, 16, 16, ngf * 8 * 2] =&gt; [batch, 32, 32, ngf * 4 * 2]decoder_3: [batch, 32, 32, ngf * 4 * 2] =&gt; [batch, 64, 64, ngf * 2 * 2]decoder_2: [batch, 64, 64, ngf * 2 * 2] =&gt; [batch, 128, 128, ngf * 2]decoder_1: [batch, 128, 128, ngf * 2] =&gt; [batch, 256, 256, generator_outputs_channels] 判别器的网络结构：判别器为卷积网络，结构如下：12345layer_1: [batch, 256, 256, in_channels * 2] =&gt; [batch, 128, 128, ndf]layer_2: [batch, 128, 128, ndf] =&gt; [batch, 64, 64, ndf * 2]layer_3: [batch, 64, 64, ndf * 2] =&gt; [batch, 32, 32, ndf * 4]layer_4: [batch, 32, 32, ndf * 4] =&gt; [batch, 31, 31, ndf * 8]layer_5: [batch, 31, 31, ndf * 8] =&gt; [batch, 30, 30, 1] 网络的总体架构如下： pix2pix的出彩的结构：选择PatchGAN进行训练： 为了能更好得对图像的局部做判断，作者提出patchGAN的结构，也就是说把图像等分成patch，分别判断每个Patch的真假，最后再取平均！PatchGAN可以看成另一种形式的纹理损失或样式损失。在具体实验时，70x70的尺寸比较合适。 损失函数加入L1 loss：众所周知，用L1和L2 loss重建的图像很模糊，也就是说L1和L2并不能很好的恢复图像的高频部分(图像中的边缘等)，但能较好地恢复图像的低频部分(图像中的色块)。 图片的高低频信息：（1）低频就是颜色缓慢变化，也就是灰度缓慢地变化，就代表着那是连续渐变的一块区域，梯度较小的一块区域，这部分就是低频。（2）高频就是相邻区域之间灰度相差很大，这就是变化快，梯度变化明显，即边缘部分，即高频显示图像边缘。图像的细节处也就是属于灰度值急剧变化的区域，正是因为灰度值的急剧变化，才会出现细节。 pix2pix的损失函数pix2pix在CGAN的基础上加上了生成图像与原始图像的L1 loss：$$L_{L1}(G)=E_{y∼p_{data(x,y)},z∼p_z(z)}[‖y−G(x,z)‖_1]$$加入L1 loss增强了生成器对低频部分的还原（颜色变化平缓的部分，色块等等）。因此最总的loss为： $$G^∗=\min_G \max_D L_{cGAN}(G,D)+\lambda L_{L1}(G)$$其中：$$\min_G \max_D L_{cGAN}(D,G)=E_{x∼pdata(x)}[\log D(x|y)]+E_{z∼pz(z)}[log(1−D(G(z|y)))].$$ 缺点由于网络引入L1 loss，同时它是学习一个输入图片到输出图片的位移映射，映射范围十分有限，因此当训练集中不存在输入图片类似的样式时，输出的结果将不可控。 pix2pix 总结pix2pix是cGAN网络出来之后，在图片变化上使用的第一个网络。网络的亮点在于生成器与判别器使用conv-BatchNorm-ReLu单元的U-Net结构，在目标函数上引入L1 loss，使得生成的图片更加的真实。网络训练方便，使用了SGD + adam共同训练，使用了BN，dropout等技术等等。最终结果图如下：]]></content>
      <categories>
        <category>项目总结</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[project one]]></title>
    <url>%2F2019%2F02%2F22%2Fproject-one%2F</url>
    <content type="text"><![CDATA[welcome to my blog,enter password to read. Incorrect Password! No content to display! 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]]></content>
      <categories>
        <category>项目总结</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[语义分割简要总结]]></title>
    <url>%2F2019%2F02%2F22%2F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%AE%80%E8%A6%81%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[语义分割简要总结CNN padding 的方式： valid padding：当卷积到边界时，最后边界位置不够卷积则直接舍弃。 SAME padding：当卷积到边界时，最后边界位置不够卷积则用0补充，保持图片尺寸大小不变。 视觉未来研究方向： 三维数据集 序列数据集 使用图卷积网络（GCN）对点云进行分割 上下文知识 实时分割 存储空间 序列数据的时间一致性 多视角整合 SegNetFCN之后，CNN网络在语义分割领域引起了人们极大的兴趣。SegNet在FCN的基础上引入了更多的shortcut，它不是直接对不同层的feature map进行融合，而是通过Max pooling的方式传到后面的层中，这种方式能够包含更多的信息。 Dilated Convolutions孔洞卷积在FCN中有两个关键，一个是pooling减小图像尺寸增大感受野，另一个是upsampling扩大图像尺寸。在先减小再增大尺寸的过程中，将会有一些信息损失掉，那么能不能设计一种新的操作，不通过pooling也能有较大的感受野看到更多的信息呢？答案就是dilated conv 空洞卷积。空洞卷积的一个最大的特点就是可以增大感受野，避免使用pooling。图中，对于一个7x7的图像patch（a图），只有9个红色的点和3x3的kernel发生卷积操作，其余的点略过。也可以理解为kernel的size为7x7(a) 普通卷积，1-dilated convolution，卷积核的感受野为$3 \times 3 = 9$。(b) 扩张卷积，2-dilated convolution，卷积核的感受野为$7 \times 7 = 49$。(c) 扩张卷积，4-dilated convolution，卷积核的感受野为$15 \times 15 = 225$。但是由于dilated convolution 存在一些问题： kernel 并不连续，也就是并不是所有的 pixel 都用来计算了，因此这里将信息看做 checker-board 的方式会损失信息的连续性。（在还原的标注图片上就会出现一些不连续的小格） 对于一些尺寸比较小的物体分割效果差。 解决方案：通向标准化设计：Hybrid Dilated Convolution (HDC)，图森组的文章对其提出了较好的解决的方法。他们设计了一个称之为 HDC 的设计结构。 第一个特性是，叠加卷积的 dilation rate 不能有大于1的公约数。比如 [2, 4, 6] 则不是一个好的三层卷积，依然会出现 gridding effect。 第二个特性是，我们将 dilation rate 设计成 锯齿状结构，例如 [1, 2, 5, 1, 2, 5] 循环结构。 第三个特性是，我们需要满足这个式子：就可以保证对所有的pixel都能够卷积到，同时层次的锯齿结构对小物体也有很好的检测效果。DeepLab V1： DCNNs的成功得益于DCNNs定位图像变换（平移等）的内在不变性, 这一属性能加强它们学习数据的分层抽象能力。不变性非常适用于高级视觉任务（目标检测）。但不利于低级任务，如语义分割，哪些我们需要知道他们精确的位置信息而不是他们的抽象特征。 DCNN定位图像变换的内在不变性理解：即对图像进行pooling 降低分辨率提高感受野，尽管图像变得比较模糊，位置信息也不准确，但是不影响网络对物体的识别，但是对于位置信息比较关心的语义分割来说就是一大缺点。 DCNN在图像标注任务应用上的两大技术障碍: 信号的降采样，分辨率低： DCNN中多次的max-pooling及downsampling(striding)造成信号分辨率减小, 信息失真比较严重 空间不灵敏性： DCNN对空间信息不敏感，它可以可靠的预测物体的存在与粗略的位置，但不能精确的定位目标的轮廓 孔洞卷积：对于信息降采样，信息丢失的问题，我们使用’atrous’孔洞卷积，减少pooling的使用，同时扩展感受野，以获得更多的上下文信息。其中（a）为stride为2的pooling，pooling之后四个神经元的感受野对应为7。（b）中为stride = 1的pooling，四个神经元的感受野为5，虽然保留了更多的信息，但是感受野降低了，信息更加的冗余。（c）中使用stride=1，hole = 2的卷积核，四个神经元的感受野仍然为7，同时保留了更多的信息（features map的分辨率较高）。 感受野的计算：感受野是这一层一个元素对应输入层的区域大小，可以一层一层回推到输入层。对于这一层相对于上一层的感受野也就是卷积核的大小。当已知上一层的感受野计算下一层的感受野时有：$$r = (m-1) stride+ksize$$其中m为上一层的感受野。空洞卷积的感受野计算：dilate rate = 1与普通卷积相同。dilate rate = 2可以视为卷积核由3\3变成了5*5，计算方法相同。对于dilate rate = 3，可可视为卷积核变成了9*9。 全连接的CRF（条件随机场）DCNN的预测图可以可靠的预测物体的存在与粗略的位置，但不能精确的定位目标的轮廓，其内在的不变性限制了对位置精度的预测（平移不变性破坏了模型对位置信息的预测），本文使用了全连接的CRF作为后处理操作，通过耦合DCNN的识别能力进一步优化分割的边缘。DenseDRF：对于每一个位置i，如果有观测值$x_i$(该位置的颜色)，标签$y_i$(类别信息)，。以像素为节点，像素与像素之间的关系作为边，就可以构成一个条件随机场，通过观测$x_i$的值来推断对应的类别信息$y_i$。denseCRF的公式如下：由式子可以看出来，CRF预测像素类别和像素的颜色强度，像素位置有关系，模型包含耦合相邻节点的能量项，有利于对空间邻近像素进行相同标签的分配。 下图是dilate conv + CRF的组合效果。 Deeplab v2deeplab v2在v1的基础上进行升级，其提出的ASPP技术来更好地分割多尺度的物体。通过采用最新的ResNet 图像分类DCNN构建了DeepLab的残差网络变体，实现了更好的语义分割性能。deeplab v2的特点： 使用Atrous Convolution 代替原来上采样的方法，能有效地扩大卷积核的视野，增加更多的上下文信息而不增加参数的数量或计算量。 提出多孔空间金字塔池化(ASPP)，在多尺度上鲁棒地分割物体。ASPP使用多个采样率和有效视野的滤波器对features map进行多尺度信息提取。 第三，通过合并DCNN和概率图模型全连接CRF方法，增强物体边界的定位。 DCNN应用于语义图像分割中的三个挑战: 特征分辨率下降问题解决：该问题是由连续DCNN层中的最大池化和下采样(滑动步长)的重复组合引起的。为了克服这一障碍并有效地产生更密集的特征图，将最后的池化层替换为atrous convolution 层 多尺度下的物体的存在该问题由于物体的多尺度状态引起的，有些物体太小或者太大无法检测出来。处理这种情况的一个标准方法是向DCNN提供相同图像的重缩放版本，然后聚合特征或分数图，但开销很大。我们的方法：我们有效地使用具有不同采样率的多个并行的多孔卷积层来实现对特征图的采样，称之为“多孔 space pyramid pooling”(ASPP)技术。在多个尺度上捕获物体的上下文信息。 DCNN位移不变性导致对位置定位模糊：一种减轻此问题的方法是当计算最终的分割结果时使用跳跃层(skip-layers)从多个网络层提取特征。我们的方法是：通过全连接的条件随机场(CRF)来提高模型捕获精细细节的能力。 deeplap v2工作流程 将所有全连接的层转换为卷积层 通过多孔卷积层对conv5输出的features map进行多尺度的特征提取，提高特征分辨率以及感受野，然后送入softmax进行分类。此时产生的features map大小为原图的1/8 采用双线性插值对features map进行8倍上采样以达到原始图像分辨率 生成全连接的CRF的输入，优化分割结果 值得注意的是：特征图的训练和全连接CRF的训练是分开进行的，先用DCNN生成预测结果图，然后用全连接CRF进行分割结果的优化。上图是DCNN产生的预测图，然后通过1，2，10次的CRF迭代的结果。 deeplab v3V3在v2的基础上进行了结构上了一些改进，v3，v3+，Xecption这里实在不想在看了，以后有机会回来补充。先留一个flag。可能就是之后几天里某一天吧。state of art 似乎绕不过。]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DP动态规划问题]]></title>
    <url>%2F2019%2F02%2F21%2FDP%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[DP动态规划问题 动态规划方法常常应用于解决优化问题，通常分析一个问题是否可以用动态规划方法求解可以从以几个方面判断： 原问题可分，原问题的数据结构(array,tree,graph)可以进行划分。 原问题有最优子结构。 原问题的最优解将由一系列的子问题的解组合而成。 动态规划法的一般思路为，通过枚举所有可能的分类策略，来组合成最优解。关键步骤在于找到一个合适的递推公式，将原问题转化为一个多步决策的问题。 下面是重点介绍几个DP问题，以及他们的解题过程。 矩阵连乘问题： 问题描述：给定n个矩阵 $（A_1,A_2,A_3…..A_n）$，其中$A_i$与$A_{i+1}$是可乘的，i=1,2,…n-1。考察n个矩阵的连乘积 $A_1A_2A_3,….A_n$ 。由于矩阵乘法满足结合律，试提出矩阵连乘积最优计算次序，使得计算量最小。分析：确定矩阵相乘次序的问题等价于在原始序列上添加括号。通过改变不同位置上的矩阵的计算次序，能够减小矩阵乘法所需要的计算次数。经典的序列划分问题。现在来判断该问题是否可以用动态规划方法来求解： 原问题是否可分：假设找到一个位置k添加括号能够得到最优解，因此将原问题转化为（1，k）与（k+1，n）两个序列，子问题性质与原问题完全相同，因此问题可分。 问题的递推公式（最优子结构）： $$ OPT[1,n] = OPT[1,k]+OPT[k+1,n] + p_0p_kp_{n+1} $$ 由于子问题之间不存在相互关系，原问题的最优解由一系列子问题的最优解组成。 矩阵连乘问题求解：若使用递归的方法，对原问题进行枚举，枚举每一种加括号的方式，能够得到原问题的解，但是计算量巨大，对这道题来说，他的时间复杂度是：$2^{n-1}$算法框架如下：12345678910111213recursive_matrix_chain(i,j)&#123; if i == j then return 0 OPT(i,j) = INF for k=i to j-1: q = recursive_matrix_chain(i,k)+ recursive_matrix_chain(k+1,j)+ p[i]*p[k+1]*p[j+1] if q&lt;OPT(i,j): OPT(i,j) = q return OPT(i,j)&#125; memorizing technique：动态规划法的英文为dynamic programming，programming 这个词最早有tabular这个词演化而来，tabular意为表格，因此DP方法可以直观的理解为动态填表法。动态规划法的一个重要思想就是：对子问题的结果进行保存。算法框架如下：123456789101112131415memorize_matrix_chain(i,j)&#123; if OPT[i,j] != NULL: //如果子问题已经算过了，就可以不用算了 return OPT[i,j] if i == j: //递归法的出口 OPT[i,j] = 0 else: for k = i to j - 1: //对每一个子问题划分情况进行枚举 q = memorize_matrix_chain(i,k)+ memorize_matrix_chain(k+1,j) + p[i]*p[k+1]*p[j+1] if q &lt; OPT[i,j]: OPT[i,j] = q return OPT[i,j] &#125; 该方法的时间复杂度为： $T(n) = O(n^3)$ ，动态规划问题的时间复杂度计算方法为：子问题的个数子问题的时间，对于本题： $O(n^2)n = O(n^3) $一种更快的实现方法，从底往上计算省略递归步骤。 具体思路是：先将分割的长度由2到n进行遍历，每次拿出一个长度然后对其进行由i到j每个位置的划分，均求一个最大，对每次的结果进行保存，最后得出结果。123456789101112131415matrix_chain_multiplication()&#123; for i = 1 to n : OPT[i,i] = 0 for l = 2 to n : //子串的长度由2到n递增 for i = 1 to n - l + 1: //l长度下，对i所有可能位置进行遍历 j = i + l -1 // j移到子串的最后位置上 opt[i,j] = INF for k = i to j - 1: //对每一个位置均遍历一下括号的位置 q = opt[i,k]+opt[k+1,j]+p[i]*p[k+1]*p[j+1] if q &lt; opt[i,j]: opt[i,j] = q s[i,j] = k return opt[1,n]&#125; 0/1背包问题 给定一个集合其中有S个物品，每个物品i有一个重量 w_i 和一个价值 v_i ，每个物品只有一个，你有一个能装重量为W的背包。问怎么装使得所装物品的价值最大。 分析：我们将0/1背包问题转化成一个多步决策的问题，在第i步决定是否选择第i个物品。因此有一下的递推表达式：$$ opt({1,2,…n},W) = \max \begin{cases} opt({1,2,…n-1},W) &amp; opt({1,2,…n-1},W-w_n)+v_n \end{cases}$$算法框架如下：123456789Knapsack(n,w)&#123; for w = 1 to W: OPT[0,w] = 0 for i = 1 to n: //现在拿i个物品 for w = 1 to W: // 现在拿出w个空间来装 if w &gt; w[i]: //当前拿出的空间够装现在的货物 OPT[i,w] = max(opt[i-1][w],opt[i-1][w-w[i]]+v[i])&#125; 回退法判断物品是否被取走：12345678910void traceback()&#123; for i = n to 2: if(m[i][c] == m[i-1][c]): x[i] = 0 else: x[i] = 1 c -= w[i] x[1] = m[1][c]&gt;0? 1:0;&#125; 时间复杂度为 O(nW) 伪多项式时间。$ O(nW) = O(n*2^{logW})$ ，W为输入的长度，当W很大时，算法效率很低。需要注意的是，我们选择物品的顺序是从头到尾挑选，而不是在一个子集中随机挑选。 最小覆盖点问题： 问题描述：在一个图中找到最少的点，使其能够覆盖图中所有的边。 问题分析：这个问题可以用一个树的结构的分析。当选取当前的点作为最优结果中的一点时，从从改点的所有子节点作为新的子问题，否则选取所有的儿子节点，从其孙子节点作为子问题。 该问题的最优子结构为：$$ opt(root) = \min ( 1 + \sum_copt(c) , children + \sum_gopt(g) )$$算法框架如下：1234567vertex_cover(root)&#123; if(root == NULL): return 0 opt(root) = min(sum_of_child+opt(g),1+opt(c)) return opt(root) &#125; 动态规划问题的适用于求解那些子问题存在大量重复的问题，可以通过存储中间结果的方式大大缩小程序的复杂度。通常的求解方式有递归法，动态填表法。]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[语义分割系列 -- FCN详解]]></title>
    <url>%2F2019%2F02%2F20%2F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%B3%BB%E5%88%97-FCN%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[语义分割系列 – FCN详解FCN是深度学习用于语义分割领域的开山之作，他的主要核心贡献在于： 全卷积（convolutional）：采样端对端的卷积网络，将普通分类网络的全连接层换上对应的卷积层（FCN） 上采样(upsample)：即反卷积（deconvolution），恢复图片的位置信息等，反卷积层可以通过最小化误差学习得到。 跳跃连接(skip layer)：通过连接不同卷积层的输出到反卷积层，来改善上采样很粗糙的问题。 FCN：Fully Convolutional Networks for Semantic Segmentationsubmit time: 2015arxiv link FCN与CNN通常CNN网络在卷积层之后会接上若干个全连接层, 将卷积层产生的特征图(feature map)映射成一个固定长度的特征向量。以AlexNet为代表的经典CNN结构适合于图像级的分类和回归任务，因为它们最后都期望得到整个输入图像的一个数值描述（概率），比如AlexNet的ImageNet模型输出一个1000维的向量表示输入图像属于每一类的概率(softmax归一化)。如下：下图中的猫, 输入AlexNet, 得到一个长为1000的输出向量, 表示输入图像属于每一类的概率, 其中在“tabby cat”这一类统计概率最高。FCN相对用于图片分类领域的经典网络如Alexnet, VGG, Googlenet等只是在最后几层稍作了修改，替换，以让它们适用在了semantic segmentation上面。下图中可看出FCN相当于分类CNN网络在模型后端所有的变化。FCN全卷积：前端输入，一般CNN分类网络选择使用固定大小的image patch来作为输入，这个patch往往是从原图当中剪切出来的；而FCN网络则使用整张原图来作为输入，允许图片大小不固定。然后在模型的后端，CNN分类网络会使用FC层对最后的CNN层生成出的feature map进行处理，从而丢掉由前端CNN各层处理所一直保存着的图片上敏感区域的位置信息，进而只抽象表达出它的类别信息来，以交由后面的softmax等层来最终预测出它的类别概率分布；FCN则不同，它丢掉了CNN分类网络后端的FC层，进而保留了图片上的区域位置信息，又在其后加上了几层CNN来进一步分析处理，整合主干网络输出特征，最终它生成出有着C+1（C为目标类别数，+1是为了考虑进去图片背景）个channels的heat map（本质上可以理解为是cnn所产生的feature map）来。由于FCN网络前端CNN处理过程中会不断选择用Pool来整合、下采样特征，从而扩大后来层次的receptive fields，因此最终我们生成出来的heat map其大小肯定要小于原输入图片大小。实际上最终生成的feature map比原图片缩小s倍，s为图片中下采样层次stride的乘积即累积下采样步长。而我们Semantic segmentation的目标是要预测输入图片每个像素点的可能类别。因此我们要想办法将输出的heat map与input raw image关联起来。简单的话可以直接使用线性二次插值来解决。FCN中通过在网络最后加入步长为s的deconvolution层来使得最终的网络输出heat map具有与输入原图一样的大小。全连接层-&gt;卷积层 第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为k=7，padding = 0,stride = 1,共4096个卷积核，这样输出数据体就为[1x1x4096]了。 第二个全连接层，令其滤波器尺寸为K=1，共有4096个卷积核，这样输出数据体为[1x1x4096]。 对最后一个全连接层，令其K=1，共1000个卷积核，最终输出为[1x1x1000] 上采样：下图是一个反卷积的过程，首先在feature map上增加padding，padding的大小为Kernel size - 1，padding部分用0来填充。随后使用卷积核在对该feature 进行卷积操作。该图是一个strides(步长)为1的反卷积，即FULL卷积方式：full: 滑动步长为1，图片大小为N1xN1，卷积核大小为N2xN2，卷积后图像大小：N1+N2-1 x N1+N2-1下图是步长为2的反卷积，可以使得图片变大，反卷积中步长指的是原图像素间填充0的行数。这时候原图中就会出现孔，可以这么理解，反卷积与卷积对应，当卷积stride为2的时候，表明下一次卷积将跨越两个像素。当反卷积stride为2时，意味着反卷积的步长为0.5，即需要走2步才能走到下一个像素位置。反卷积效果： 跳跃连接(skip layer)：由于直接从最后的feature map上采样到图片大小，精度上过于粗糙，这是因为当网络较深时可以学到比较深度的特征，同时过深的网络也会丢失空间位置信息。这意味着较浅层的输出具有更多位置信息。如果我们将两者结合起来，我们就提高结果。训练过程：第一阶段：用经典的分类网络进行初始化，最后两层参数不使用。从特征图（16*16*4096）预测分割小图（16*16*21），之后直接上采样为大图。反卷积（橙色）的步长为32，即特征图放大16倍，这个网络称为FCN-32s。升采样分为三次完成（橙色×3）。进一步融合了第3个pooling层的预测结果。 第三次反卷积步长为8，记为FCN-8s。 LOSSFCN的loss 为交叉墒loss，先接一个soft Max将网络的输出转化为概率。用概率计算交叉墒。tensorflow 中调用的函数为：12loss = tf.reduce_mean((tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=tf.squeeze(annotation, squeeze_dims=[3]),name="entropy"))) CNN 网络优化通常使用的是SGD，随机梯度下降法来优化。]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cross entropy 交叉熵以及softmax]]></title>
    <url>%2F2019%2F02%2F20%2Fcross-entropy-%E4%BA%A4%E5%8F%89%E7%86%B5%E4%BB%A5%E5%8F%8Asoftmax%2F</url>
    <content type="text"><![CDATA[cross entropy 交叉熵以及softmax交叉熵常常用在CNN网络中，作为网络的loss，他描述的是模型数据分布与训练数据之间的相似程度。最小化交叉熵等价于模型产生数据与训练数据相似度越高。 信息量：用来衡量一件事情的不确定程度，一件事情发生概率越大，他的不确定性越小，信息量越少。信息量的计算公式为：$$I(x_0) = - \log(P(x_0))$$例如，当$p(x_0) = 0.1,I(x_0) = 3.32$，$p(x_0) = 0.999,I(x_0) = 0.0014$ 熵：用于衡量一个系统的混乱程度，代表一个系统信息量的总和。当一个系统信息量越大越不稳定。熵等于所有事件所带来的信息期望总和。$$H(x) = - \sum_{x\in X}p(x) \log p(x)$$交叉熵：交叉熵描述两个事件之间的相互关系：$$H(A,B) = -\sum_i P_{A}(x_i)log(P_{B}(x_i))$$ 如何计算两个分布之间的不同： KL散度KL散度，有时候也叫KL距离，一般被用于计算两个分布之间的不同，KL散度不具备有对称性。$$D_{KL}(A||B) = \sum_i P_A(x_i)\log(\frac{P_A(x_i)}{P_B(x_i)}) = \sum_{i}P_{A}(x_i)log(P_{A}(x_i ))- \sum_i P_{A}(x_i)log(P_{B}(x_i))$$由上式可以发现，KL散度 = - 熵 + 交叉熵，当熵固定不变时，认为交叉熵等价于KL散度。 机器学习中使用交叉熵代替KL散度：机器学习的过程中希望在训练数据上模型学到的分布 P(model) 和训练数据groundtruth的分布 P(real) 越接近越好，可以通过优化KL散度，使其KL散度最小来达到目的。由于训练数据groundtruth是固定的因此求解KL散度将会等价于求解交叉熵。因此最小化交叉熵将会得到一个比较好模型。 softmax：在神经网络分类任务来说，最后一层将会输出x的一维特征，每一个位置表示一个特征表示值。这个表示值越大认为这张图片是这个类别的概率越大。因此可以用特征表示值来判断类别。但是在实际运用中，特征表示值的用途不大, 我们更希望得到具有统计意义的概率。例如可以利用概率来优化KL散度，使得预测结果更加准确。softmax它将多个神经元的输出，映射到（0,1）区间内，表示类别的概率，从而进行多分类。softmax的公式如下：$$S_i = \frac{e^{V_i}}{\sum_j{e^{V_j}}}$$其中V表示神经网络输出的一维数组。 softmax在实际使用时需要注意数值溢出的问题。如上公式，在计算概率的时候存在指数运算，当V数值很大的时候将会发生溢出。因此需要对上式做一下处理，将指数部分同时减去指数中的最大值。$$D = max(V) \\S_i = \frac{e^{V_i - D}}{\sum_j{e^{V_j - D}}} = \frac{e^{V_i}}{\sum_j{e^{V_j}}} / \frac{D}{D}$$经过处理后，保证数值不会发生溢出现象。 神经网络中的应用大多数的CNN网络中，均适用softmax + cross entropy作为损失函数。首先是交叉熵LOSS：$$Loss = -\sum_i P_{groundtruth} \log P_{predict}$$其中$P_{groundtruth}$是真值的类别分布概率。在多分类问题中，一张图片只属于一个类别，因此$P_{groundtruth}$表示成one hot编码，即[0,0,…,1,0,0]这种形式。对于$P_{predict}$来说，神经网络输出的特征值经过softmax层，转换为概率的形式。因此Loss 最终会等于：$$-\log p_i$$i表示这个图片真实的类别。 交叉熵+ softmax反向求导：由上式可知，交叉熵的形式非常简单，其中$p_i$由softmax计算得到。带入softmax公式，得到交叉熵最后的形式为：$$L = - \log \frac{e^{V_{i}}}{\sum_j e^{V_{j}}}$$ 对交叉熵的求导：在进行BP方向传播的时候，更新参数的时候，误差需要由交叉熵提供，即$-\log p_i$，然后对每一个参数值通过链式法制都求一次偏导,例如对$W_{ij}$：$$\frac{\partial{L}}{\partial{W_{ij}}} = - \frac{1}{\frac{e^{a_i}}{\sum_k e^{a_k}}} \frac{\partial{ \frac{e^{a_i}}{\sum_k e^{a_k}} }}{\partial{a_{j}}} \frac{\partial{a_j}}{\partial{W_{ij}}}$$对softmax进行求导如下：上式中间部分为对softmax求导，令$$ y_i = \frac{e^{a_i}}{\sum_{k=1}^{C}e^{a_k}}$$对softmax求偏导数：$$\frac{\partial{y_{i}}}{\partial{a_{j}}} = \frac{\partial{ \frac{e^{a_i}}{\sum_k e^{a_k}} }}{\partial{a_{j}}}$$当 i!=j 时：$$\frac{\partial{y_{i}}}{\partial{a_{j}}} = \frac{\partial{ \frac{e^{a_i}}{\sum_{k=1}^{C}e^{a_k}} }}{\partial{a_{j}}}= \frac{ 0 - e^{a_i}e^{a_j}}{\Sigma^2}=-\frac{e^{a_i}}{\Sigma}\frac{e^{a_j}}{\Sigma}=-y_iy_j$$当 i==j 时：$$\frac{\partial{y_{i}}}{\partial{a_{j}}} = \frac{\partial{ \frac{e^{a_i}}{\sum_{k=1}^{C}e^{a_k}} }}{\partial{a_{j}}}= \frac{ e^{a_i}\Sigma - e^{a_i}e^{a_j}}{\Sigma^2}=\frac{e^{a_i}}{\Sigma}\frac{\Sigma - e^{a_j}}{\Sigma}=y_i(1 - y_j)$$求导过程比较简单，算一下就知道了，不要害怕。得到误差对权重的偏导数就可以对该权重进行更新了，CNN网络通常使用的更新方式为SGD。 SGD 随机梯度下降法最优化算法的核心是从当前点走到下一个点，是的目标函数得到下降。即$x_0 -&gt; x_1$，最优化算法考虑两个问题，即从当前点，移动的方向和步长。可以写成下面形式：$$x_{k+1} = x_k + \eta P_k$$其中$P_k$是前进方向。令$P_k = -\nabla f_k$，即负梯度方向时，下降速度最快。这种方法在及其学习中称为梯度下降法。他有一个缺点，就是需要严格求解出整个数据集的梯度，才能走到下一步。而且十分容易陷入局部极小点,因此我们使用SGD来改善这一现象。SGD 算法的表达式和GD差不多:$$x_{t+1}=x_t+\eta_t g_t$$这里 $g_t$ 就是所谓的Stochastic Gradient，它满足 $E[g_t]=-\nabla f(x_t)$。它对导数的要求非常低，导数算起来非常快。由于数据样本中存在大量无用的冗余信息，因此使用随机梯度下降法可以得到近似的下降梯度，而仅仅话费少量的计算资源。softmax tensorflow 实现版本12345678910111213141516171819202122232425262728293031323334import tensorflow as tfimport numpy as np# downlown the datafrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets("MNIST_data/",one_hot=True)# input dataX = tf.placeholder(tf.float32,[None,784])Y = tf.placeholder(tf.float32,[None,10])# model variableW = tf.Variable(tf.zeros([784,10]))b = tf.Variable(tf.zeros([10]))# define modely_predict = tf.matmul(X,W) + bcross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y,logits=y_predict))optimizer = tf.train.GradientDescentOptimizer(0.5)train_step = optimizer.minimize(cross_entropy)sess = tf.InteractiveSession()global_initial = tf.global_variables_initializer()sess.run(global_initial)for i in range(1000): batch = mnist.train.next_batch(100) sess.run(train_step,feed_dict=&#123;X:batch[0],Y:batch[1]&#125;)correct_prediction = tf.equal(tf.argmax(y_predict,1),tf.argmax(Y,1))accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))print(sess.run(accuracy,feed_dict=&#123;X:mnist.test.images,Y:mnist.test.labels&#125;))print(sess.run(b))]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 题解(持续更新)]]></title>
    <url>%2F2019%2F02%2F20%2FLeetCode-%E9%A2%98%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[本篇文章置顶，长期更新，用于记录日常刷题题解以及需要注意的tip。 2019年的关键词：思路要紧！ 28/5/2019 116.Populating Next Right Pointers in Each Node,117这一题看leetcode上的表示方式十分的唬人，实际上还算是比较简单。思路就是层次遍历，然后每次遍历用两个数来维护每一层的遍历次数。在元素进队列的时候进行左右的连接。（116的树为完全树，117的树不是完全树，同样的做法） 12345678910111213141516171819202122232425262728293031323334353637383940"""# Definition for a Node.class Node(object): def __init__(self, val, left, right, next): self.val = val self.left = left self.right = right self.next = next"""class Solution(object): def connect(self, root): """ :type root: Node :rtype: Node """ if root == None: return root que = [] que.append(root) count = 1 record = 0 while len(que) &gt; 0: node = que.pop(0) count -= 1 if node.left: que.append(node.left) record += 1 if len(que)&gt;1: que[-2].next = que[-1] if node.right: que.append(node.right) record += 1 if len(que)&gt;1: que[-2].next = que[-1] if count == 0: node.next = None count = record record = 0 return root 21/5/2019 103. Path Sum II思路：这一题可以沿着深度遍历的方向去做，然后在遍历的过程中，记录下路径。然后判断，当前path上的元素之和是否等于sum，并且当前节点是叶子结点。划重点：sum(path) + root.val 之和来判断，而不是把所有val都加到path上。 123456789101112131415161718192021222324252627282930313233# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def __init__(self): self.result = [] def find_path(self,root,sums,path): if root == None: return if sum(path) + root.val == sums and root.left == None and root.right == None: self.result.append((path+[root.val])[:]) return path.append(root.val) self.find_path(root.left,sums,path) self.find_path(root.right,sums,path) if path!=[]: path.pop() def pathSum(self, root, sum): """ :type root: TreeNode :type sum: int :rtype: List[List[int]] """ path = [] self.find_path(root,sum,path) return self.result 114.Flatten Binary Tree to Linked List思路：这一题太巧妙啦，要把所有节点压到右支上，这时候用的方法是先后续遍历，用一个变量记录上一个节点，然后作为当前节点的右节点，同时砍掉当前的左节点。 1234567891011121314151617181920212223# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def flatten(self, root): """ :type root: TreeNode :rtype: None Do not return anything, modify root in-place instead. """ self.prev = None def dfs(root): if root == None: return None dfs(root.right) dfs(root.left) root.right = self.prev root.left = None self.prev = root dfs(root) 19/4/2019 107. Binary Tree Level Order Traversal II思路：哇，类似的专题好多啊，这一题要求按层次，从最后一层依次打印到第一层，与前面几题的区别在于，每次将层插入第一个位置。 12345678910111213141516171819202122232425class Solution(object): def levelOrderBottom(self, root): """ :type root: TreeNode :rtype: List[List[int]] """ res = [] line =[root] if root == None: return res num = 1 val = [] while len(line): node = line.pop(0) num-=1 val.append(node.val) if node.left: line.append(node.left) if node.right: line.append(node.right) if num == 0: res.insert(0,val[:]) val = [] num = len(line) return res 18/4/2019 102. Binary Tree Level Order Traversal思路：层次遍历一棵树，最近做的题都比较接地气啊，都是大一做的题，哈哈我感觉记得这么清楚全要谢谢林老师。这一题要求把每一行的的元素依次打印出来，每一行一个list。 用队列结构来处理这个问题。首先从根节点开始，依次进队列，每次循环头节点出队列，并将其子节点进队列。然后维护一个计数器，计数器初始化为每一行list长度，当这个变量变成0的时候说明这一行已经遍历完成了。重新开一个list。 1234567891011121314151617181920212223242526class Solution(object): def levelOrder(self, root): """ :type root: TreeNode :rtype: List[List[int]] """ res = [] if root == None: return res line = [] line.append(root) val = [] num = 1 while len(line): node = line.pop(0) val.append(node.val) num -= 1 if node.left != None: line.append(node.left) if node.right != None: line.append(node.right) if num == 0: res.append(val) val = [] num = len(line) return res 103. Binary Tree Zigzag Level Order Traversal思路：这一题沿着Z字形进行输出，只需要在上一题的基础上加一个记录层数的变量即可。 1234567891011121314151617181920212223242526272829303132class Solution(object): def zigzagLevelOrder(self, root): """ :type root: TreeNode :rtype: List[List[int]] """ res = [] if root == None: return res line = [] line.append(root) val = [] num = 1 level = 0 while len(line): node = line.pop(0) num -= 1 val.append(node.val) if node.left: line.append(node.left) if node.right: line.append(node.right) if num == 0: num = len(line) if level%2 == 1: val.reverse() res.append(val) else: res.append(val) val = [] level += 1 return res 15/4/2019 96. Unique Binary Search Trees思路：这一题说给一个数字，求出所有平衡二叉树的个数。根据平衡二叉树的性质可以知道，左子树小于根节点，右子树大于根节点。因此有这种关系： f(1) = f(0) x f(2), f(2)=f(1) x f(1), … f(n) = f(n-1)xf(0) 12345678class Solution(object): def numTrees(self,n): res = [0] *(n+1) res[1] = 1 for i in range(1,n+1): for j in range(j): res[i] += res[j]*res[i-j-1] return res 94. Binary Tree Inorder Traversal这一题要求按中序遍历的方式输出一颗二叉树。可用递归的方式解决。想起这道题，林老师上课的画面迎面而来，哈哈。 中序遍历思路为沿着树的枝往下走，当回溯时，第二次遇到这个节点的时候返回，此时记录下遍历的节点值。 1234567891011121314151617class Solution(object): def __init__(self): self.res = [] def dfs(self,root): if root == None: return self.dfs(root.left) self.res.append(root.val) self.dfs(root.right) def inorderTraversal(self, root): """ :type root: TreeNode :rtype: List[int] """ self.dfs(root) return self.res 101. Symmetric Tree思路：这一题判断树是否是镜像。用树的结构进行递归，每次递归判断是否为镜像，如果不是则返回False。每次进行递归的时候传入树的对称边。 12345678910111213141516171819class Solution(object): def dfs(self,left,right): if left == None or right == None: if left != right: return False else: return True if left.val != right.val: return False return self.dfs(left.left,right.right) and self.dfs(left.right,right.left) def isSymmetric(self, root): """ :type root: TreeNode :rtype: bool """ if root == None: return True return self.dfs(root.left,root.right) 25/3/2019 92. Reverse Linked List II分析：这一题需要定义头节点，关于元素的调换的问题，都需要定义头节点。然后记住tail，head，思路清晰一点，就很好做了。 12345678910111213141516171819202122232425262728293031323334# Definition for singly-linked list.# class ListNode(object):# def __init__(self, x):# self.val = x# self.next = Noneclass Solution(object): def reverseBetween(self, head, m, n): """ :type head: ListNode :type m: int :type n: int :rtype: ListNode """ if m == n: return head dummy = ListNode(-1) dummy.next = head p = dummy newhead = p for i in range(m): newhead = p p = p.next tail = p q = p p = p.next for i in range(n-m): p_pre = p.next p.next = q q = p p = p_pre tail.next = p_pre newhead.next = q return dummy.next 93. Restore IP Addresses分析：这一题蛮有意思的我感觉。它的内循环是从1到3，即截取的字符长度，每个截取的长度都作为ip地址的一部分。每次截取子串的时候需要对他们进行合法性判断。 123456789101112131415161718class Solution(object): def __init__(self): self.res = [] def helper(self,s,ret,index,count): if count&gt;4: return if count == 4 and index == len(s): self.res.append(res[:-1]) for i in range(1,4): if i + index &gt; len(s): break temp = s[index,index+i] if (temp[0] == '0' and len(temp)&gt;1) and (len(temp) and int(temp)&gt;=256): continue helper(s,ret+temp+'.',index+i,count+1) def restoreIpAddresses(self,s): self.helper(s,'',0,0) return self.res 24/3/2019 91. Decode Ways分析：这一题是典型的动态规划题，主要就是想到状态转移方程该怎么写就行了。有几种情况要进行分析。首先当前位置上为0的时候，当前的字母需要与前一个字母组成一个合法数据才行。否则就是按照正常的方式单个字母，两个字母的方式。 123456789101112131415161718192021222324252627class Solution(object): def numDecodings(self, s): # 动态规划 """ :type s: str :rtype: int """ if len(s) == 0 or s[0] == '0': return 0 dp = [0]*len(s) dp[0] = 1 for i in range(1,len(s)): if s[i] == '0': # 必须与前一个组成一个二位数 if s[i-1] == '2' or s[i-1] == '1' : if i == 1: dp[i] = 1 else: dp[i] = dp[i-2] elif int(s[i-1:i+1])&lt;=26 and s[i-1]!='0': if i == 1: dp[i] = 2 else: dp[i] = dp[i-1]+dp[i-2] else: dp[i] = dp[i-1] return dp[len(s)-1] 23/3/2019 这两天的状态和前两天一样，没办法调整🤢 90. Subsets II这题用递归的方法做，我觉得在做题的时候应该要多总结思路，首先就要确定这一题是什么类型的题目。然后向方法，一定唔要无头苍蝇似的，面试题差不多就median了，加油咯⛽️。 12345678910111213141516171819class Solution(object): def dfs(self,nums,pos,temp,res): if sorted(temp) not in res: res.append(sorted(temp)) for i in range(pos,len(nums)): temp.append(nums[i]) self.dfs(nums,i+1,temp,res) temp.pop() def subsetsWithDup(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ res = [] if len(nums) == 0: return [] self.dfs(nums,0,[],res) return res 这一题有一个地方，需要注意一下，就是深浅拷贝的问题。（错过的问题） 12345678temp = [1,2,3]a = temp # 浅拷贝，a随着temp而变化a = temp[:] # 深拷贝，a与temp无关import copya = copy.deepcopy(temp) # 深拷贝## 排序问题a.sort() # 直接改变asorted(a) # 返回值为排序后的结果 89. Gray Code分析：这一题本来想要递归的方法来做，但是奈何，递归不满足格雷码依次变一位的原则。因此本题采用格雷码的公式求解。G(i) = i ^ (i/2) 12345678910class Solution(object): def grayCode(self, n): """ :type n: int :rtype: List[int] """ res = [] for i in range(1&lt;&lt;n): res.append(i^i&gt;&gt;1) return res 21/3/2019 100. Same Tree最近有点儿奇怪呀， 明天想着做的事情，都没能做起来。 分析： 这一题用递归调用的方式求解。 123456789101112131415161718192021# Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution(object): def isSameTree(self, p, q): """ :type p: TreeNode :type q: TreeNode :rtype: bool """ if p == None: return q==None if q == None: return False if p.val != q.val: return False return self.isSameTree(p.right,q.right) and self.isSameTree(p.left,q.left) 18/3/2019 73. Set Matrix Zeroes 一直想刷题一直没刷，很惭愧。 分析： 这一题是找出行活列含1的数，然后将整行置0。对呀python的数组，可以整行整行的赋值： 1234matrix[key] = [0]*n # 对key这一行整行赋值#对列赋值,不可以整行for i in range(n): matrix[i][key] = 1 1234567891011121314151617181920212223242526class Solution(object): def setZeroes(self, matrix): """ :type matrix: List[List[int]] :rtype: None Do not return anything, modify matrix in-place instead. """ dict_x = &#123;&#125; dict_y = &#123;&#125; if len(matrix) == 0: return if len(matrix[0]) == 0: return m = len(matrix) n = len(matrix[0]) for i in range(m): for j in range(n): if matrix[i][j] == 0: if i not in dict_x: dict_x[i] = 1 if j not in dict_y: dict_y[j] = 1 for key in dict_x.keys(): matrix[key] = [0]*n for key in dict_y.keys(): for i in range(m): matrix[i][key] = 0 77. Combinations 分析：这一题目的就是用递归的方式来求解，需要记住的是上一次的递归起点。需要注意的一点是，当一个list要添加另一个list作为一项时，使用：list.append(list1[:]) 12345678910111213141516class Solution(object): def dfs(self,n,idx,k,res,cur): if k == 0: res.append(cur[:]) else: for i in range(idx,n): if k &gt; n-i: return [] cur.append(i+1) self.dfs(n,i+1,k-1,res,cur) cur.pop() def combine(self,n,k): res = [] cur = [] dfs(n,0,k,res,cur) return res 78. Subsets 分析：这一题的思路是，看到这种递归问题，想到需要用循环来做。需要所有长度的情况都考虑进去。需要把所有的长度都考虑进去。因此要维护一个长度，由于不重复，因此需要维护一个下标。 1234567891011121314151617181920class Solution(object): def dfs(self,nums,idx,ilen,res,cur): if ilen &gt; len(nums): return if len(cur) == ilen: res.append(cur[:]) for i in range(idx,len(nums)): cur.append(nums[i]) self.dfs(nums,i+1,ilen+1,res,cur) cur.pop() def subsets(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ res = [] cur = [] self.dfs(nums,0,0,res,cur) return res 80. Remove Duplicates from Sorted Array II这题从头扫描到尾巴，当情况符合的时候进行覆盖。le表示重复的个数，每一次覆盖条件满足都需要覆盖。 12345678910111213141516171819202122class Solution(object): def removeDuplicates(self, nums): """ :type nums: List[int] :rtype: int """ if len(nums) == 0: return 0 le = 0 pos = 0 for i in range(1,len(nums)): if nums[i-1] == nums[i]: le += 1 if le&lt;2: pos+=1 nums[pos] = nums[i] else: le = 0 pos+=1 nums[pos] = nums[i] # nums[pos] = nums[len(nums)-1] return pos+1 14/3/2019 分析： 犹豫要不要用python刷题，发现python实在是方便,这一题用stack的思路来做。首先用/把字符进行分割，然后用一个dict组织。 1234567891011121314151617181920class Solution(object): def simplifyPath(self, path): """ :type path: str :rtype: str """ str = path.split('/') res = [] for ch in str: if ch == '..': if len(res) != 0: res.pop() elif ch!='' and ch!='.': res.append(ch) ans = '/' for ch in res: ans += ch+'/' if len(ans) == 1: return ans return ans[:len(ans)-1] 11/3/2019 63. Unique Paths II 分析：用动态规划做，递推公式为：$path[i][j] = path[i-1][j]+path[i][j-1]$。需要先把第一行和第一列先填上1。 1234567891011121314151617181920212223class Solution &#123;public: int uniquePathsWithObstacles(vector&lt;vector&lt;int&gt;&gt;&amp; obstacleGrid) &#123; if(obstacleGrid.size() == 0||obstacleGrid[0].size() == 0) return 0; if(obstacleGrid[0][0] == 1 ) return 0; int height = obstacleGrid.size(); int width = obstacleGrid[0].size(); vector&lt;vector&lt;double&gt;&gt; path(height,vector&lt;double&gt;(width,0)); for(int i = 0;i&lt;width&amp;&amp;obstacleGrid[0][i]!=1;i++)&#123; path[0][i] = 1; &#125; for(int i = 1;i&lt;height&amp;&amp;obstacleGrid[i][0]!=1;i++)&#123; path[i][0] = 1; &#125; for(int i = 1;i&lt;height;i++)&#123; for(int j = 1;j&lt;width;j++)&#123; if(obstacleGrid[i][j] == 1) continue; path[i][j] = path[i-1][j]+path[i][j-1]; &#125; &#125; return path[height-1][width-1]; &#125;&#125;; 64. Minimum Path Sum 分析：这一题和上一题差不多，唯一的区别在于这一题是找到最小的代价，因此去min就可以了。 1234567891011121314151617181920class Solution &#123;public: int minPathSum(vector&lt;vector&lt;int&gt;&gt;&amp; grid) &#123; if(grid.size() == 0 || grid[0].size() == 0) return 0; int height = grid.size(); int width = grid[0].size(); for(int i = 1;i&lt;width;i++)&#123; grid[0][i] += grid[0][i-1]; &#125; for(int j = 1;j&lt;height;j++)&#123; grid[j][0] += grid[j-1][0]; &#125; for(int i = 1 ;i&lt;height;i++)&#123; for(int j = 1;j&lt;width;j++)&#123; grid[i][j] += min(grid[i-1][j],grid[i][j-1]); &#125; &#125; return grid[height-1][width-1]; &#125;&#125;; 65. Valid Number 分析：字符串的转移这种问题很讨厌啊，情况太多了，总之思路就是从头到位扫一遍，判断很多边界情况。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980class Solution &#123;public: bool isNumber(string s) &#123; if( !s.empty() )&#123; s.erase(0,s.find_first_not_of(" ")); s.erase(s.find_last_not_of(" ") + 1); &#125; if(s.size() == 0) return false; if(s.size() == 1&amp;&amp;s[0] == '.') return false; unordered_map&lt;char,int&gt; amap; amap['-'] = 0; amap['+'] = 0; amap['.'] = 0; amap['e'] = 0; int i = 0; int flag = 0; while(i&lt;s.size())&#123; if('0'&lt;=s[i]&amp;&amp;s[i]&lt;='9')&#123; flag = 1; i++; continue; &#125; if(s[i] == '-'||s[i] == '+')&#123; if(amap['-'] + amap['+'] &gt; 0)&#123; if(i&gt;0&amp;&amp;s[i-1]=='e')&#123; i++; if(i&gt;=s.size()) return false; continue; &#125;else&#123; return false; &#125; &#125; else&#123; if(i!= 0&amp;&amp;s[i-1]!='e') return false; i++; if(i&gt;=s.size()) return false; continue; &#125; amap[s[i]]++; i++; if(i&gt;=s.size()) return false; continue; &#125; if(s[i] == '.')&#123; if(amap['.'] != 0) return false; if(amap['e']!=0) return false; if(i==0||('0'&lt;=s[i-1]&amp;&amp;s[i-1]&lt;='9'))&#123; i++; amap['.']++; continue; &#125; if(s[i-1]=='-'||s[i-1]=='+')&#123; i++; amap['.']++; continue; &#125; &#125; if(s[i] == 'e')&#123; if(amap['e']!=0) return false; if(i&gt;0&amp;&amp;('0'&lt;=s[i-1]&amp;&amp;s[i-1]&lt;='9'))&#123; i++; amap['e']++; if(i&gt;=s.size()) return false; continue; &#125; if(s[i-1]=='.'&amp;&amp;flag == 1)&#123; i++; amap['e']++; if(i&gt;=s.size()) return false; continue; &#125; &#125; return false; &#125; if(amap['.']||amap['-']||amap['+'])&#123; if(flag == 0) return false; &#125; return true; &#125;&#125;; 69. Sqrt(x) 分析：这一题用二分法做比较快。 123456789101112131415161718192021class Solution &#123;public: int mySqrt(int x) &#123; // int a = 0;// a = sqrt(x);// return a; int l = 1; int r = x; while(l&lt;=r)&#123; int m = l+(r-l)/2; if(m&gt;(x/m))&#123; r = m-1; &#125; else&#123; l = m+1; &#125; &#125; return l-1; &#125;&#125;; 10/3/2019 54. Spiral Matrix 分析：这一题用最简单的四个循环这种思路求救最合适！然后需要注意的是，在对边界进行缩减的时候，需要保证仍然满足begin&lt;end的条件。 123456789101112131415161718192021222324252627282930313233class Solution &#123;public: vector&lt;int&gt; spiralOrder(vector&lt;vector&lt;int&gt;&gt;&amp; matrix) &#123; vector&lt;int&gt; res; if(matrix.size() == 0 || matrix[0].size() == 0) return res; int rowbegin = 0; int rowend = matrix.size()-1; int colbegin = 0; int colend = matrix[0].size()-1; while(rowbegin&lt;=rowend&amp;&amp;colbegin&lt;=colend)&#123; for(int i = colbegin;i&lt;=colend;i++)&#123; res.push_back(matrix[rowbegin][i]); &#125; rowbegin++; for(int i = rowbegin;i&lt;=rowend;i++)&#123; res.push_back(matrix[i][colend]); &#125; colend--; if(rowbegin&gt;rowend || colbegin&gt;colend) return res; for(int i = colend;i&gt;=colbegin;i--)&#123; res.push_back(matrix[rowend][i]); &#125; rowend--; if(rowbegin&gt;rowend || colbegin&gt;colend) return res; for(int i = rowend;i&gt;=rowbegin;i--)&#123; res.push_back(matrix[i][colbegin]); &#125; colbegin++; &#125; return res; &#125;&#125;; 55. Jump Game 分析：与某一题很类似，总之记住记住当前位置能达到的最远距离的方法来求解。 1234567891011121314151617class Solution &#123;public: bool canJump(vector&lt;int&gt;&amp; nums) &#123; if(nums.size() &lt;= 1)&#123; return true; &#125; int lastindex = 0; int cur = 0; while(lastindex&lt;nums.size())&#123; cur = max(lastindex+nums[lastindex],cur); if(cur&gt;=nums.size()-1) return true; if(cur==lastindex &amp;&amp; nums[lastindex] == 0) return false; lastindex++; &#125; return true; &#125;&#125;; 59. Spiral Matrix II 分析：这一题属于构造nxn的一个数组，可以按照读取的方式进行构造。 123456789101112131415161718192021222324252627282930313233343536class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; generateMatrix(int n) &#123; vector&lt;vector&lt;int&gt;&gt; matrix(n,vector&lt;int&gt;(n,0)); if(n==0) return matrix; int rowbegin = 0; int rowend = n-1; int colbegin = 0; int colend = n-1; int count = 1; while(rowbegin&lt;=rowend &amp;&amp; colbegin&lt;=colend)&#123; for(int i = colbegin;i&lt;=colend;i++)&#123; matrix[rowbegin][i] = count++; &#125; rowbegin++; for(int i = rowbegin;i&lt;=rowend;i++)&#123; matrix[i][colend] = count++; &#125; colend--; if(rowbegin&gt;rowend &amp;&amp; colbegin&gt;colend) return matrix; for(int i = colend;i&gt;=colbegin;i--)&#123; matrix[rowend][i] = count++; &#125; rowend--; if(rowbegin&gt;rowend &amp;&amp; colbegin&gt;colend)&#123; return matrix; &#125; for(int i = rowend;i&gt;=rowbegin;i--)&#123; matrix[i][colbegin] = count++; &#125; colbegin++; &#125; return matrix; &#125;&#125;; 60. Permutation Sequence 分析：递归全排列，当满足长度的个数到达k个时得到结果。 12345678910111213141516171819202122232425262728293031class Solution &#123;public: int count = 0; string res = ""; string ans; vector&lt;int&gt; visit; void dfs(int n,int k)&#123; if(res.size() == n)&#123; count++; if(count == k)&#123; ans = res; return; &#125; &#125; if(count!=k)&#123; for(int i = 1;i&lt;=n;i++)&#123; if(visit[i]==1) continue; res += to_string(i); visit[i] = 1; dfs(n,k); res = res.substr(0,res.size()-1); visit[i] = 0; &#125; &#125; &#125; string getPermutation(int n, int k) &#123; visit = vector&lt;int&gt;(n+1,0); dfs(n,k); return ans; &#125;&#125;; 61. Rotate List 分析：这题需要处理掉循环插的情况，即取模即可。然后就是正常的链表。 123456789101112131415161718192021222324252627class Solution &#123;public: ListNode* rotateRight(ListNode* head, int k) &#123; if(k == 0||head == NULL) return head; int n = 0; auto p = head; while(p!=NULL)&#123; n++; p = p-&gt;next; &#125; k = k%n; if(k == 0) return head; p = head; while(n - k -1 &gt; 0)&#123; p = p-&gt;next; k++; &#125; auto q = p-&gt;next; auto ans = q; p-&gt;next = NULL; while(q-&gt;next!=NULL)&#123; q = q-&gt;next; &#125; q-&gt;next = head; return ans; &#125;&#125;; 70. Climbing Stairs 分析：动态规划法求解。 1234567891011121314class Solution &#123;public: int climbStairs(int n) &#123; if(n == 0) return 0; if(n == 1) return 1; vector&lt;int&gt; dp(n,0); dp[0] = 1; dp[1] = 2; for(int i = 2;i&lt;n;i++)&#123; dp[i] = dp[i-1]+dp[i-2]; &#125; return dp[n-1]; &#125;&#125;; 7/3/2019 不知道为什么漏了6号，我明明都有做🐸 51. N-Queens N皇后递归最经典的问题，我觉得我在求解递归的问题的时候思路不是很清晰，总是做的不好，有必要总结一下。 递归递归就是你需要确定一个循环机制，然后每次递归需要进行标记（不标记的话每次都执行一样的东西了），当然是根据条件进行标记的。因此对于递归的条件判断也需要十分注意，每次递归结束需要释放掉当前状况所添加的约束。 定义约束变量，比如visit矩阵用于判断是否遍历过 确定主循环，主循环指需要对所有的子问题进行完整解析 将当情况的约束加到visit上，进行递归 确定递归返回条件，比如temp.size()&gt;=n 结束递归将当前约束释放掉 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869class Solution &#123;public: //回溯法 vector&lt;vector&lt;string&gt;&gt; res; vector&lt;vector&lt;int&gt;&gt; visit; void dfs(vector&lt;string&gt; temp,int pos,int n)&#123; if(temp.size() == n)&#123; res.push_back(temp); return; &#125; if(pos&gt;=n) return; for(int i = 0;i&lt;n;i++)&#123; string s(n,'.'); if(pos == 0)&#123; s[i] = 'Q'; temp.push_back(s); visit[pos][i] = 1; dfs(temp,pos+1,n); temp.pop_back(); visit[pos][i] = 0; &#125; else if((i==0||visit[pos-1][i-1]!=1)&amp;&amp; (i+1==n||visit[pos-1][i+1]!=1))&#123; int flag = 0; for(int j = 0;j&lt;n;j++)&#123; if(visit[j][i] == 1) &#123;flag = 1;break;&#125; &#125; int tempi = i-1; int tempj = pos-1; while(tempj&gt;=0&amp;&amp;tempi&gt;=0)&#123; if(visit[tempj--][tempi--] == 1)&#123;flag = 1;break;&#125; &#125; tempi = i+1; tempj = pos+1; while(tempj&lt;n&amp;&amp;tempi&lt;n)&#123; if(visit[tempj++][tempi++] == 1) &#123;flag = 1;break;&#125; &#125; tempi = i+1; tempj = pos-1; while(tempj&gt;=00&amp;&amp;tempi&lt;n)&#123; if(visit[tempj--][tempi++] == 1) &#123;flag = 1;break;&#125; &#125; tempi = i-1; tempj = pos+1; while(tempj&lt;n&amp;&amp;tempi&gt;=0)&#123; if(visit[tempj++][tempi--] == 1) &#123;flag = 1;break;&#125; &#125; if(flag == 0)&#123; s[i] = 'Q'; temp.push_back(s); visit[pos][i] = 1; dfs(temp,pos+1,n); temp.pop_back(); visit[pos][i] = 0; &#125; &#125; else&#123; continue; &#125; &#125; &#125; vector&lt;vector&lt;string&gt;&gt; solveNQueens(int n) &#123; if(n&lt;=0) return res; visit = vector(n,vector&lt;int&gt;(n,0)); vector&lt;string&gt; temp; dfs(temp,0,n); return res; &#125;&#125;; 206. Reverse Linked List递归题 1234567891011121314151617181920212223class Solution &#123;public: ListNode* reverseList(ListNode* head) &#123; if(head == NULL||head-&gt;next == NULL) return head; ListNode* p = head-&gt;next; ListNode* q = head; q-&gt;next = NULL; while(p)&#123; auto temp = p-&gt;next; p-&gt;next = q; q = p; p = temp; &#125; return q; //递归做法，先将所有的节点打散，然后从最后一个慢慢往前连接/* if(head==NULL||head-&gt;next == NULL) return head; auto last = head-&gt;next; head-&gt;next = NULL; ListNode* newhead = reverseList(last); last-&gt;next = head; return newhead;*/ &#125;&#125;; 226. Invert Binary Tree 分析：在每一次递归时进行左右交换。树的遍历方式算是递归的一种。 123456789101112class Solution &#123;public: TreeNode* invertTree(TreeNode* root) &#123; if(root == NULL) return NULL; auto p = root-&gt;left; root-&gt;left = root-&gt;right; root-&gt;right = p; invertTree(root-&gt;left); invertTree(root-&gt;right); return root; &#125;&#125;; 104. Maximum Depth of Binary Tree 分析：每一次进步一个深度，然后如果为零返回。 12345678910111213141516class Solution &#123;public: int maxn = 0; void dfs(TreeNode* root,int level)&#123; if(root == NULL) return; dfs(root-&gt;left,level+1); dfs(root-&gt;right,level+1); if(maxn&lt;level) maxn = level; &#125; int maxDepth(TreeNode* root) &#123; if(root == NULL) return 0; dfs(root,1); return maxn; &#125;&#125;; 5/3/2019 49. Group Anagrams 分析：这道题使用哈希表来解决，记录是否有相同的元素被访问过。 12345678910111213141516171819202122class Solution &#123;public: vector&lt;vector&lt;string&gt;&gt; groupAnagrams(vector&lt;string&gt;&amp; strs) &#123; vector&lt;vector&lt;string&gt;&gt; res; if(strs.size() == 0) return res; unordered_map&lt;string,int&gt; amap; for(int i = 0;i&lt;strs.size();i++)&#123; auto temp = strs[i]; sort(temp.begin(),temp.end()); if(amap.count(temp) == 0)&#123; amap[temp] = res.size(); vector&lt;string&gt; a; a.push_back(strs[i]); res.push_back(a); &#125; else&#123; res[amap[temp]].push_back(strs[i]); &#125; &#125; return res; &#125;&#125;; 82. Remove Duplicates from Sorted List II 分析：这一题的思路其实很简单，就是当你要删除一个数的时候，你应该保证目前的指针指向要删除的数的前一个,因此需要保证next和next之后的数都不为空。 123456789101112131415161718192021class Solution&#123;public: ListNode* deleteDuplicates(ListNode* head) &#123; if(head == NULL) return head; ListNode* dummy = new ListNode(-1); dummy-&gt;next = head; auto p = dummy; while(p-&gt;next &amp;&amp; p-&gt;next-&gt;next)&#123; if(p-&gt;next-&gt;val == p-&gt;next-&gt;next-&gt;val)&#123; int same = p-&gt;next-&gt;val; while(p-&gt;next&amp;&amp;p-&gt;next-&gt;val == same)&#123; p-&gt;next = p-&gt;next-&gt;next; &#125; &#125; else&#123; p = p-&gt;next; &#125; &#125; return dummy-&gt;next; &#125;&#125; 83. Remove Duplicates from Sorted List 分析：这一题比较好做，唯一要注意的是不要判断p不为空。 1234567891011121314class Solution&#123;public: ListNode* deleteDuplicates(ListNode* head) &#123; if(head == NULL) return head; auto p = head; while(p-&gt;next)&#123; if(p-&gt;next-&gt;val == p-&gt;val)&#123; p-&gt;next = p-&gt;next-&gt;next; &#125; else p = p-&gt;next; &#125; return head; &#125;&#125; 86. Partition List 分析：我发现我链表的题做得还行。这一题思路是先走到链表尾巴，然后遇到比目标大的数，就截取下来放到最后。 12345678910111213141516171819202122232425262728293031class Solution &#123;public: ListNode* partition(ListNode* head, int x) &#123; if(head == NULL) return head; ListNode* dummy = new ListNode(-1); dummy-&gt;next = head; auto p = dummy; auto q = head; int count = 0; while(p-&gt;next)&#123; p = p-&gt;next; count++; &#125; q = p; p = dummy; while(count)&#123; count--; if(p-&gt;next-&gt;val &lt; x)&#123; p = p-&gt;next; &#125; else&#123; q-&gt;next = p-&gt;next; p-&gt;next = p-&gt;next-&gt;next; q = q-&gt;next; &#125; &#125; q-&gt;next = NULL; return dummy-&gt;next; &#125;&#125;; 87. Scramble String 分析：这一题用递归的方法做，感觉所有用递归的方法其实都是最耗时的方法，更好的方法可能是动态规划方法。总之递归之后应该有一个动归才是。然后基本思路是做两次判断，第一次两个串切在同一个位置上，第二次在首尾位置上。 1234567891011121314151617181920212223242526class Solution &#123;public: bool isScramble(string s1, string s2) &#123; if(s1.size()==0||s2.size() == 0) return false; if(s1 == s2) return true; vector&lt;int&gt; letters(26); for(int i = 0;i&lt;s1.size();i++)&#123; letters[s1[i]-'a']++; letters[s2[i]-'a']--; &#125; for(int i = 0;i&lt;26;i++)&#123; if(letters[i]!=0) return false; &#125; for(int i = 1;i&lt;s1.size();i++)&#123; if(isScramble(s1.substr(0,i),s2.substr(0,i))&amp;&amp; isScramble(s1.substr(i),s2.substr(i))) return true; if(isScramble(s1.substr(0,i),s2.substr(s1.size()-i))&amp;&amp; isScramble(s1.substr(i),s2.substr(0,s1.size()-i))) return true; &#125; return false; &#125;&#125;; 4/3/2019 46. Permutations分析：这一题是典型的排列问题，用递归的方式完成，然后用一个数组来标记当前的位置是否被读取过。1234567891011121314151617181920212223242526class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; res; void dfs(vector&lt;int&gt; nums,vector&lt;int&gt; visit,vector&lt;int&gt; temp)&#123; if(temp.size() == nums.size())&#123; res.push_back(temp); return; &#125; for(int i = 0;i&lt;nums.size();i++)&#123; if(visit[i] == 0)&#123; visit[i] = 1; temp.push_back(nums[i]); dfs(nums,visit,temp); temp.pop_back(); visit[i] = 0; &#125; &#125; &#125; vector&lt;vector&lt;int&gt;&gt; permute(vector&lt;int&gt;&amp; nums) &#123; if(nums.size() == 0) return res; vector&lt;int&gt; visit(nums.size(),0); vector&lt;int&gt; temp; dfs(nums,visit,temp); return res; &#125;&#125;; 47. Permutations II分析：这一题与上一题的一个改善是，有重复的数，去重复的一个方法是对数组排序，如果当前的元素与上一个元素相同，并且上一个元素没有被访问过（意味着上一个元素曾经在这个位置上），直接跳过这个位置进入下一个位置。1234567891011121314151617181920212223242526272829class Solution &#123;public: // set&lt;vector&lt;int&gt;&gt; res; vector&lt;vector&lt;int&gt;&gt; res; void dfs(vector&lt;int&gt; nums,vector&lt;int&gt; visit,vector&lt;int&gt; temp)&#123; if(temp.size() == nums.size())&#123; res.push_back(temp); return; &#125; for(int i = 0;i&lt;nums.size();i++)&#123; if(visit[i] == 0)&#123; if(i&gt;0&amp;&amp;nums[i-1] == nums[i]&amp;&amp;visit[i-1] == 0) continue; temp.push_back(nums[i]); visit[i] = 1; dfs(nums,visit,temp); visit[i] = 0; temp.pop_back(); &#125; &#125; &#125; vector&lt;vector&lt;int&gt;&gt; permuteUnique(vector&lt;int&gt;&amp; nums) &#123; if(nums.size() == 0) return res; vector&lt;int&gt; visit(nums.size(),0); vector&lt;int&gt; temp; sort(nums.begin(),nums.end()); dfs(nums,visit,temp); return res; &#125;&#125;; 45. Jump Game II分析：这一题用动态规划或者greedy来做，具体看代码即可。dp中对i之前每个位置进行判断，时间复杂度为$O(n^2)$ , greedy中cur指当前能到最远位置，last指上一步能到最远位置。然后需要排除掉一步不走的情况。1234567891011121314151617181920212223242526272829303132class Solution &#123;public:/* int jump(vector&lt;int&gt;&amp; nums) &#123; if(nums.size() == 0) return 0; vector&lt;int&gt; dp(nums.size(),INT_MAX); dp[0] = 0; for(int i = 0;i&lt;nums.size() ;i++)&#123; for(int j = 0;j&lt;i;j++)&#123; if(nums[j]&gt;=i-j)&#123; dp[i] = min(dp[i],dp[j]+1); &#125; &#125; &#125; return dp[nums.size()-1]; &#125;*/ int jump(vector&lt;int&gt;&amp; nums) &#123; if(nums.size() == 0) return 0; int res = 0; int last = 0; int cur = 0; for(int i = 0;i&lt;nums.size()-1 ;i++)&#123; cur = max(cur,i+nums[i]); if(i == last)&#123; last = cur; res++; if(last&gt;=nums.size()-1) return res; &#125; &#125; return res; &#125;&#125;; 50. Pow(x, n)分析：由于指数乘法可以由比他小的指数乘起来得到，一次可以用分治法来做。12345678910111213141516class Solution &#123;public: double myPow(double x, int n1) &#123; if(n1 == 0) return 1.0; if(n1 == 1) return x; long long n = n1; if(n&lt;0)&#123; n = -n; x = 1.0/x; &#125; double res = myPow(x,n/2); if(n%2 == 0) return res*res; return res*res*x; &#125;&#125;; 3/3/2019 40. Combination Sum II分析：这一题用递归来求解，对于重复的问题，在执行一次递归之后，对重复的元素进行排除。12345678910111213141516171819202122232425class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; res; void dfs(vector&lt;int&gt; candidates,vector&lt;int&gt; temp,int target,int pos)&#123; if(target == 0)&#123; res.push_back(temp); return; &#125; for(int i = pos;i&lt;candidates.size()&amp;&amp;target&gt;=candidates[i];i++)&#123; temp.push_back(candidates[i]); dfs(candidates,temp,target-candidates[i],i+1); while(i+1&lt;candidates.size()&amp;&amp;candidates[i] == candidates[i+1]) i++; temp.pop_back(); &#125; &#125; vector&lt;vector&lt;int&gt;&gt; combinationSum2(vector&lt;int&gt;&amp; candidates, int target) &#123; if(candidates.size() == 0) return res; sort(candidates.begin(),candidates.end()); vector&lt;int&gt; temp; dfs(candidates,temp,target,0); return res; &#125;&#125;; 42. Trapping Rain Water分析：这一题之前做过，思路就是用两个数组，从左到右记录最大的val，从右到左记住最大的val，然后水坑的值就等于三个数组相减。12345678910111213141516171819202122class Solution &#123;public: int trap(vector&lt;int&gt;&amp; height) &#123; if(height.size() == 0) return 0; vector&lt;int&gt; left(height.size()); vector&lt;int&gt; right(height.size()); left[0] = height[0]; right[height.size()-1] = height[height.size()-1]; for(int i = 1;i&lt;height.size();i++)&#123; left[i] = max(left[i-1],height[i]); &#125; for(int i = height.size()-2;i&gt;=0;i--)&#123; right[i] = max(right[i+1],height[i]); &#125; int res = 0; for(int i = 0;i&lt;height.size();i++)&#123; int temp = min(left[i],right[i])-height[i]; if(temp&gt;0) res += temp; &#125; return res; &#125;&#125;; 44. Wildcard Matching分析：字符串匹配问题多可以用动态规划来求解，思考动态规划问题的时候不要想太多步。就想着当前这一步有多少种情况就可以了。同时需要注意边界问题。递推情况如下：当p[j] = &#39;*&#39;: s[i-1]和p[j-1]进行匹配，s[i]和p[j]进行匹配。此时考虑*表示1个字符。 s[i-1]已经和p[j]进行了匹配，s[i]也仍然和p[j]进行匹配。此时考虑*表示n个字符。 s[i]和p[j - 1]进行了匹配，此时考虑*表示0个字符。 当p[j] = &#39;?&#39;等：p[j-1]与s[i-1]进行匹配，p[j],s[i]匹配。 12345678910111213141516171819202122class Solution &#123;public: bool isMatch(string s, string p) &#123; int m = s.size(),n = p.size(); vector&lt;vector&lt;bool&gt;&gt; dp(m+1,vector&lt;bool&gt;(n+1,false)); dp[0][0] = true; for(int i = 1;i&lt;=n;i++)&#123; if(p[i-1] == '*') dp[0][i] = dp[0][i-1]; // s为空，p为连续* 号 &#125; for(int i = 1;i&lt;=m;i++)&#123; for(int j = 1;j&lt;=n;j++)&#123; if(p[j-1] == '*')&#123; dp[i][j] = dp[i-1][j]||dp[i][j-1]||dp[i-1][j-1]; &#125; else if(p[j-1] == '?'||p[j-1] == s[i-1])&#123; dp[i][j] = dp[i-1][j-1]; &#125; &#125; &#125; return dp[m][n]; &#125;&#125;; 67. Add Binary分析：做过类似的面试题，然后思路就是这样没错了。1234567891011121314151617181920212223242526272829303132333435363738class Solution &#123;public: string addBinary(string a, string b) &#123; int n = a.size()-1; int m = b.size()-1; int add = 0; string res = ""; while(n&gt;=0&amp;&amp;m&gt;=0)&#123; int le = a[n] - '0'; int ri = b[m] - '0'; if(le+ri+add&gt;=2)&#123; res = to_string(le+ri+add -2) + res; add = 1; &#125; else&#123; res = to_string(le+ri+add) + res; add = 0; &#125; n--; m--; &#125; res = (n&gt;=0? a.substr(0,n+1):b.substr(0,m+1)) + res; if(add == 0) return res; int left = n&gt;=0? n:m; while(left&gt;=0)&#123; if(res[left] == '0')&#123; res[left] = '1'; return res; &#125; else&#123; res[left] = '0'; &#125; left--; &#125; res = '1'+ res; return res; &#125;&#125;; 2/3/2019 32. Longest Valid Parentheses分析：这一题括号匹配，用栈的结构来解决，每次将括号的下标存入栈的结构中。1234567891011121314151617181920212223class Solution &#123;public: int longestValidParentheses(string s) &#123; if(s.size() == 0) return 0; int maxn = 0; stack&lt;int&gt; sta; sta.push(-1); for(int i = 0;i&lt;s.size();i++)&#123; if(s[i] == '(')&#123; sta.push(i); &#125; else&#123; sta.pop(); if(!sta.empty()) maxn = max(maxn,i-sta.top()); else&#123; sta.push(i); &#125; &#125; &#125; return maxn; &#125;&#125;; 34. Find First and Last Position of Element in Sorted Array这一题题目要求复杂度是O(log(n)) 很显然就是用二分法来做的，然后如果找到了target，就往target的两边去找相同的元素。12345678910111213141516171819202122232425262728293031class Solution &#123;public: vector&lt;int&gt; searchRange(vector&lt;int&gt;&amp; nums, int target) &#123; vector&lt;int&gt; res = &#123;-1,-1&#125;; if(nums.size() == 0) return res; int low = 0; int high = nums.size()-1; int mid; while(low&lt;=high)&#123; mid = (low+high)/2; if(nums[mid] == target) break; else if(nums[mid]&gt;target)&#123; high = mid-1; &#125; else&#123; low = mid+1; &#125; &#125; if(low&gt;high) return res; int i = 0; for(i = mid-1;i&gt;=0;i--)&#123; if(nums[i] != target) &#123;break;&#125; &#125; res[0] = i+1; for(i = mid + 1;i&lt;nums.size();i++)&#123; if(nums[i]!=target) &#123; break;&#125; &#125; res[1] = i-1; return res; &#125;&#125;; 36. Valid Sudoku分析：判断横排，竖排，里头九宫格即可。123456789101112131415161718192021222324252627282930313233343536class Solution &#123;public: bool isValidSudoku(vector&lt;vector&lt;char&gt;&gt;&amp; board) &#123; if(board.size() == 0) return false; //横排 for(int i = 0;i&lt;9;i++)&#123; unordered_map&lt;char,int&gt; amap; for(int j = 0;j&lt;9;j++)&#123; if(amap.count(board[i][j]) != 0 &amp;&amp;board[i][j]!='.') return false; amap[board[i][j]] = 1; &#125; &#125; //竖排 for(int i = 0;i&lt;9;i++)&#123; unordered_map&lt;char,int&gt; amap; for(int j = 0;j&lt;9;j++)&#123; if(amap.count(board[j][i])!=0&amp;&amp;board[j][i]!='.') return false; amap[board[j][i]] = 1; &#125; &#125; //九宫格 for(int i = 0;i&lt;9;i += 3)&#123; for(int j = 0;j&lt;9;j+=3)&#123; unordered_map&lt;char,int&gt; amap; for(int h = i;h&lt;i+3;h++)&#123; for(int k = j;k&lt;j+3;k++)&#123; if(amap.count(board[h][k])!=0&amp;&amp;board[h][k]!='.') return false; amap[board[h][k]] = 1; &#125; &#125; &#125; &#125; return true; &#125;&#125;; 39. Combination Sum分析：经典的一道递归题，下次一定要会做才行，因为最基本的递归就长这个样子。123456789101112131415161718192021class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; res; void digui(vector&lt;int&gt;&amp; candidates,int target,vector&lt;int&gt; temp,int pos)&#123; if(target == 0) res.push_back(temp); for(int i = pos;i&lt;candidates.size();i++)&#123; if(target&gt;=candidates[i])&#123; temp.push_back(candidates[i]); digui(candidates,target-candidates[i],temp,i); temp.pop_back(); &#125; &#125; &#125; vector&lt;vector&lt;int&gt;&gt; combinationSum(vector&lt;int&gt;&amp; candidates, int target) &#123; if(candidates.size() == 0) return res; vector&lt;int&gt; temp; digui(candidates,target,temp,0); return res; &#125;&#125;; 1/3/2019 38. Count and Say分析：这一题是递归的题，出口是n = 0 或 1，然后用for循环判断当前生成的字符。123456789101112131415161718class Solution &#123;public: string countAndSay(int n) &#123; if(n &lt;= 0) return ""; if(n == 1) return "1"; string s = countAndSay(n-1); string newS = ""; for(int i = 0;i&lt;s.size();i++)&#123; int count = 1; while(i+1&lt;s.size()&amp;&amp;s[i] == s[i+1])&#123; count++; i++; &#125; newS += to_string(count) + s[i]; &#125; return newS; &#125;&#125;; 2/28/2019 30. Substring with Concatenation of All Words分析：控制一个words的所有字符长度的子串，然后在子串里面看是否满足条件。用hash_map做。 1234567891011121314151617181920212223242526class Solution &#123;public: vector&lt;int&gt; findSubstring(string s, vector&lt;string&gt;&amp; words) &#123; vector&lt;int&gt; res; if(s.empty()||words.size() == 0) return res; int m = words[0].size(); int n = words.size(); unordered_map&lt;string,int&gt; m1; for(int i = 0;i&lt;words.size();i++)&#123; ++m1[words[i]]; &#125; for(int i = 0;i&lt;=(int)s.size()-m*n;i++)&#123; cout&lt;&lt;s.size(); unordered_map&lt;string,int&gt; m2; int j = 0; for(;j&lt;words.size();j++)&#123; string t = s.substr(i+j*m,m); if(m1.find(t) == m1.end()) break; ++m2[t]; if(m2[t]&gt;m1[t]) break; &#125; if(j == words.size()) res.push_back(i); &#125; return res; &#125;&#125;; 2/26/2019 53. Maximum Subarray分析：这一题时简单的DP问题，用一个数存之前的序列和，当和小于0时则清零。1234567891011121314class Solution &#123;public: int maxSubArray(vector&lt;int&gt;&amp; nums) &#123; if(nums.size() == 0) return 0; int maxn = INT_MIN; int ans = 0; for(int i = 0;i&lt;nums.size();i++)&#123; if(ans&lt;0) ans = 0; ans += nums[i]; maxn = max(maxn,ans); &#125; return maxn; &#125;&#125;; 15. 3Sum 分析：这一题要找出所有的相加为0的组合，可以定义三个变量，用来控制数组中相加的数字，一个数字控制外循环，里头两个数字当遇到与前一个相同时，需要跳过。 123456789101112131415161718192021222324252627class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; threeSum(vector&lt;int&gt;&amp; nums) &#123; sort(nums.begin(),nums.end()); vector&lt;vector&lt;int&gt;&gt; res; if(nums.size() == 0) return res; for(int i = 0;i&lt;nums.size();i++)&#123; int begin = i+1,end = nums.size()-1; if(i&gt;0&amp;&amp;nums[i-1]==nums[i]) continue; while(begin&lt;end)&#123; int result = nums[i]+nums[end]+nums[begin]; if(i!=end&amp;&amp; result == 0)&#123; vector&lt;int&gt; temp = &#123;nums[i],nums[begin],nums[end]&#125;; res.push_back(temp); end--; while(end&gt;=0&amp;&amp;nums[end+1] == nums[end]) end--; begin++; while(begin&lt;nums.size()&amp;&amp;nums[begin-1] == nums[begin]) begin++; &#125; else if(i==end||result&gt;0) end--; else begin++; &#125; &#125; return res; &#125;&#125;; 16. 3Sum Closest这一题是上一题的变形，省去了判断过滤重复的步骤，只要求一个绝对值最接近1就好。1234567891011121314151617181920212223242526class Solution &#123;public: int threeSumClosest(vector&lt;int&gt;&amp; nums, int target) &#123; if(nums.size() == 0) return 0; int sum = INT_MAX; int ans; sort(nums.begin(),nums.end()); for(int i = 0;i&lt;nums.size();i++)&#123; int j = i+1,k = nums.size()-1; while(j&lt;k)&#123; int result = nums[i]+nums[j]+nums[k]; if(i!=k&amp;&amp;abs(result-target)&lt;=sum)&#123; sum = abs(result-target); ans = result; if(result&lt;target)j++; else if(result&gt;target) k--; else return result; &#125; else if(result&gt;target) k--; else j++; &#125; &#125; return ans; &#125;&#125;; 17. Letter Combinations of a Phone Number这一题比较简单，把存结果的数组当作栈来用就行了。12345678910111213141516171819202122232425262728293031class Solution &#123;public: vector&lt;string&gt; letterCombinations(string digits) &#123; vector&lt;string&gt; res; if(digits.size() == 0) return res; vector&lt;vector&lt;char&gt;&gt; alphabet = &#123; &#123;&#125;, &#123;&#125;,&#123;'a','b','c'&#125;,&#123;'d','e','f'&#125;,&#123;'g','h','i'&#125;,&#123;'j','k','l'&#125;,&#123;'m','n','o'&#125;, &#123;'p','q','r','s'&#125;,&#123;'t','u','v'&#125;,&#123;'w','x','y','z'&#125; &#125;; vector&lt;char&gt; tem(alphabet[digits[0]-'0']); string a =""; for(int i = 0;i&lt;tem.size();i++)&#123; a += tem[i]; res.push_back(a); a = ""; &#125; for(int i = 1;i&lt;digits.size();i++)&#123; vector&lt;char&gt; te(alphabet[digits[i]-'0']); int resSize = res.size(); for(int j = 0;j&lt;resSize;j++)&#123; string ahead = res[0]; for(int k = 0;k&lt;te.size();k++)&#123; res.push_back(ahead+te[k]); &#125; res.erase(res.begin()); &#125; &#125; return res; &#125;&#125;; 18. 4Sum分析：这一题是前面三个数的加强版，注意一些重复的判断就行了。12345678910111213141516171819202122232425262728class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; fourSum(vector&lt;int&gt;&amp; nums, int target) &#123; vector&lt;vector&lt;int&gt;&gt; res; if(nums.size() &lt; 4) return res; sort(nums.begin(),nums.end()); for(int i = 0;i&lt;nums.size();i++)&#123; if(i&gt;0&amp;&amp;nums[i] == nums[i-1]) continue; for(int j = i+1;j&lt;nums.size();j++)&#123; if(j&gt;i+1&amp;&amp;nums[j] == nums[j-1]) continue; int begin = j+1,end = nums.size()-1; while(begin&lt;end)&#123; int result = nums[i]+nums[j]+nums[begin]+nums[end]; if(result == target)&#123; res.push_back(&#123;nums[i],nums[j],nums[begin],nums[end]&#125;); begin++; end--; while(end&gt;begin&amp;&amp;nums[end] == nums[end+1]) end--; while(begin&lt;end&amp;&amp;nums[begin-1] == nums[begin]) begin++; &#125; else if(result &gt; target) end--; else begin++; &#125; &#125; &#125; return res; &#125;&#125;; 19. Remove Nth Node From End of List分析：这一题要求执行一趟，删除掉倒数第n个节点。可以用两个指针来完成，第一个指针领先第二个指针n的位置，当第一个指针到达终点时，第二个指针的位置就是倒数n的位置。然后需要注意删除第一个元素的情况。12345678910111213141516171819202122232425262728/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* removeNthFromEnd(ListNode* head, int n) &#123; if(head == NULL) return NULL; ListNode* pre = head; ListNode* last = head; ListNode* pos = head; while(n--)&#123; pos = pos-&gt;next; &#125; if(pos == NULL) return head-&gt;next; //当删除第一个元素时 while(pos!=NULL)&#123; pre = last; last = last-&gt;next; pos = pos-&gt;next; &#125; pre-&gt;next = last-&gt;next; return head; &#125;&#125;; 22. Generate Parentheses分析：这是一道很经典的递归的题目，我做出一道就有感觉了。就是说看递归一定是这一步做了某种选择，待会还要回来。而且要比较注重递归程序的出口。1234567891011121314151617class Solution &#123;public: int nn; vector&lt;string&gt; ans; void digui(string res,int left,int right)&#123; if(res.size() == 2*nn)&#123; ans.push_back(res);return;&#125; if(left&lt;nn) digui(res+"(",left+1,right); if(right&lt;nn&amp;&amp;right&lt;left) digui(res+")",left,right+1); &#125; vector&lt;string&gt; generateParenthesis(int n) &#123; if(n == 0) return ans; nn = n; digui("",0,0); return ans; &#125;&#125;; 24. Swap Nodes in Pairs调换两个数，需要三个指针，然后注意特殊情况只有一个数的时候的。直接放回head。123456789101112131415161718192021222324252627282930/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* swapPairs(ListNode* head) &#123; if(head == NULL) return NULL; ListNode* pre = new ListNode(-1); ListNode* first = head; ListNode* second = head-&gt;next; pre-&gt;next = head; if(second == NULL) return head; head = second; while(second != NULL)&#123; pre-&gt;next = second; first-&gt;next = second-&gt;next; second-&gt;next = first; pre = first; first = first-&gt;next; if(first!=NULL) second = first-&gt;next; else return head; &#125; return head; &#125;&#125;; 29. Divide Two Integers分析：这一题由于有越界问题，可以用long long申请变量，保证不会溢出。12345678910111213141516171819class Solution &#123;public: int divide(int dividend, int divisors) &#123; long long divide = dividend; long long divisor = divisors; if(divisor == 0) return divide; int sign = 1; if(divisor&lt;0) sign = -1,divisor *= -1; if(divide&lt;0) sign *= -1,divide *= -1; long time = 0; while(divide&gt;=divisor)&#123; time++; divide -= divisor; &#125; if(time*sign&gt;INT_MAX) return INT_MAX; if(time*sign&lt;INT_MIN) return INT_MIN; return time*sign; &#125;&#125;; 4. median of two sorted array分析：这一题要找两个排序好的数组的中位数。 中位数有一个性质就是一定位于数列的中间位置，而且中位数左边的数都小于中位数，中位数右边的数都大于中位数 因此我们对数组位置进行分析时，需要保持中位数位置一定为数组长度的一半，又因为这道题对两个排序好的数组寻找中位数，因此可以分别对他们使用分治法求解。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class Solution &#123;public: double findMedianSortedArrays(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123; //首先拿到数组的长度，并设置nums1的长度大于nums2 int m = nums1.size(); int n = nums2.size(); if(m&gt;n)&#123; auto temp = nums1; nums1 = nums2; nums2 = temp; swap(n,m); &#125;// n &gt; m int imin = 0,imax = m,half = (m+n+1)/2; //half保证了长度为数列的一半 //接下来在nums2数组中对中位数位置进行遍历 while(imin&lt;=imax)&#123; int i = (imax-imin)/2 + imin; // seperate nums1 int j = half - i; // j为num2的分割点，可以看出来j为一半的长度，不是下标 if(i&lt;m &amp;&amp; nums1[i]&lt;nums2[j-1]) // i is too small &#123; imin = i+1; &#125; else if(i&gt;0&amp;&amp;nums1[i-1]&gt;nums2[j])&#123; // i is to big imax = i-1; &#125; else&#123; // ferfect int max_left,min_right; if(i == 0)&#123; max_left = nums2[j-1]; &#125; else if(j == 0)&#123; max_left = nums1[i-1]; &#125; else&#123; max_left = max(nums2[j-1],nums1[i-1]); &#125; if((m+n)%2 == 1)&#123; return max_left; &#125; else&#123; if(i == m)&#123; min_right = nums2[j]; &#125; else if(j == n)&#123; min_right = nums1[i]; &#125; else&#123; min_right = min(nums1[i],nums2[j]); &#125; return (min_right+max_left)/2.0; &#125; &#125; &#125; return -1.0; &#125;&#125;; 26. Remove Duplicates from Sorted Array 分析：这一题思路比较简单，由于数组是排序过的，因此重复的数在相邻的位置上。所以做法就是用i遍历一边数组，用j保持数组不重复的长度，当出现不重复时j++，将不重复的数补充到j位置上。123456789101112131415161718class Solution &#123;public: int removeDuplicates(vector&lt;int&gt;&amp; nums) &#123; if(nums.size() == 0) return 0; int count = 0; int j = 0; for(int i = 1;i&lt;nums.size();i++)&#123; while(i&lt;nums.size()&amp;&amp;nums[j] == nums[i])&#123; i++; &#125; if(i&lt;nums.size()) &#123; j++;nums[j] = nums[i]; &#125; &#125; return j+1; &#125;&#125;; 837. New 21 Game 分析：先吐槽一下自己，最近刷题有点儿太慢了。这一题的题意是说，Alice每次都可以在1～W之间随机选择一个数，当Alice选择的数累加起来大于等于K的时候，Alice停止游戏。这时候这个累加和如果大于N那么Alice就输了，小于等于N Alice就赢了。题目叫我们算Alice赢得概率，就是累加和小于等于N的概率。 这一题可以用DP来求解,维护一个累加和窗。设dp[i]为当前累加和为i的时候的概率。要求i的概率有下面关系：dp[i] = 1/w * (dp[i-1]+dp[i-2]...dp[i-w])，即我可以先选择i-1，然后选1。由于可以选择的数只有W个，因此窗口宽度为W。对于累加和有下面的关系： i&lt;K : Wsum += dp[i] 表明当前的i可以作为下一次两步选择的第一步 i-W&gt;=0: Wsum -= dp[i-W] 表明对于下一个i来说，因为W的范围限定，取不到第dp[i-W]作为前两步选择的第一步，需要把概率减去，维护窗内概率。 N&gt;= i &gt;=K: res += dp[i]；结果为res,即这个时候分数在K与N之间。 1234567891011121314151617181920class Solution &#123;public: double new21Game(int N, int K, int W) &#123; if(K == 0) return 1; // 共有N+1个状态 vector&lt;double&gt; dp(N+1); double Wsum = 1.0; // 记录前W个数的概率 double res = 0.0; dp[0] =1; for(int i = 1;i&lt;=N;i++)&#123; dp[i] = Wsum/W; if(i&lt;K) Wsum+=dp[i]; // 当前的i可以作为下一次两步选择的第一步 else&#123; res += dp[i]; &#125; if(i-W&gt;=0) Wsum -= dp[i-W]; //对于下一个i来说，当前的i-W下一个无法取到 &#125; return res; &#125;&#125;; 481. Magical String 这一题的题意是说，1和2将会交替出现，最开始1先出现，然后去产生下面的数，最后会发现产生的数组和每一行数字的个数序列将会是同一个序列。最后统计一下序列中1的个数。数字的产生规则如下： 先产生1 1与2交替出现 当前字符串最末尾的数字控制添加入字符串的字符个数，如122，表示下一次将加入2个1，变成12211 前三个数比较特殊，直接生成122 123456789101112class Solution &#123;public: int magicalString(int n) &#123; if(n == 0) return 0; string s = "122"; int i = 2; while(s.size()&lt;n)&#123; s+= string(s[i++]-'0',s.back() == '1'? '2':'1'); &#125; return count(s.begin(),s.begin()+n,'1'); &#125;&#125;; 有几个新函数记录一下：12s = string(char_num,char); //产生char_num个charcount(s.begin(),s.begin()+n,&apos;1&apos;);//计算字符串s中含&apos;1&apos;的个数 2. Add Two Numbers这一题题意说的是用链表表示数字，表头为个位。然后将两个链表相加，计算他们的和，返回一个新的链表。这一题比较简单，要注意的有种情况： 链表相加完，有一个链表长度还有剩余 链表要记录进位，最后可能进位项还为1 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* addTwoNumbers(ListNode* l1, ListNode* l2) &#123; ListNode* head = new ListNode(-1); auto p = head; int step = 0; while(l1!=NULL&amp;&amp;l2!=NULL)&#123; int sum = l1-&gt;val + l2-&gt;val+step; if(sum&gt;=10)&#123; sum -= 10; step = 1; &#125; else&#123; step = 0; &#125; p-&gt;next = new ListNode(sum); p = p-&gt;next; l1 = l1-&gt;next; l2 = l2-&gt;next; &#125; auto l = l1 != NULL ? l1 : l2; if(l!=NULL)&#123; while(l!=NULL)&#123; int sum = l-&gt;val +step; if(sum&gt;=10)&#123; sum -= 10; step = 1; &#125; else&#123; step = 0; &#125; p-&gt;next = new ListNode(sum); p = p-&gt;next; l = l-&gt;next; &#125; &#125; if(step==1)&#123; p-&gt;next = new ListNode(1); &#125; return head-&gt;next; &#125;&#125;; 数据大小及其表示的问题 整数int的上下界： 12最小的表示方式：-1&lt;&lt;31，INT_MIN最大的表示方式：1&lt;&lt;31 -1,INT_MAX 其他类型： 123unsigned int -&gt;UINT_MAXlong-&gt;LONG_MAXunsigned long-&gt;ULONG_MAX 无穷大的选择： 1const int INF = 0x7fffffff; 0x7fffffff 是32-bit int的最大值。 1const int INF = 0x3f3f3f3f 0x3f3f3f3f的十进制是1061109567，是10^9级别的（和一个数量级），而一般场合下的数据都是小于10^9的，可以用来表示无穷大。此外，0x3f3f3f3f * 2 =2122219134，这非常大但却没有超过32-bit int的表示范围，所以0x3f3f3f3f能够满足“无穷大加无穷大还是无穷大”的需求。如果我们想要将某个数组清零，我们通常会使用memset(a,0,sizeof(a))。但是当我们想将某个数组全部赋值为无穷大时，就不能使用memset函数而得自己写循环了，因为memset是按字节操作的。如果我们将无穷大设为0x3f3f3f3f，0x3f3f3f3f的每个字节都是0x3f！所以要把一段内存全部置为无穷大，我们只需要memset(a,0x3f,sizeof(a))。 123#include&lt;cstring&gt;memset(a,0,sizeof(a)); //给a数组置0memset(a，0x3f,sizeof(a));//给a数组赋值正无穷 表示一个很小的数：1const long double eps = 1e-8; 1e-8 是0.00000001，用来表示一个很小很小的数，通常可以用来判断两个数是否相同，即精度的差距。 2/22/1019 3. Longest Substring Without Repeating Characters分析：这一题题目非常好理解，找到字符串中的最长非重复子串。一看这一题的题目就感觉会有大量的元素比较，重复计算，因此可以用DP来做，用一个数组存储子问题的解。 维护一个数组res[j]，用来存储子问题的解，遍历原始数组，如果发现循环到的元素s[i]与res中最后一个位置所代表的元素不同，这res[j]++;如果发现相同这j++;res[j] = res[j-1]-1。具体写代码的时候里面有很多陷阱，看代码注释：12345678910111213141516171819202122232425262728class Solution &#123;public: int lengthOfLongestSubstring(string s) &#123; if(s.size() == 0) return 0; vector&lt;int&gt; res(s.size()); int j = 0; res[0] = 1; int flag = 0; for(int i = 1;i&lt;s.size();i++)&#123; for(int k = j;k&lt;i;k++)&#123; //判断当前循环元素与子串中是否有重复 if(s[k]==s[i])&#123; flag = 1; &#125; &#125; if(j&gt;=i) continue; //由于底下有i--的操作，需要保证j&lt;i if(flag == 0)&#123; // all different res[j]++; &#125; else&#123; //如果有重复 flag = 0; j++; //res表示的子串向前缩减 i--; //当前遍历到的元素需要保留 res[j] = res[j-1]-1&gt;0? res[j-1]-1 : 1; //保证res[j]最小为1 &#125; &#125; return *max_element(res.begin(),res.end()); &#125;&#125;; tip：关于vector找最大值最小值：12int maxValue = *max_element(s.begin(),s.end());int minValue = *min_element(s.begin(),s.end()); 5. Longest Palindromic Substring 分析：找到最长的回文子串，可以用一个窗口去扫描，窗口的长度有2到字符串长度。该做法的时间复杂度为$O(n^2)$。 1234567891011121314151617181920212223242526272829class Solution &#123;public: string longestPalindrome(string s) &#123; if(s.size() == 0) return ""; int pos = 0; int length = 1; for(int l = 2;l&lt;=s.size();l++)&#123; // 回文的长度 cout&lt;&lt;l&lt;&lt;" "; for(int i = 0;i&lt;s.size()-l+1;i++)&#123; int j = i+l-1; int temp = i; while(temp&lt;j)&#123; // 判断窗口内是否满足回文 if(s[temp]==s[j])&#123; temp++; j--; &#125; else&#123; break; &#125; &#125; if(temp&gt;=j)&#123; //说明满足回文 pos = i; length = l; &#125; &#125; &#125; return s.substr(pos,length); &#125;&#125;; 2/23/2019 6. ZigZag Conversion分析：这一题题意要求生成zigZag字形的序列，如图。可以用下标间关系求解，规定i为行数，j为要输出位置的下标，则该序列中下标间存在以下关系： V口向上： j += 2*（numRows-i-1） V口向下：j +=2i 第一行和最后一行处于V的交界位置，需要排除掉一种即可。 1234567891011121314151617181920class Solution &#123;public: string convert(string s, int numRows) &#123; if(s.size() == 0) return ""; if(numRows == 1) return s; string res = ""; for(int i = 0;i&lt;numRows;i++)&#123; int j = i; while(j&lt;s.size())&#123; res += s[j]; j += 2*(numRows-i-1); if(j&lt;s.size()&amp;&amp;i!=0&amp;&amp;i!=numRows-1)&#123; res += s[j]; &#125; j += 2*i; &#125; &#125; return res; &#125;&#125;; 8. String to Integer (atoi)分析：这一题做的我很狼狈，可以按从头到尾扫描的方式来做，我的做法太蠢了。特例很多。 从头到位扫描。 当判断一个string 转成int是否超过精度的时候，可以申请一个long long类型的变量，判断他是否大于边界值。 char 转int的方式： str[i] - &#39;0&#39;;即可。 我的做法：（不推荐，虽然挺快的）1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123;public: int myAtoi(string str) &#123; if(str.size() == 0) return 0; string s = ""; for(int i = 0;i&lt;str.size();i++)&#123; if(s ==""&amp;&amp;str[i] == ' ') continue; if(s=="")&#123; if(isdigit(str[i])) s = str[i]; else if(str[i] =='-') s += '-'; else if(str[i] == '+') s += '+'; else return 0; &#125; else&#123; if(!isdigit(str[i])) break; s+=str[i]; &#125; &#125; if(s.size() == 0) return 0; if(s.size() == 1 &amp;&amp; s[0] == '-') return 0; int flag = 1; if((s[0]=='+'||s[0]=='-')&amp;&amp;!isdigit(s[1])) return 0; if(s[0] == '-')&#123; flag = -1; s = s.substr(1,s.size()-1); &#125; else if(s[0] == '+') &#123; s = s.substr(1,s.size()-1); &#125; if(s.size()&gt;1)&#123; while(s.size()&gt;1&amp;&amp;s[0]=='0')&#123; s = s.substr(1,s.size()-1); &#125; &#125; string min = "2147483648"; if(s.size()&gt;10) return flag == 1? INT_MAX:INT_MIN; if(s.size()&gt;=min.size()&amp;&amp;s&gt;=min) return flag == 1? INT_MAX:INT_MIN; else return stoi(s)*flag; &#125;&#125;; 比较合理的做法：12345678910111213int myAtoi(string str) &#123; if(str.size() == 0) return 0; long long base = 0; int sign = 1,i = 0; while(str[i] == ' ') i++; if(str[i] == '+') i++; else if(str[i] == '-') sign = -1,i++; while(i&lt;str.size()&amp;&amp;str[i]&gt;='0'&amp;&amp;str[i]&lt;='9')&#123; base = base*10 + str[i++]-'0'; if(base&gt;INT_MAX) return sign == 1?INT_MAX:INT_MIN; &#125; return base*sign;&#125; 35. Search Insert Position分析： 这一题可以用分治法来做，主要的点在于当要找的数不存在时，它如果比num[high]大，那么插入点为high（需要保证high&gt;=0），如果比high小，插入点为high+112345678910111213141516class Solution &#123;public: int searchInsert(vector&lt;int&gt;&amp; nums, int target) &#123; if(nums.size() == 0) return 0; int low = 0; int high = nums.size()-1; while(low&lt;=high)&#123; int mid = (high+low)/2; if(nums[mid] == target) return mid; else if(nums[mid]&gt;target) high = mid -1; else low = mid + 1; &#125; if(high&gt;=0&amp;&amp;nums[high]&gt;target) return high; else return high+1; &#125;&#125;; 24/2/2019 10. Regular Expression Matching这道题的题意是判断两个字符串是否能够匹配，由于*号可以替换多个字符，因此这一题有一个递归的过程，也就是说，替换的个数可能是1，2…等等。所以要用递归的方法求解。 当p[1] == &#39;*&#39;: 两种情况，*直接跳过；match一个字符； 当p[1]!=*: 则两个字符对应位置match 递归解法：12345678910111213class Solution &#123;public: bool isMatch(string s, string p) &#123; if(p.empty()) return s.empty(); if(p[1]=='*')&#123; return isMatch(s,p.substr(2))||(!s.empty()&amp;&amp;(s[0] == p[0]||p[0]=='.')&amp;&amp;isMatch(s.substr(1),p)); &#125; else return !s.empty()&amp;&amp;(s[0]==p[0]||p[0]=='.')&amp;&amp;isMatch(s.substr(1),p.substr(1)); &#125;&#125;; 动态规划法利用dp数组把所有的子情况都进行保存。有以下几种情形： dp[i][j]: s(0,i) ,p(0,j)是否match 当 p[j-1] != *: dp[i][j] == d[i-1][j-1]&amp;&amp;s[i-1] == p[j-1] 当p[j-1] == *： 两种：有替换或无替换：123dp[i][j] = dp[i-1][j] //in this case, a* counts as multiple a dp[i][j] = dp[i][j-1] // in this case, a* counts as single a dp[i][j] = dp[i][j-2] // in this case, a* counts as empty 动态规划：123456789101112131415161718192021class Solution &#123;public: bool isMatch(string s, string p) &#123; if(p.empty()) return s.empty(); int len1=s.size(),len2=p.size(); vector&lt;vector&lt;bool&gt;&gt; dp(len1+1,vector&lt;bool&gt;(len2+1,false)); dp[0][0]=true; for(int i=0;i&lt;=len1;i++) &#123; for(int j=1;j&lt;=len2;j++) &#123; if(p[j-1]=='*') dp[i][j] = dp[i][j-2] || ( i&gt;0 &amp;&amp; dp[i-1][j] &amp;&amp; (s[i-1]==p[j-2] || p[j-2]=='.') ); else dp[i][j] = i&gt;0 &amp;&amp; dp[i-1][j-1] &amp;&amp; (s[i-1]==p[j-1] || p[j-1]=='.'); &#125; &#125; return dp[len1][len2]; &#125; &#125;; 12. Integer to Roman 分析：这一题题意要求将普通数字表示称罗马数字，注意一一对应的关系即可。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class Solution &#123;public: string intToRoman(int num) &#123; string res = ""; while(num&gt;=1000)&#123; res += "M"; num -= 1000; &#125; if(num&gt;=900)&#123; res+= "CM"; num -= 900; &#125; else if(num&gt;=100)&#123; if(num&gt;=500) res+='D',num -= 500; else if(num&gt;=400) res += "CD",num -= 400; while(num&gt;=100)&#123; res +='C'; num-=100; &#125; &#125; if(num&gt;=90)&#123; res += "XC"; num -= 90; &#125; else if(num&gt;=10)&#123; if(num&gt;=50) res += 'L',num -= 50; else if(num&gt;=40)res+="XL",num -= 40; while(num&gt;=10)&#123; res +='X'; num -= 10; &#125; &#125; if(num&gt;=9)&#123; res += "IX"; num -= 9; &#125; else if(num&gt;=1)&#123; if(num&gt;=5) res += "V",num -= 5; else if(num&gt;=4)res +="IV",num-=4; while(num&gt;=1)&#123; res +='I'; num -= 1; &#125; &#125; return res; &#125;&#125;;]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络-- CNN]]></title>
    <url>%2F2019%2F02%2F19%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-CNN%2F</url>
    <content type="text"><![CDATA[卷积神经网络 – CNNCNN最早由LeCun 在1998年《Gradient-based learning applied to document recognition》中提出，并提出了一个目标检测的模型：LeNet-5，随后在2012年ImageNet竞赛上，基于CNN网络的AlexNet取得了第一，且正确率超出第二近10%，取得了历史性的突破。CNN开始大放异彩，VGG Net，Google Net，ResNet等，都是基于CNN网络的一些杰出的工作。 CNN基本模块CNN由输入和输出层以及多个隐藏层组成，隐藏层可分为卷积层，池化层、RELU层和全连通层，如下图：输入层CNN的输入为原始图像，三维（RGB）或二维的向量。卷积层卷积层是CNN的核心，卷积层由一组可学习的滤波器（filter）或内核（kernels）组成，它们具有小的感受野，每个卷积核具有kernel size，padding，stride等参数。从图像的左上角依次做内积操作，提取出图片的高层次特征。pooling layer池化层对conv后输出的feature map进行下采样操作，这样的好处有降低参数的数量，防止过拟合等作用。relu激活函数在CNN中使用relu激活函数，在网络中引入了非线性。通过relu激活函数传递卷积运算的结果。因此，最终特征映射中的值不是简单的线性关系。全连接层全连接层的输入是一维向量，需要将pooling 层的输出向量flatten成一个一维的向量，然后输入到全连接层中，最后送到soft Max层进行类别的分类。 值得注意的是：在很多CNN网络结构中，pooling层的kernel = 2x2, stride = 2 ， padding = 0,经过这样的pooling后，输出图片缩小一半。 卷积层的kernel = 3x3, stride = 1， padding = 1。经过这样的卷积，输出大小与输入相同。 CNN的特点局部感知局部感知即卷积核的感受野，指的是卷积核所覆盖的像素面积，由于每个卷积核所覆盖的面积仅是很少的一部分，是局部特征，即为局部感知。CNN是一个从局部到整体的过程（局部到整体的实现是在全连通层）。下图是全连接层和卷积层的对比。 权重共享传统的神经网络的参数量巨大，例如对1000X1000像素的图片做一次全连接操作，需要（1000X1000）10的6次方个参数。而CNN除全连接层外，卷积层的参数完全取决于滤波器的设置大小，比如10x10的滤波器，仅有100个参数。整个图片共享一组滤波器的参数，参数数量少，计算简单。多卷积核一种卷积核代表的是一种特征，为获得更多不同的特征集合，允许有多个卷积核，卷积生成的feature map有几个channel就有几个卷积核。 dropout技术dropout是一种防止过拟合的正则化技术，具体做法是，对每个隐藏层的输入进行一个概率判决，比如我们设置概率为0.5（通常命名为keep_prob）,根据0.5，随机生成一个跟隐藏层神经元个数相同的向量，true:false的比例是1：1（因为keep_prob=0.5），与隐藏层的神经元进行相乘，那么会有一半隐藏层的神经元被舍弃，不参与训练。重复迭代上诉操作。dropout的实现：12345678910111213141516171819#用一个二项分布的函数，等概率的生成0/1，既可以随机的屏蔽掉某些神经元#dropout函数的实现def dropout(x, level): if level &lt; 0. or level &gt;= 1:#level是概率值，必须在0~1之间 raise Exception('Dropout level must be in interval [0, 1[.') retain_prob = 1. - level #我们通过binomial函数，生成与x一样的维数向量。binomial函数就像抛硬币一样，我们可以把每个神经元当做抛硬币一样 #硬币 正面的概率为p，n表示每个神经元试验的次数 #因为我们每个神经元只需要抛一次就可以了所以n=1，size参数是我们有多少个硬币。 sample=np.random.binomial(n=1,p=retain_prob,size=x.shape)#即将生成一个0、1分布的向量，0表示这个神经元被屏蔽，不工作了，也就是dropout了 print sample x *=sample#0、1与x相乘，我们就可以屏蔽某些神经元，让它们的值变为0 print x x /= retain_prob # 归一化 return x#对dropout的测试x=np.asarray([1,2,3,4,5,6,7,8,9,10],dtype=np.float32)dropout(x,0.4) dropout通常使用在一些较大的网络中，在训练阶段使用，在测试或者预测时并不会去dropout，工业上的做法是在输入的X上乘以P得到X的期望，或者输入不做变化而是对所有的有dropout层都做X/p。dropout能防止过拟合： 多样化学习：由于每次dropout舍弃的神经元均不相同，因此每次训练都产生一个不同的神经网络。这种组合多种神经网络的综合效果的方式能够有效的缓解过拟合效应。 阻止特征的协同作用：可以通过阻止某些特征的协同作用来缓解。在每次训练的时候，每个神经元有百分之50的几率被移除，这样可以让一个神经元的出现不应该依赖于另外一个神经元。 CNN经典框架：LeNet：开始用于手写数字字体识别32*32，处理不了大型的图片，用于缺少计算机资源的时候。3个卷积层大小为5x5，2个pooling 层，大小为2x2。 输入层，尺寸大于任何一个字母，以保证每个字母都会出现在第七层单元的感受野的中心。 中间五层分别是：卷积层→降采样层→卷积层→降采样层→卷积层。 第一个卷积层使用了六种滤波器，因此具有六个通道的 feature maps 。 第二个卷积层上升到16个通道。每一个通道与前6个通道的关系都不一样，见上图，目的是破坏对称性，迫使每个通道学习不同的特征（理想情况是互补特征）。 在全连接层，特征进行内积和非线性激活。 最后是输出层，10种数字对应10个输出单元，分别计算输出向量和该分类参考向量的欧式距离。 loss 为 MSE loss，输出向量和分类参考向量最近则将其判为这一类。AlexNet：AlexNet在2012年imageNet比赛上大放异彩，引发了神经网络的高潮，AlexNet共有5个卷积，5个pool，loss为softMax loss。 该网络有以下的创新：A. ReLU之前使用的 tanh 和 sigmoid 激活函数都存在饱和区。改用无饱和的 ReLU ，收敛速度可以达到数倍于 tanh ！B. Training on Multiple GPUs2个 GPU 协同，最直接的作用是加快了训练速度。作者尝试将网络改为单GPU，同时保证参数数量不变，速度略逊于双 GPUs 。C. Overlapping Pooling实验证明，重叠池化可以更好地抑制过拟合，使准确率提高约0.4%和0.3%。D. Data Augmentation最简单的抑制过拟合技术，就是 label-preserving transformations 。简单来说，就是让图像进行各种不影响目标本质的变换，扩大数据量。 - 镜像对称变换； - 图像光照强度和色彩变换。 第二点具体而言： - 先提取 RGB 三通道分量； - 对每一个通道分别进行主成分分析，提取出主成分； - 然后再进行三通道的随机系数线性组合。 E. Dropout如果我们有多个不同的模型合作进行预测，那么泛化误差将会有效降低。问题是，训练多个模型的计算成本很高昂。Dropout 为我们提供了新思路：让这些模型分享相同的权重系数，但神经元的输出结果不尽相同。具体而言，是让 hidden neuron 的输出有50%的概率被置零。这样，每次反向传播时，参考的 loss 都是由不同模型计算得到的。总的来说，Dropout 技术打破了神经元之间的依赖性，强迫网络学习更鲁棒的神经元连接。我们只在全连接层使用，因为全连接层的连接非常多。在测试阶段不采用 Dropout 。Dropout 会延长收敛时间，但能有效抑制过拟合。 VGG Net：VGG相对Googlenet虽然精度略逊些，但其整体网络框架还是延续了Alexnet及更早的Lenet等的一贯思路，此外还更深入的探讨了ConvNet深度对模型性能可能的影响。由于其整个网络结构的简单、强大，VGG16/VGG19曾一度广泛被用作各种检测网络框架像Faster-RCNN/SSD等的主干特征提取网络，直到Resnet提出之后，它才渐渐完成了其历史使命，退居二线。VGGnet有许多中深度的版本，他们基本采用了3x3的Conv kernel，pad/stride为1，只是在其中的若干Conv层后会置MaxPool层来作特征的上采样以高度抽象特征，节省后续的计算。然后在每个网络的最后则是同其它分类网络一样的若干个FCs层及Softmax。其中VGG16与VGG19最为受人欢迎（最深）。 作者表明：两个级联的3x3 conv或三个级联的3x3 conv分别在理论上等价于一个5x5 conv及一个7x7 conv。不过它们所具的模型参数要大大小于后面两者的参数。同时作者实验表明更深（层数更多）而非更宽（conv channels更多）的网络有着自动规则自己参数的能力，因此有着更好的学习能力。VGG使用与AlexNet相同的SGD对网络进行训练。VGG16相比AlexNet的一个改进是采用连续的几个3x3的卷积核代替AlexNet中的较大卷积核（11x11，5x5）。对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核是优于采用大的卷积核，因为多层非线性层可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少），相比于AlexNet使用更小的卷积核级联，更深的网络。 GoogleNet：（inception）尽管VGG可以在ImageNet上表现很好，但是将其部署在一个适度大小的GPU上是困难的，因为需要VGG在内存和时间上的计算要求很高。由于卷积层的通道数过大，VGG并不高效。在此之前经典的CNN模型像LeNet/Alexnet/VGG等无不是一个模子即使用Conv/Pool/Normalization/Activation等层来不断累积而成。模型对数据集概率分布的表达能力则往往通过单纯增加模型的深度（层数）或宽度（层的channels数）来提高（当然这也亦是当下深度学习领域的共识）。但这样进行网络设计一般会等来巨量的计算开销，因为每一层channels数目的增加都会随着层深而指数级增加，这大大地限制了模型的实际应用。 GoogleNet则从提高精度以及减少计算量的角度出发，想通过一种spared layer architecture来实现较优的多维度特征表达（inception module），然后通过对这种结构进行叠加，中间不时再插入一些MaxPool层以减少参数数目（从而节省内存与计算开销），最终就形成了Inception v1分类模型。GoogleNet团队计算效率以及GPU对密集计算的优化等等，选择了密集计算子结构组合而成的稀疏模块来用于特征提取及表达，这就是用于构建Inception v1的Inception module如上图中a所示。其中1x1/3x3/5x5这三种Conv kernels的选择决定是基于方便，因为这几种kernels用的多，而且比较容易对齐,padding。但是a中的模型计算量太大，因此作者在每个子conv层里使用了1x1的conv来作上一层的输入feature maps的channels数缩减、归总。例如：假设输入时 256 个 feature map 进来，256 个 feature map 输出，假设 Inception 层只执行 3x3 的卷积，那么这就需要这行 (256x256) x (3x3) 次卷积左右（大约 589,000 次计算操作），此时每一个特征的channel为256。现在 Bottleneck layer 的思想是先来减少特征的通道数， 操作量(每次卷积核参数)是：256(channel)×64(个) × 1×1 = 16,000s -&gt; 与1x1的卷积层做一次卷积，通道数缩减为6464(channel)× 64(个) × 3×3 = 36,000s64× 256(个) × 1×1 = 16,000s上诉处理能够大大减小计算量。模型的最后会选通过一个7x7的AvgPool层来处理最终的feature maps，大大降低了参数量。然后再由FC层汇总生成1000个输出，进而由Softmax来得到1000类的概率分布。 ResNet：Resnet分类网络是当前应用最为广泛的CNN特征提取网络。它的提出于2015年。残差学习：若将输入设为X，将某一有参网络层设为H，那么以X为输入的此层的输出将为H(X)。一般的CNN网络如Alexnet/VGG直接通过训练学习出参数函数H的表达，即直接得到H(X)。而残差学习则是学习输入、输出之间的残差即H(X) - X。即学习得到 (H(X) - X) 。最终的网络输出为：残差+X，其中X直接由identity mapping得到，而H(X) - X则为有参网络层要学习的输入输出间残差，优化难度大大减小。identity mapping：我们在输入与输出之间建立了一条连接，成为identity map，主要作用是将X传递到输出中，当输出与输入的channel数不一致时，通过直接补0或者用1x1 conv来映射。在处理一些很复杂的数据集时，作者引入bottleneck结构，即下图的1x1 的conv，第一个conv用来降低通道数，最后一个conv用来恢复通道数，这样的操作是的中间的conv维度不受输入影响，降低运算量。 退化现象：退化现象产生的原因在于当模型的结构变得复杂时，随机梯度下降的优化变得更加困难，导致网络模型的效果反而不如浅层网络。通过建立identity map可以将浅层的信息传入深层网络，可以很好的缓解退化现象。 inception V2/V3：inception V2/V3遵循上面的思路，进一步对inception v1结构中较大的卷积核进行分解。例如将5x5的卷积核分解成两个级联的3x3的卷积核，减少参数的同时，增加了网络的学习能力。更高效的下采样方式：由于对features map做pooling将会损失掉一部分的信息，为了减少这种信息的损失，在VGGnet中，通常的做法是pooling 的同时增大features map的channel的数量。googlenet中的做法是，分类对features map进行conv以及pooling，然后将最后得到的feature maps进行组合，得到最终的feature map。作者认为，inception v1 中的辅助分类器起到的作用是对网络底层的参数进行归一化的作用，因此inception v3 在inception v2的基础上在辅助分类器中使用BN对参数进行regularization。同时在最终的loss中增加了标签平滑，用label的先验避免过拟合发生。 inception v4inception v4使用tensorflow完成，涉及结构更加复杂，计算量也相比比较小。]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习中常用的技术（面试考点）]]></title>
    <url>%2F2019%2F02%2F19%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8A%80%E6%9C%AF%EF%BC%88%E9%9D%A2%E8%AF%95%E8%80%83%E7%82%B9%EF%BC%89%2F</url>
    <content type="text"><![CDATA[深度学习中常用的技术（面试考点）（一）神经网络中，防止过拟合的方法有： early stop（及早停止），当在测试集上出现错误率上升时，及时停止。 data expanding (扩大训练数据) dropout 技术（随机丢弃） 加入正则项 BN（让激活函数的输入分布保持在一个稳定状态来尽可能避免它们陷入梯度饱和区。） dropout技术dropout是一种防止过拟合的正则化技术，具体做法是，对每个隐藏层的输入进行一个概率判决，比如我们设置概率为0.5（通常命名为keep_prob）,根据0.5，随机生成一个跟隐藏层神经元个数相同的向量，true:false的比例是1：1（因为keep_prob=0.5），与隐藏层的神经元进行相乘，那么会有一半隐藏层的神经元被舍弃，不参与训练。重复迭代上诉操作。dropout的实现：12345678910111213141516171819#用一个二项分布的函数，等概率的生成0/1，既可以随机的屏蔽掉某些神经元#dropout函数的实现def dropout(x, level): if level &lt; 0. or level &gt;= 1:#level是概率值，必须在0~1之间 raise Exception('Dropout level must be in interval [0, 1[.') retain_prob = 1. - level #我们通过binomial函数，生成与x一样的维数向量。binomial函数就像抛硬币一样，我们可以把每个神经元当做抛硬币一样 #硬币 正面的概率为p，n表示每个神经元试验的次数 #因为我们每个神经元只需要抛一次就可以了所以n=1，size参数是我们有多少个硬币。 sample=np.random.binomial(n=1,p=retain_prob,size=x.shape)#即将生成一个0、1分布的向量，0表示这个神经元被屏蔽，不工作了，也就是dropout了 print sample x *=sample#0、1与x相乘，我们就可以屏蔽某些神经元，让它们的值变为0 print x x /= retain_prob # 归一化 return x#对dropout的测试x=np.asarray([1,2,3,4,5,6,7,8,9,10],dtype=np.float32)dropout(x,0.4) dropout通常使用在一些较大的网络中，在训练阶段使用，在测试或者预测时并不会去dropout，工业上的做法是在输入的X上乘以P得到X的期望，或者输入不做变化而是对所有的有dropout层都做X/p。dropout能防止过拟合： 多尺度学习：由于每次dropout舍弃的神经元均不相同，因此每次训练都产生一个不同的神经网络。这种组合多种神经网络的综合效果的方式能够有效的缓解过拟合效应。 阻止特征的协同作用：可以通过阻止某些特征的协同作用来缓解。在每次训练的时候，每个神经元有百分之50的几率被移除，这样可以让一个神经元的出现不依赖于另外一个神经元。 Batch Normalization 参考链接深层网络难以训练，由于底层网络中参数发生微弱变化时，由于每一层中的线性变换与非线性激活映射，这些微弱变化随着网络层数的加深而被放大（类似蝴蝶效应）；参数的变化导致每一层的输入分布会发生改变，进而上层的网络需要不停地去适应这些分布变化，使得我们的模型训练变得困难。上述这一现象叫做Internal Covariate Shift。Internal Covariate Shift： 在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal Covariate Shift。因此而带来的问题： 上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低 网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度（可以使用线性整流函数ReLU因为它可以在一定程度上解决训练进入梯度饱和区的问题） 缓解Internal Covariate shiftICS产生的原因是由于参数更新带来的网络中每一层输入值分布的改变，并且随着网络层数的加深而变得更加严重，因此我们可以通过固定每一层网络输入值的分布来对减缓ICS问题。常用的方法如下：白化：使得输入特征分布具有相同的均值与方差。其中PCA白化保证了所有特征分布均值为0，方差为1；而ZCA白化则保证了所有特征分布均值为0，方差相同；去除特征之间的相关性。但是由于白化操作计算成本高，且将会改变网络每一层参数的分布，使得网络底层学到的信息被丢弃batch normalization 单独对每个特征进行normalizaiton(每一层)，让每个特征都有均值为0，方差为1的分布，减少计算量。线性变换操作，让网络恢复本身的表达能力。BN插在在全连接层之后如下图：BN操作如下： 对输入取均值 对输入取方差 计算normalize后的输入（其中 $\epsilon$ 是为了防止方差为0产生无效计算） 反标准化进行学习 反标准化是为了让神经网络能够学习batch normalization的平移拉伸，让数据再能够尽可能恢复本身的表达能力就好，达到学习的目的。 BN作用： BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度 BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定。BN不会受到权重scale的影响，因此其能够使模型保持在一个稳定的训练状态；而没有加入BN的网络则在一开始就由于学习率过大导致训练失败BN的网络能够克服如此bad的权重初始化 BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题 BN具有一定的正则化效果：在Batch Normalization中，由于我们使用mini-batch的均值与方差作为对整体训练样本均值与方差的估计，尽管每一个batch中的数据都是从总体样本中抽样得到，但不同mini-batch的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，在一定程度上对模型起到了正则化的效果。 正则项L1 正则项： L1是模型各个参数的绝对值之和,将它添加到损失函数上：$$\min \frac{1}{N}\sum_{i = 1}^{N}{(y_{i} -\omega^{T} x_{i})^{2} } + C||\omega||_1$$L2 正则项：是模型各个参数的平方和的开方值：$$\min \frac{1}{N} \sum_{i = 1}^{N}{(y_{i} -\omega^{T} x_{i})^{2} } + C||\omega||_{2}^{2}$$添加L1和L2正则项之后的损失函数如下：如图可以看出，如果仅有损失函数的话，优化目标为损失函数最内圈的紫色的环。但是给loss function加上正则化项，能使得新得到的优化目标函数h = f+normal，需要在f和normal中做一个权衡（trade-off），即最优解应该使得正则项和模型损失函数之和最小。 可以看出来，L1正则项与loss更多的相交于坐标轴，因此L1更容易产生稀疏解。L2的解比较接近与坐标轴，L2范数能让解比较小（靠近0），但是比较平滑（不等于0）。 正则项降低过拟合程度： L1正则化：在loss function后边所加正则项为L1范数，加上L1范数容易得到稀疏解（0比较多），能够避免过拟合。有助于生成一个稀疏权重矩阵，进而可用于特征选择。 L2正则化：在loss function后边所加正则项为L2范数的平方，L2控制w的大小，则w的幅度较小且较均匀。一般认为参数值较小的模型比较简单，能适应不同的数据集，一定程度上避免了过拟合。（缺点是L2对离群点敏感，而且容易造成梯度爆炸） 在Faster RCNN中，边框回归通常情况下，使用平方误差最小，即L2loss，但是由于，L2 loss对离群点比较敏感，同时，当预测边框距离真值边框比较远的时候，容易出现梯度爆炸的问题，因此使用smooth L1替代L2 loss，smooth L1 相比于正常的L1它是可导的。且导数是一个常数。 如何处理数据特征缺失项： 如果数据集样本很多，可以删除掉缺失的特征的个别样本。 用平均值，中位数，众数进行替换补全。（人为的增加了噪声） 使用一些机器学习的算法对数据特征进行恢复，如EM算法等等，KNN算法 异常值的检测： 当数值在$(\mu -3\sigma,\mu+3\sigma)$之外时，属于异常数值。 使用K nearnest neighbour计算每一个点的K近邻，然后距离临近点距离最远，而且周围的邻居位置很稀疏的情况下，这个点很可能是异常点。 canny边缘检测介绍：canny边缘检测是一个基于图像梯度的边缘检测算法。由于图像边缘即图像中的高频部分，噪音也属于高频信息，因此首先需要对图像进行去噪（高斯滤波器），然后提取图片梯度，然后对提取的梯度做一些例如非极大值抑制等处理，总之canny算子没有考虑到图片全局的信息，仅仅使用了梯度来提取边缘。对于一些梯度不明显的边缘信息可能无法很好的提取。 max pooling 与 average pooling的应用有何不同： 使用pooling技术将小邻域内的特征点整合，同时保持某种不变性（旋转、平移、伸缩等）。average-pooling对领域内特征取平均值，结果融合了所有的特征。平均操作类似与平滑处理，能够保留图片的低频信息，即更多的保留图像的背景信息。因此更多用在最后的分类中。max-pooling对领域内的特征值取最大值，即能够极大的保留图片的边缘信息，纹理信息。一张图片的高频信息能够极大程度的表示一个物体，因此进行下采样特征缩减时更多用到max-pooling。 训练过程中学习率如何调整： 从大到小依次衰减 或者使用RMSprop更新法，在累计梯度的平方项上进行衰减。 CNN网络中全连接层的作用：全连接层将学到的“分布式特征表示”映射到样本标记空间（进行分类）。全连接层参数过多（一个大型的分类问题，参数量通常占到80%）不宜有太多层全连接层。是把卷积提取的特征看做多层感知机的输入节点，后面只需要接两层全连接理论上就可以拟合任意非线性函数， GAP（全局平均池化）：将每张feature的值全部加起来，取平均，每一个均值代表一个类别。比如有10个类，就在最后输出10个 feature map，每个feature map中的值加起来求平均值，这十个数字就是对应的概率或者叫置信度。然后把得到的这些平均值直接作为属于某个类别的 confidence value，再输入softmax中分类。用GAP代替全连接层可以大幅减小参数量，同时检测效果不会变差。 维度灾难：对于大多数数据，在一维空间或者说是低维空间都是很难完全分割的，但是在高维空间间往往可以找到一个超平面，将其完美分割。于是我们将维度提升，例如从2维到3维这样就可以区分开物体了。但是无限制的增大数据的纬度，会出现分类进度极速下降的问题。即分类器过拟合，出现维度灾难。 聚类方法：K-means 聚类 首先确定样本的类别数n，然后在样本上随机确定n个中心 然后计算每一个样本到样本中心的距离，将该样本划分到距离它最近的那一类中 对划分过的样本重新计算各类的类中心 重复上述步骤，直到类中新位置不发生明显变化为止 基于密度的聚类方法(DBSCAN) 首先确定半径r和minPoints. 从一个没有被访问过的任意数据点开始，以这个点为中心，r为半径的圆内包含的点的数量是否大于或等于minPoints，如果大于或等于minPoints则该点被标记为central point，反之则会被标记为noise point。 重复上面的步骤，如果一个noise point存在于某个central point为半径的圆内，则这个点被标记为边缘点，反之仍为noise point。重复上述步骤，直到所有的点都被访问过。 混合高斯模型（GMM）最大期望（EM）聚类： 选择簇的数量（与K-Means类似）并随机初始化每个簇的高斯分布参数（均值和方差）。 给定每个簇的高斯分布，计算每个数据点属于每个簇的概率。一个点越靠近高斯分布的中心就越可能属于该簇。 基于这些概率我们计算高斯分布参数使得数据点的概率最大化，可以使用数据点概率的加权来计算这些新的参数，权重就是数据点属于该簇的概率。 重复迭代2和3直到在迭代中的变化不大。 自顶向下的层次分类： 将所有样本视为一类，然后对样本进行m次二分实验，然后选择一种分类，分类后的两簇SSE（Sum of the Squared Error）之和最小。 选择最大SSE的簇，然后对他重复上述分类，直到分类到k个簇。 L1 loss 为什么会导致稀疏解：如下图，原函数设为L，它的极小点为绿色的点，不在原点。加上L2 loss之后L+L2的极小点为黄点。加上L1后L+L1的极小点为红点。 为什么L1 loss的最小点就是原点呢？要形成极小值点，以上图为例，x0 时导数&gt;0 (函数增)，x从左边趋近于0 时，C|x|的导数是-C，假设此时 L 的导数为 La ，必须有 La -C &lt;0，即C&gt;La，同理x从右边趋近于0时，必须有 Lb + C &gt; 0 ，即C&gt;-Lb，所以说C要大于L在0点附近的绝对值。即原点左右两边的导数正负不同，原点为一个极小点。 海量数据球中位数：使用堆的思想。查找中位数，也就是找出中间最大的数字，总共10G的数据，查找第5G大的数据，创建一个1G的大顶堆，遍历一遍这个10G的数据，找出前1G大的数据，在这个大顶堆中找出最小的值，这个最小的值就是这10G数据中第1G大的元素，然后利用这个元素在创建大顶堆，比这个元素小的才能进堆，那么就创建了从1G到2G的元素，这么一来，就找到了第2G大的元素，利用第2G大的元素就可以找到第5G大的元素，这么一来就可以找到中位数了。 pooling 层如何进行反向传播： max pooling层：对于max pooling，下一层的误差项的值会原封不动的传递到上一层对应区块中的最大值所对应的神经元，而其他神经元的误差项的值都是0； mean pooling层：对于mean pooling，下一层的误差项的值会平均分配到上一层对应区块中的所有神经元。]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Faster RCNN详解]]></title>
    <url>%2F2019%2F02%2F14%2FFaster-RCNN%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Faster RCNN详解Faster RCNN 是在Fast RCNN的基础上，进一步改进，解决select search 算法选择候选框速度太慢的问题。 Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networkssubmit time: 2016arxiv link fast R-CNN和faster R-CNN之间的区别在于我们不使用特殊区域提议方法来创建region proposal。而是训练一个region proposal network（RPN），该网络将features map 作为输入并输出region proposals。然后将这些proposal输入Fast R-CNN中的RoI池化层。以下是fast RCNN与Faster RCNN的网络结构对比图。Faster RCNN 关键步骤： Conv layers。作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。 Region Proposal Networks。RPN网络用于生成region proposals。该层通过softmax判断anchors属于foreground或者background，再利用bounding box regression修正anchors获得精确的proposals。 Roi Pooling。该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。 Classification。利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。 Faster RCNN 网络是用于目标检测的一种比较流行的框架。它主要由以下四个部分组成 分别是conv layer 输入为原始图片，用于提取图片的feature map RPN网络，输入为features map，用于生成region proposal，该层为features map 上每个像素生成若干个anchors（9个），随后通过softmax 判断每个anchor是属于foreground（目标）或者background（背景），再利用bounding box regression修正anchors获得精确的proposal位置。 RoI pooling，该层输入为proposal位置信息和features map，通过proposal的位置信息在features map 上提取region features map候选区，然后通过pooling产生一个固定长度的特征，送入全连接层进行目标判别。 classification，利用proposal feature maps计算proposal的类别，同时再次进行一次bounding box regression，对proposal位置进行精修，随后将结果输出。 总结一套介绍网络框架的先后顺序的方法。 可以先大后小，按照先后顺序从前到后，按功能性介绍一件事情，每件事情的功能介绍的时候，说清楚输入，工作流程附带其具体功能，输出。 Faster RCNN 详细网络结构如图： 将一副任意大小PxQ的图像，首先缩放至固定大小MxN，然后将MxN图像送入网络；而卷积层 Conv layers中包含了13个conv层+13个relu层+4个pooling层；RPN网络首先经过3x3卷积，再分别生成foreground anchors与bounding box regression偏移量，然后计算出proposals；而Roi Pooling层则利用proposals以及feature maps，提取proposal feature送入后续全连接和softmax网络作classification。 conv layer Conv layers部分共有13个conv层，13个relu层，4个pooling层。 所有的conv层都是： kernel_size=3 ， pad=1 ，stride=1，因此conv层不改变原图大小 所有的pooling层都是： kernel_size=2 ，pad=0 ， stride=2，pooling 层将原图缩小为原来的一半 经过Conv layer后，一个MxN大小的矩阵将变为(M/16)x(N/16) Region Proposal Networks(RPN)Faster RCNN 层在fast RCNN 的基础上，对提取候选框进行优化。 RPN网络分为2条线，上面一条通过softmax分类anchors获得foreground和background（检测目标是foreground），下面一条用于计算anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合foreground anchors和bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。 anchorsanchor为由一个中心点，周围生成了9个矩形，矩形长宽比由三个尺寸1:1,1:2;2:1三种，如下图，基本覆盖了各种尺寸和形状，引入检测中常用到的多尺度方法。Faster RCNN遍历Conv layers计算获得的feature maps，为feature map上每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，之后将会在RPN层，以及最后进行2次的bounding box regression修正检测框位置。如上图，对于每一个点的k个anchor来说，从conv layer提取出得特征具有256维，对于每一个anchor，需要分foreground与background，因此共有2k个score，对于每一个anchor共有$(x_1,y_1,x_2,y_2)$四个坐标值。因此共有4k个coordinates。在训练阶段，程序将会从这些anchor中挑选出一些合适的anchor进行训练。因此RPN最终就是在原图尺度上，对每一个像素设置9个尺度的候选anchor。然后用cnn去判断哪些Anchor是里面有目标的foreground anchor，哪些是没目标的backgroud。所以，仅仅是个二分类而已！那么Anchor一共有多少个？原图800x600，VGG下采样16倍，feature map每个点设置9个Anchor，所以：$$ceil(800/16) \times ceil(600/16) \times 9=50\times38 \times9=17100$$其中ceil()表示向上取整，是因为VGG输出的feature map size= 50*38。 softmax判定foreground与backgroundRPN网络中利用anchors和softmax初步提取出foreground anchors作为候选区域。features map 首先做一个1*1的卷积，这个卷积的作用是生成一个$W*H*(9*2)$大小的矩阵。该矩阵用于存储上面提到的foreground与background信息（2*k score）。将该特征后接softmax分类获得foreground anchors，也就相当于初步提取了检测目标候选区域box（一般认为目标在foreground anchors中）。前后两个reshape 操作目的为便于程序实现。clc layer输出预测区域共k个，每个有的2个参数，即预测为前景的概率和背景的概率，损失用softmax loss（cross entropy loss）。监督信息是Y=0,1，表示这个区域是否为groundtruth。确定groundtruth时，我们需要确定k个区域中的各个区域是不是有效的，是前景还是背景。K个区域分配标签规则： 与某个ground truth(GT)的IoU最大的区域的分配正标签 与任意GT的IoU大于0.7的区域分配正标签 与所有GT的IoU都小于0.3的区域分配负标签 bounding box regression原理对于窗口一般使用四维向量 (x, y, w, h) 表示，分别表示窗口的中心点坐标和宽高。对于图 11，红色的框A代表原始的Foreground Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G’，即： 给定：$anchor A=(A_{x}, A_{y}, A_{w}, A_{h}) 和 GT=[G_{x}, G_{y}, G_{w}, G_{h}]$寻找一种变换F，使得：$F(A_{x}, A_{y}, A_{w}, A_{h})=(G_{x}^{‘}, G_{y}^{‘}, G_{w}^{‘}, G_{h}^{‘})，$其中$(G_{x}^{‘}, G_{y}^{‘}, G_{w}^{‘}, G_{h}^{‘})≈(G_{x}, G_{y}, G_{w}, G_{h})$那么经过何种变换F才能从图10中的anchor A变为G’呢？ 比较简单的思路就是先做平移，然后进行缩放，边框回归与RCNN中边框回归相同。bounding box 原理参考链接RPN中所涉及的边框回归首先经过一个1*1的卷积层，输出一个$W*H*(9*4)$的矩阵，用于存储box的坐标信息（4k coordinate） RPN值得注意的地方: - RPN在原图的尺度上选择anchor的大小- anchor的数目是feature map上每个像素选择9个长宽比不同的矩形- soft Max层用于判断anchor是否为前景（含有目标）- bounding box regression 预测的输出是anchor的偏移变换- proposal层，结合前景的anchor（背景anchor被忽略）与anchor偏移变换，对anchor位置进行调整，计算出proposal的精确位置。- bounding box 本质上是学习一个W权重矩阵，即那个1*1的网络的参数（输出为4K regreason,对应anchor的（x，y,w,h）四个偏移），利用W参数乘以 CNN pool5层输出的features map，通过最小二乘，得到anchor的偏移。- 为什么bounding box regression不直接预测坐标呢？ 因为坐标间的关系不是简单的一维关系，难以优化。当anchor 与 ground truth比较接近时，他们之间的位置关系（偏移）就可以用一维关系来近似。- proposal层输出的proposal坐标是在原图的尺度上的proposal坐标。#### proposal layerRPN 最后一层为proposal layer，用于前景anchors，以及anchor对应的边框回归微调参数$[d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)]$和im_info=[M, N, scale_factor]（传入Faster RCNN前首先reshape到固定MxN，im_info则保存了此次缩放的所有信息）来计算产生的proposal位置，此时输出的proposal坐标为原图尺度上的proposal坐标。Proposal Layer forward（caffe layer的前传函数）按照以下顺序依次处理：- 生成anchors：利用$[d_{x}(A),d_{y}(A),d_{w}(A),d_{h}(A)]$对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致）- 按照输入的foreground softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的foreground anchors。- 限定超出图像边界的foreground anchors为图像边界（防止后续roi pooling时proposal超出图像边界）- 剔除非常小（width&lt;threshold or height&lt;threshold）的foreground anchors- 进行nonmaximum suppression- 再次按照nms后的foreground softmax scores由大到小排序fg anchors，提取前post_nms_topN(e.g. 300)结果作为proposal = [x1, y1, x2, y2]输出。输出的proposal=[x1, y1, x2, y2]，由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应MxN输入图像尺度的。RPN网络结构主要步骤如下：生成anchors -&gt; softmax分类器提取前景 anchors -&gt; bbox reg回归前景 anchors -&gt; Proposal Layer生成proposals#### RoI pooling layerRoI Pooling layer负责收集proposal，并计算出proposal feature maps，送入后续网络。Rol pooling层有2个输入：- 原始的feature maps- RPN输出的proposal boxes（大小各不相同）RoI Pooling layer forward过程：- 由于proposal是对应$ M\times N$ 尺度的，所以首先使用spatial_scale参数将其映射回 $(M/16)\times(N/16) $大小的feature map尺度；- 再将每个proposal对应的feature map区域水平分为 $\text{pooled_w}\times \text{pooled_h} $的网格；- 对网格的每一份都进行max pooling处理。经过上述处理后，即使大小不同的proposal输出结果都是 $\text{pooled_w}\times \text{pooled_h}$ 固定大小，实现了固定长度输出。#### ClassificationClassification部分利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出cls_prob概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量bbox_pred，用于回归更加精确的目标检测框。从PoI Pooling获取到7x7=49大小的proposal feature maps后，送入后续网络，可以看到做了如下2件事：- 通过全连接和softmax对proposals进行分类- 再次对proposals进行bounding box regression，获取更高精度的rect box#### Faster R-CNN训练Faster R-CNN的训练，是在已经训练好的model（如VGG_CNN_M_1024，VGG，ZF）的基础上继续进行训练。实际中训练过程分为6个步骤：- 在已经训练好的model上，训练RPN网络- 利用步骤1中训练好的RPN网络- 第一次训练Fast RCNN网络- 第二训练RPN网络- 再次利用步骤4中训练好的RPN网络- 第二次训练Fast RCNN网络可以看到训练过程类似于一种“迭代”的过程，不过只循环了2次。至于只循环了2次的原因是应为作者提到：”A similar alternating training can be run for more iterations, but we have observed negligible improvements”，即循环更多次没有提升了。#### RPN 训练与检测网络类似的是，依然使用Conv Layers提取feature maps。整个网络使用的Loss如下：$$L({p_i},{t_i})=\frac{1}{N_{cls}}\sum_{i} L_{cls}(p_i,p_i^*)+\lambda \frac{1}{N_{reg}}\sum_{i} p_i^* L_{reg} (t_i,t_i^*)$$上述公式中 i 表示anchors index，$ p_{i}$ 表示foreground softmax probability，$p_{i}^{*}$代表对应的GT predict概率（即当第i个anchor与GT间IoU&gt;0.7，认为是该anchor是foreground，$p_{i}^{*}=1$；反之IoU&lt;0.3时，认为是该anchor是background，$ p_{i}^{*}=0 $；至于那些0.3&lt;IoU&lt;0.7的anchor则不参与训练）；t代表predict bounding box，$ t^{*} $ 代表对应foreground anchor对应的GT box。可以看到，整个Loss分为2部分：- cls loss，即rpn_cls_loss层计算的softmax loss，用于分类anchors为forground与background的网络训练- reg loss，即rpn_loss_bbox层计算的soomth L1 loss，用于bounding box regression网络训练。注意在该loss中乘了 $p_{i}^{*}$ ，相当于只关心foreground anchors的回归（其实在回归中也完全没必要去关心background）。Smooth L1 loss 相比于L2 loss对离群点更加不敏感，更加鲁棒。当预测值与目标相差很大时，L2 loss的梯度是x-t，容易产生梯度爆炸，而L1的梯度为常数，使用L1 loss 可以防止梯度爆炸。 关于softMax loss 和 边框回归loss与fast RCNN 相同。链接]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fast RCNN详解]]></title>
    <url>%2F2019%2F02%2F14%2FFast-RCNN%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Fast RCNN详解SPP-Net改造了RCNN，使用SPP layer使得输入图片大小不受限制，同时使用region proposal映射的方式，大大加速了目标检测的速度，但是SPP-net训练需要花费很多时间，同时fine-tune不能越过SPP层，因为pyramid BP开销太大了（金字塔感受野比较大），只能fine-tune全连接层，tune不到卷积层，所以在一些较深的网络上准确率上不去。Fast RCNN 受到SPP-Net网络，在网络卷积层后加入ROI层（region of interesting）。此外，损失函数使用了多任务损失函数(multi-task loss)，将分类和边框回归两个loss统一到一个网络中一起训练。 Fast RCNNsubmit time: 2015arxiv link Fast RCNN网络结构如下：Fast RCNN关键步骤： select search 算法提取2k个region proposal 区域 将整张图片输入CNN网络中，提取出整张图片的特征 将2k个region proposal区域映射到feature maps上（RoI projection） 通过RoI pooling layer，将features map上的大小不一致的region proposal变成固定长度的特征向量。 将特征向量通过一系列FCs层分别输入softMax，以及bbox regression。利用Softmax Loss(探测分类概率) 和Smooth L1 Loss(探测边框回归)对分类概率和边框回归(Bounding box regression)联合训练。 ROI pooling layerRoI池化层使用最大池化将任何有效区域内的特征转化成一个小的带有固定空间范围HxW（比如7×7）的特征图，其中H和W是层的超参数，和任何特定的RoI无关。本文中，一个RoI是针对卷积特征图的一个矩形窗口。每个RoI定义成四元组（r, c, h, w），左上角为（r, c），高和宽是（h, w）。RoI最大池化将hxw的RoI窗口分成HxW的子窗口网格，每个子窗口大小大约是h/H x w/W。然后每个子窗口进行最大池化放入网格对应的单元。池化以标准最大池化的形式独立应用在每个特征图的channel上。RoI层是SPPnets中的空间金字塔层的一个特例，因为他是一个一层的金字塔结构。即将一个hw大小的框转化为HW大小的框，每个H*W的网格为h/H x w/W区域内最大的值(max-pooling)表示。 为目标检测任务做微调 分层采样得到SGD的mini-batch，首先采样N个images，然后每个image采样R/N个ROIs。来自同一个image的ROIs在前向后向传输时共享计算和内存，减小N 则能降低mini-batch的计算。这种分层采样的策略实际中不会减慢收敛速度。作者使用N=2, R=128， 并发现SGD迭代次数比R-CNN的还少。 联合优化softmax分类器和bbox regressor回归器 Multi-task Losssoftmax类别分类器R-CNN与SPPNet均使用SVM作为分类器，而Fast R-CNN使用softmax作为分类器，以下为真实类属u的log loss，即p的值越到loss越接近1。$$L_{cls}(p, u) = -logp_u$$ softmax函数可以将连续数值转换为相对概率：$$P_i= \frac{e^{V_i}}{\sum_i^C{e^{V_i}}}​$$实际应用中，使用 Softmax 需要注意数值溢出的问题。因为有指数运算，如果 V 数值很大，经过指数运算后的数值往往可能有溢出的可能。所以，需要对 V 进行一些数值处理：即 V 中的每个元素减去 V 中的最大值。$$\begin{align}D = \max(V) \nonumber\\P_i= \frac{e^{V_i-D}}{\sum_i^C{e^{V_i-D}}} \nonumber\end{align}$$由于log函数不会改变函数单调性，所以通常对softMax函数取一个 $-\log$，表示损失函数。 边框回归loss：边框回归使用$L_1$ loss，第二个loss $L_{loc}$是定义真值和预测值上，第一个是针对类u的真实标注约束框回归目标 $v=(v_x, v_y, v_w, v_h)$，第二个也是针对类u的预测值$t^u = (t^u_x, t^u_y, t^u_w, t^u_h)$。对于约束框回归，边框回归的loss为：$$L_{loc}(t^{u},v) = \sum_{ i \in {x,y,w,h}} smooth_{L_{1}}(t_i^u - u_i)$$其中： 联合loss：$$L(p,u,t^u,v) = L_{cls}(p,u)+\lambda[u\geq 1] L_{loc}(t^{u},v)$$ 其中中括号项代表这样一个函数：当u ≥ 1时，返回1，否则返回0。根据约定代表全部剩余一切的背景类标注成u=0。所以对于背景RoI而言，没有真是标注框信息，因而$L_{loc}$就忽略了。 Mini-Batch 采样微调阶段，每次SGD迭代所用的mini-batch从N=2个images中获取， 这N个images随机选择，mini-batch的大小为128，每个image中采样64个ROIs。其中25%的样本为正样本，也就是IOU大于0.5的，其他样本为负样本，同样使用了困难负样本挖掘的方法（hard negative mining），也就是负样本的IOU区间为[0.1，0.5），负样本的u=0，$[u\geq 1]$函数为艾弗森指示函数。 RoI 反向传播不同于SPPNet，ROI Pooling可以反向传播，以Max Pooling为例，根据链式法则，对于最大位置的神经元偏导数为1，对于其他神经元偏导数为0。ROI Pooling 不用于常规Pooling，因为很多的region proposal的感受野可能是相同的或者是重叠的，因此在一个Batch_Size内，我们需要对于这些重叠的神经元偏导数进行求和，因此反向传播公式如下：$$\frac{\partial L }{ \partial x_{i}} = \sum_r \sum_{j} [i = i^*(r,j)]\frac{\partial L }{\partial y_{rj}}$$ $i^*(r, j) = argmax (i)∈R(r,j)$，也就是在R(r, j)这个区域中做max pooling得到的结果 $[i = i * (r, j)]$ 是一个条件表达式，就是判断input的xi是否是max pooling的结果，如果不是，输出的梯度就不传到这个值上面,不提供loss r是RoI数量，j是在一个region中，与x对应的输出个数 $y_{rj}$是第j个跟x对应的输出 如下，RoI层反向传播例子：fast rcnn的网络结构如下：]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SPP-Net详解]]></title>
    <url>%2F2019%2F02%2F13%2FSPP-Net%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[SPP-Net详解在fast RCNN 之前，RCNN的进化中SPP Net的思想对其贡献很大，下面先介绍一下SPP Net。 SPP-Net Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognitionsubmit time: 2015arxiv link 空间金字塔池化spatial pyramid pooling，是一种词袋(Bag-of-Words, BoW)模型的扩展。池袋模型是计算机视觉领域最成功的方法之一。它将图像切分成粗糙到精细各种级别，然后整合其中的局部特征。SPP-net允许任意尺寸的输入，也允许图像可以有各种尺寸和缩放尺度。SPP使用了多级别的空间箱(bin)，而滑窗池化则只用了一个窗口尺寸。多级池化对于物体的变形十分鲁棒。RCNN的不足之处 1）输入图像需要crop成固定尺寸将导致失真：rcnn里将所有的region warp成固定尺寸，导致图片会出现不同程度的缺失和失真扭曲2）时间和空间成本高：对每个region proposals都需要过一遍AlexNet，且需要落盘到本地磁盘，存储量大3）检测速度慢：对每个regions proposals均需要分别提取特征，用VGG16一幅图片需要47s SPP-Net关键步骤： 通过select search算法提取出2k个proposal region 将整张图片输入CNN中提取特征，得到整张图片的feature maps 将选择性搜索得到的2k个proposal区域映射到feature maps上(RCNN需要对每一个proposal提取一次特征，SPP-net只需要提取一次) 对feature map上的候选框采用金字塔空间池化，提取出固定长度的特征向量，输入FC层(SPP-net不需要对图片进行crop等操作) 将proposals区域的特征向量输入SVM分类器中进行类别分类。 SPP-Net 解决了下面几个问题： 如何解决输入图片尺寸必须固定的要求？金字塔池化 如上图由features map上确定的region proposal大小不固定，将提取的region proposal分别经过三个卷积4*4，2*2，1*1，都将得到一个长度为21的向量(21是数据集类别数，可以通过调整卷积核大小来调整)，因此不需要对region proposal 进行尺寸调整。 如何解决只进行一次特征提取的要求? SPP-Net在原图上选择region proposals区域，随后对图片提取特征，得到整张图片的features map，然后通过映射将region proposals区域映射到features map上，得到region proposal区域的特征。因此仅仅需要对原图提取一次特征即可。 如何将region proposal映射到特征空间?SPP-Net在提取完整图像的feature map后，要将候选框的位置映射到feature map中得到对应特征。映射原则如下：假设(x,y)是原始图像上的坐标点，(x′,y′)是特征图上的坐标，S是CNN中所有的步长的乘积，那么左上角的点转换公式如下：$$x′=\frac{x}{S}+1$$右下角的点转换公式为：$$x′=\frac{x}{S}−1$$计算S有下面例子：论文中使用的ZF-5: S = 2*2*2*2 = 16Overleaf-5/7：S = 2*3*2 = 12 SPP-Net训练策略：理论上，无论输入什么尺寸的图像，都可以输入SPP-net中进行训练。但是实际上由于GPU实现中，更适合在固定尺寸的输入图像上，因此提出了一些训练策略。 Single-size training:使用固定的224x224的输入，是从原始图像中裁切得到的，目的是为了数据扩增；对于给定的输入尺寸，可以预先计算出空间金字塔池化需要的bin size，假如feature map是axa的大小，那么在SPP layer中，窗口尺寸$win=\frac{a}{n}$上取整，步长$stride=\frac{a}{n}$下取整。 Multi-size training：考虑两种输入，180x180和224x224，这里不再用裁切，而是直接进行缩放，比如把224x224的图像直接缩放为180x180，它们之间的区别只是分辨率不同。实现两个固定输入尺寸的网络，训练过程中先在1号网络上训练一个epoch，然后用它的权重去初始化2号网络，训练下一个epoch；如此转换训练。通过共享两种尺寸输入的网络参数，实现了不同输入尺寸的SPP-Net的训练。 原始图片中的ROI如何映射到到feature map感受野：卷积神经网络CNN中，某一层输出结果中一个元素所对应的输入层的区域大小，被称作感受野receptive field。感受野的大小是由kernel size，stride，padding , outputsize 一起决定的。经过一层卷积后输出的features map大小计算： $$W_2 = （W_1- K + 2P）/S + 1$$（其中 $W_1$是输入卷积层的特征的尺寸，K是卷积核大小，P是填充padding，S是步长stride）上一层features map大小计算： $$W_1 = (W_2 - 1)*S -2P+K$$ 感受野的计算：感受野是这一层一个元素对应输入层的区域大小，可以一层一层回推到输入层。对于这一层相对于上一层的感受野也就是卷积核的大小。当已知上一层的感受野计算下一层的感受野时有：$$r = (m-1) stride+ksize$$其中m为上一层的感受野。空洞卷积的感受野计算：dilate rate = 1与普通卷积相同。dilate rate = 2可以视为卷积核由3\3变成了5*5，计算方法相同。对于dilate rate = 3，可可视为卷积核变成了9*9。感受野坐标映射： $$p_i = s_i \cdot p_{i+1} +( (k_i -1)/2 - padding)$$ SPP-Net中的坐标映射： SPP-Net 是把原始ROI的左上角和右下角 映射到 feature map上的两个对应点。 有了feature map上的两队角点就确定了 对应的 feature map 区域(下图中橙色)。变换公式见上。 总结 SPPNet在R-CNN的基础上提出了改进，通过候选区域和feature map的映射，配合SPP层的使用，从而达到了CNN层的共享计算，减少了运算时间，允许输入图片的大小不固定。Fast R-CNN受SPPNet的启发，进一步改进完网络。]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RCNN详解]]></title>
    <url>%2F2019%2F02%2F11%2FRCNN%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[目标检测目标检测任务主要有两个不同的思路。一种思路是借鉴语义分割的做法，这方面的工作有YOLO和SSD另一种思路是把目标检测看作分类任务（bounding box中对象的类别）和回归任务（回归bounding box）的组合。主要的工作有R-CNN，SPP-Net，Fast R-CNN，Faster R-CNN。方法一速度快但精度稍差，方法二速度慢但精度高，是主流方法。 RCNNRCNN: Region-based Convolutional NetworkSubmitted on 2014 RCNN目标识别的主要任务为检测物体类别以及边框的大小以及位置。主要贡献： 根据Selective search 算法提取Region proposal。 将每个Region Proposal 缩放到统一大小后输入CNN，输出固定大小的特征。 将特征用SVM进行分类。 训练一个回归器，对边框（bounding box）进行微调。 边框的选择：原始产生边框的方法为通过滑窗的方式产生region proposal，作者做过实验，原话如下： 我们也考虑了采用滑动窗口方法。然而，在我们的网络中，具有五个卷积层的单元在输入图像中具有非常大的接收场（195×195像素）和步进（32×32像素），这使得在滑动窗口内的精确定位成为开放的技术挑战。 selective search 算法 使用一种过分割手段，将图像分割成小区域 (2k~3k 个) 查看现有小区域，按照合并规则合并可能性最高的相邻两个区域。重复直到整张图像合并成一个区域位置 输出所有曾经存在过的区域，所谓候选区域 selective search 合并规则：颜色相近(颜色直方图)；纹理相近(梯度直方图)；合并后总面积小的；合并后总面积在其BBOX中所占比例大的(保证合并后形状规则) 多样化与后处理为尽可能不遗漏候选区域，上述操作在多个颜色空间中同时进行（RGB,HSV,Lab等）。在一个颜色空间中，使用上述四条规则的不同组合进行合并。所有颜色空间与所有规则的全部结果，在去除重复后，都作为候选区域输出。 RCNN卷积：将生成的region proposal 减去像素平均值后，使用各向异性的缩放方式（直接缩放），将图片缩放到227*227大小，随后对每个proposal 提取特征，对每个proposal经过五层卷积层以及两层全连接层，在cf7层得到提取出的4096维特征。提取特征使用了pre-training的AlexNet网络，作者原文如下： 检测面临的第二个挑战是带标记的数据很少，目前可用的数量不足以训练大型CNN … 本文的第二个主要贡献是识别网络在大型辅助数据集(ILSVRC)上进行监督预训练，然后对小数据集(PASCAL)进行指定域的微调，这是在数据稀缺时训练高容量CNN模型的有效方法。 即提取特征需要训练一个大型的CNN识别网络，作者使用了hinton在2012年image net上做识别的AlexNet，此网络提取的特征为4096维，之后送入一个4096-&gt;1000的全连接(fc)层进行1000个类别的分类，学习率0.01。针对特定的小数据集对该识别网络进行微调。同样使用上述网络，最后一层换成4096-&gt;21的全连接网络。 学习率0.001，网络各层参数不变。每一个batch包含32个正样本（属于20类）和96个背景（背景多于正样本是因为实际图片中背景部分就是比样本要多）。网络结构如下： 目标类别与分类器作者在cf7层提取出特征后，未直接通过最后一层softMax层进行分类，而是将cf7层提取出的特征用于训练SVM分类器。原因在于： svm训练和cnn训练过程的正负样本定义方式不同，softmax得到的结果比svm精度低。 cnn在训练的时候，对训练数据做了比较宽松的标注（例如bounding box只包含物体的一部分，我们也把它标注为正样本），采用这个方法的主要原因在于CNN容易过拟合，要扩大正样本的样本量，所以在CNN训练阶段我们是对Bounding box的位置限制条件限制的比较松(IOU只要大于0.5的region proposal都被标注为正样本) svm分类器原理是最小距离最大化，样本的定义越严格分类效果越好，所以对于训练样本数据的IOU要求比较严格（大于0.7为正样本） SVM训练对每一个类别训练一个二分类器，我们用IoU重叠阈值来解决正负样本的问题，在0.3阈值以下的区域被定义为负样本，0.3-0.7阈值的样本被忽略，0.7-1.0的样本被定义为正样本。（重叠阈值0.3是通过在验证集上尝试了0,0.1,…,0.5的不同阈值选择出来的。选择这个阈值是很重要，将很大程度上影响最后的结果。）正样本被简单地定义为每个类的检测框真值。我们提取了特征并应用了训练标签，就可以对每一个类别训练一个线性SVM，当我们用CNN提取2000个候选框，可以得到2000 * 4096这样的特征向量矩阵，然后我们只需要把这样的一个矩阵与svm权值矩阵4096 * N点乘(N为分类类别数目，因为我们训练的N个svm，每个svm包含了4096个权值w)，就可以得到结果。 边框回归学习一个线性回归器，用于bounding box的边框回归，输入为Alexnet pool5的输出。bbox回归认为候选区域和ground-truth之间是线性关系(因为在最后从SVM内确定出来的区域比较接近ground-truth,这里近似认为可以线性关系)。训练回归器的输入为N对值，${(P^i, G^i)}_{i=1,2,…,N}$，分别为候选区域的框坐标和真实的框坐标。这里选用的Proposal必须和Ground Truth的IoU大于0.6才算是正样本(避免一些远离groundtruth的边框参与计算)，通过学习四个变换函数，得到变换后的边框坐标。对每一类目标，使用一个线性脊回归器进行精修。正则项λ=10000。所谓脊回归，就是对于一个线性模型，在原来的损失函数加入参数的l2范数的惩罚项。当使用最小二乘法计算线性回归模型参数的时候，如果数据集合矩阵（也叫做设计矩阵(design matrix)）XX，存在多重共线性，那么最小二乘法对输入变量中的噪声非常的敏感，其解会极为不稳定。为了解决这个问题，就有了这一节脊回归（Ridge Regression ）。脊回归当矩阵存在多重共线性的时候（数学上称为病态矩阵），最小二乘法求得的参数W在数值上会非常的大，输入变量X有一个微小的变动，其反应在输出结果上也会变得非常大，因而结果对输入变量噪声非常敏感。 如果能限制参数W的增长，使W不会变得特别大，那么模型对输入W中噪声的敏感度就会降低。这就是脊回归和套索回归（Ridge Regression and Lasso Regrission）的基本思想。为了限制模型参数W的数值大小，就在模型原来的目标函数上加上一个惩罚项，这个过程叫做正则化（Regularization）。 如果惩罚项是参数的$l_2$范数，就是脊回归(Ridge Regression) 如果惩罚项是参数的$l_1$范数，就是套索回归（Lasso Regrission） 正则化同时也是防止过拟合有效的手段 非极大值抑制（NMS）：RCNN 网络会对一个目标标定了多个标定框，使用极大值抑制算法滤掉多余的标定框。NMS算法搜索局部的极大值，并且抑制那些分数低的窗口。首先对RCNN产生的边框分类概率从大到小排序，将最大概率边框设置为保留边框，并选择与该边框重合IoU大于某一个阈值的所有边框，将他们过滤，接下来从剩下的边框中重复上述步骤，直到所有边框都被处理过。 RCNN网络结构图：]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MAC 私人订制]]></title>
    <url>%2F2019%2F02%2F07%2FMAC-%E7%A7%81%E4%BA%BA%E8%AE%A2%E5%88%B6%2F</url>
    <content type="text"><![CDATA[配置gitMac上安装Xcode命令行工具，命令行工具包是一个小型独立包,可供下载独立于Xcode的和允许您执行命令行开发OS X:1xcode-select --install 设置用户名，邮箱：12git config --global user.name &quot;wenhui-zhou&quot;git config --global user.email &quot;765647930@qq.com&quot; 创建ssh-key：1ssh-keygen 在当前目录下找到/.ssh/id_rsa.pub，将其中的内容配置到GitHub账号中的ssh中完成配置。验证：1ssh -T git@github.com 若输出一下内容则说明配置成功。 Hi WenHui-Zhou! You’ve successfully authenticated, but GitHub does not provide shell access. 网络端口80端口：http端口，用于网页访问443端口：https访问端口，用于https的网页访问http与https是两种不同的协议，https协议安全xing Mac 系统环境配置Mac系统的环境变量，加载顺序为： /etc/profile /etc/paths ~/.bash_profile ~/.bash_login ~/.profile ~/.bashrc /etc/profile和/etc/paths是系统级别的，系统启动就会加载，后面几个是当前用户级的环境变量。后面3个按照从前往后的顺序读取，如果~/.bash_profile文件存在，则后面的几个文件就会被忽略不读了，如果~/.bash_profile文件不存在，才会以此类推读取后面的文件。~/.bashrc没有上述规则，它是bash shell打开的时候载入的。 windows上hexo博客迁移到Mac上的方法 安装node.js 安装git 安装hexo（使用npm安装） 新建博客文件夹，依次hexo init,sudo npm install 将原来文件夹中的文件替换Mac文件夹中的文件 博客恢复使用 安装anaconda后设置iterm的默认python版本打开iterm环境配置文件：vim ~/.zshrc在文件末尾添加指令：123456789101112131415161718192021#### Mac选择大段文字的方法由于使用触控板，抛弃了鼠标，但是选择大段文字则成了一个问题，还好有解决方案：选择段落：鼠标在段落内点击三下即选中选择大段篇幅：按住shift，鼠标在起始位置点击一下，在末尾点击一下即选中。#### 电池使用次数mac居然有点电池的充放电次数一说，以后使用电脑尽量插着插头。人事有代谢给我一个启发就是，万事万物都有尽头的一天，比如一个茶杯，身体，细胞等等，每天都在消耗，只是没人给你列一个上限而已。#### MAC 开启本地服务器MAC 开启本地的服务器，可以通过http的方式传递文件，具体做法如下：1. 打开终端，移动到需要分享文件的文件夹下；2. 在终端中输入：`python -m http.server 80`，开启web服务；3. 查询本机ip（百度输入本机IP即可），随后访问 `http://ip` 即可。4. 该方法下载文件夹：```shellwget -r -np -nH -R index.html http://include/file -r : 遍历所有子目录 -np : 不到上一层子目录去 -nH : 不要将文件保存到主机名文件夹 -R index.html : 不下载 index.html 文件]]></content>
      <categories>
        <category>Mac</category>
      </categories>
      <tags>
        <tag>tip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法-递归]]></title>
    <url>%2F2019%2F01%2F30%2F%E7%AE%97%E6%B3%95-%E9%80%92%E5%BD%92%2F</url>
    <content type="text"><![CDATA[递归递归(recursion)，是指函数的定义中使用函数自身的方法。用于表示用相似的方法重复事物的过程。 问题描述：从 1~n 这 n 个整数中随机选取任意多个，输出所有可能的选择方案。 输出格式：输入一个整数n。 输出格式：输出所有的方案。 数据范围：1 $\leq n \leq15$ 输入样例：13 输出样例：12345678322 311 31 21 2 3 方案一：主循环确定方案的长度，循环里头进一个dfs()，来控制填入的数。 1234567891011121314151617181920212223242526272829#include &lt;iostream&gt;using namespace std;int a[20];//记录序列int vis[20]; //记录是否访问过void dfs(int pos,int tar,int start)&#123; if(pos == tar+1)&#123; for(int i = 1;i&lt;=tar;i++)&#123; cout&lt;&lt;a[i]&lt;&lt;" "; &#125; cout&lt;&lt;endl; return; &#125; for(int i = start;i&lt;=n;i++)&#123; if(!vis[i])&#123; vis[i] = true; a[pos] = i; dfs(pos+1,tar,i+1); vis[i] = false; &#125; &#125;&#125;int main()&#123; cout &lt;&lt; endl; cin &gt;&gt; n; for(int i = 1;i&lt;= n;i++)&#123; dfs(1,i,1); &#125;&#125; 二进制优化：用二进制表示选了哪些书，用来代替之前使用的a[20]数组。| 或操作将i位置置为1（选中）： state |= 1&lt;&lt;(i-1)^ 异或操作将i位置还原为0（未选）： state ^= 1&lt;&lt;(i-1) 123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;using namespace std;bool vis[20];void dfs(int pos,int tar,int start,int state)&#123; if(pos == tar+1)&#123; for(i = 1;i&lt;=n;i++)&#123; if((state&gt;&gt;i)&amp;1) cout &lt;&lt; i&lt;&lt;&quot; &quot;; &#125; cout &lt;&lt;endl; return; &#125; for(int i = start;i&lt;=n;i++)&#123; if(!vis[i])&#123; vis[i] = true; state |= 1&lt;&lt;(i-1); dfs(pos+1,tar,i+1,state); state ^= 1&lt;&lt;(i-1); vis[i] = false; &#125; &#125;&#125;int main()&#123; cout&lt;&lt;endl; cin &gt;&gt; n; for(int i =1;i&lt;= n;i++)&#123; dfs(1,i,start = 1, 0); &#125; return 0;&#125; 状态压缩递归：用一个$2^n$的数的各个位上取0或取1来表示选中或未选中。 000 ： \n001 ： 1010 ： 2011 ： 3…… 123456789101112131415#include &lt;iostream&gt;using namespace std;int main()&#123; int n; cout &lt;&lt;endl; cin&gt;&gt; n; for(int state = 1;state&lt; 1&lt;&lt;n;state++)&#123; for(int j = 0;j&lt;n;j++)&#123; if(state&gt;&gt;j&amp;1) cout&lt;&lt;j+1&lt;&lt;&quot; &quot;; &#125; cout &lt;&lt;endl; &#125; return 0;&#125; 状态压缩的递归：1234567891011121314151617181920212223#include &lt;iostream&gt;using namespace std;int n;//u表示当前枚举到的数，state表示二进制的表示，记录哪些数字被选过void dfs(int u,int state)&#123; if(u == n)&#123; for(int i = 0;i&lt;=n;i++)&#123; if(state &gt;&gt; i &amp;1)&#123; cout&lt;&lt;i+1&lt;&lt;&quot; &quot;; &#125; &#125; cout&lt;&lt;endl; return; &#125; dfs(u+1,state); // 不用u这个数 dfs(u+1,state|(1&lt;&lt;u)); //用u这个数&#125;int main()&#123; cin &gt;&gt;n; dfs(0,0); return 0;&#125;]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[循环神经网络RNN,LSTM]]></title>
    <url>%2F2019%2F01%2F29%2F%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN-LSTM%2F</url>
    <content type="text"><![CDATA[RNNRNN适用背景当一段序列是连续的，且序列长度不一（音频序列），难以直接差分成一个个独立的样本来训练DNN/CNN，传统的神经网络无法用前面的场景来影响后面的预测。此时，可以使用RNN来解决这个问题。 循环神经网络内部具有循环边，允许信息持续存在。前一个节点传递消息给他的后继者。结构如下图：正是借助于这个链式的信息传导结构，RNN在处理序列相关的数据时，具有先天的优势。 RNN的缺点在需要利用的历史信息离当前节点较近时，RNN能够利用该信息去进一步学习。但是当需要的背景信息离当前的节点距离较远时，RNN无法学到这些信息，即RNN无法处理这种需要长连接的信息。 LSTM NetWork长短式记忆模型是一种特殊的RNN模型，能够解决长依赖无法学习的问题。所有循环神经网络均具有相同的模块链，在标准的RNN中，该重复的模块链是一个简单的tanh层。LSTM中的重复模块则由四个部分组成。 LSTM背后的思想LSTM关键是细胞的状态，表示细胞状态的这条水平线从图中顶部穿过。细胞在链上运行，其下有一些小的线性操作作用在它的上面。LSTM模型中具有很多门（gate）结构。他有一个sigmoid神经节点和一个点乘运算组成。sigmoid输出0到1之间的数字，表明这个组件可以有多少信息可以通过。LSTM中有三个门，用于控制细胞的状态。 一步步拆解LSTMLSTM第一步为决定从输入中丢弃什么信息，这一步称为遗忘门，遗忘门的输入为$h_{t-1}$（前一个细胞的输出）与$X_t$（当前细胞输入），通过一个sigmoid，输出0-1之间的数，添加到上一个细胞的状态$C_{t-1}$中。下一步决定细胞需要的存储信息。该部分由两步构成。sigmoid层决定了哪些值需要更新，接下来一个tanh层创建候选向量$C_t$，该向量将会被加入到细胞状态中。更新上一个状态值$C_{t-1}$，生成$C_{t}$最后决定我们要输出什么，此输出将局域我们的细胞状态，首先先运行一个sigmoid层，他决定我们要输出的细胞状态的哪些部分。随后将单元格通过tanh（将值规范化到-1到1之间），并乘以sigmoid 输出，得到最后的$h_t$部分。 总结 为什么具有记忆功能：由于存在递归结构，上一时刻的隐层的状态参与到了这个时刻的计算过程中，即每一步的选择和决策参考了上一次的状态。 为什么LSTM的记忆时间长（解决长连接问题）：由于传统的RNN在训练过程中引入一个激活函数，经过多步推导之后，这个乘子连乘，当参数发生轻微变化时，梯度将发生距离的波动，甚至将导致梯度消失问题。为了解决这个问题，特意设计了一个CEC常数误差流，即激活函数是线性的，将上一个节点的output由连乘改为连加。$|f_{ij}(x) W_{ij}| = 1,W_{ij}$是上一个状态与下一个状态的权值连接。误差没有衰减，使得序列很长之前带来的影响仍然能够保持到最后。LSTM在原来RNN的基础上是一个叫做CEC的部件，这个部件保证了误差将以常数的形式流动。同时添加输入门和输出门，使得模型变成非线性的。]]></content>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目总结]]></title>
    <url>%2F2019%2F01%2F25%2F%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[sketch2Cloth虚拟试衣总结虚拟试衣允许用户定制衣服的纹理，颜色。能够改善生成图像的真实感。虚拟试衣项目分成训练数据的处理以及GAN图片生成。 Human-parsing：输入一张人像图片，使用DeepLab+SSL框架对人体图像不同部位进行解析标记。从而根据不同分类的标记信息可以将图片分割成人体皮肤部分，服饰部分，首饰部分等等。根据不同的标注信息，仅保留含有人体皮肤，脑袋的图片；以及保留仅含有服饰的图片。随后对含有服饰的图片提取边缘信息，得到服饰的边缘纹理。最终将人体皮肤信息与服饰边缘信息相结合，得到最终的图片和标记，完成数据的预处理。这样处理的好处是，保留了绝大部分人体皮肤，头发等信息，利用GAN进行服饰样式生成时，仅需要生成服饰的颜色，纹理信息，尽可能保证图片的真实感。 DeepLab v2 将多孔卷积应用到密集预测任务上，有效扩大感受野。 采用多看空金字塔模型，使用不同采样率多尺度获取图像上下文信息。 将DCNN与完全连接条件场（CRFS）结合，增强物体边界定位。 SSL：引入自监督结构敏感学习方法进行训练，将人体姿态引入解析结果中，提升实验性能。 边缘检测 canny 算子：通过计算像素梯度幅值，方向，确定图片的边缘信息。使用费最大值抑制使得边缘更加清晰。比较看重像素的梯度变化，不看重整体的空间信息。 edge detection using structure forest：使用随机森林算法学习一个隐状态，将图形映射成边缘。 HED：整体嵌套边缘检测，将多次度的Edge进行融合，得到整体信息对边缘信息的反映。HED有vgg改造而来，可提取图片特征信息，多次度同和，反映了空间特征。 pix2pix在CGAN的基础上加上L1约束，作为图片的生成器。 GAN 生成对抗网络，同时训练一个生成器和判别器。优化生成器使其生成的东西更接近原始样本，优化判别器，使其能够更好地判断样本的真假。JS散度：度量两个概率分布的相似度，但是当两个分布距离很远时，将会导致梯度消失。因此引入wasserstein（earth-mover距离）：能够在联合分布的下，样本间的距离，当两个分布距离很远时，也可以提供梯度。 CGAN条件生成网络，GAN生成数据太过于自由，数据不可控。因此条件生成对抗网络，在生成器与判别器作用是加入一个条件概率，使得结果更符合实际条件。 pix2pix：在CGAN的基础上加入L1约束（模糊图片），使得生成图像更接近真实图。自动学习损失函数。使用U-net,使得上层图像获取更多的底层图像信息。 海量地震数据三维可视化 地震数据segy文件解析：segy文件为GB级别的数据，对多种格式的解析，里头包含了五种不同的数据存储格式，需要对地震数据进行解析以及筛选合适的数据这些操作。 地震数据预处理：使用SVD分解技术，留下数据中分量比较中的那部分数据 数据分块读取：按切片载入内存，设计颜色传递函数，使用shader将绘制部分迁移到GPU上执行。使用shader进行绘制。Shader上分为上色，]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AutoMatic Image Colorization 整理]]></title>
    <url>%2F2019%2F01%2F23%2FAutoMatic-Image-Colorization-%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[《Let there be Color: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simutaneous Classification》 是由三位知名的日本学者，发表在2016年的SIGGRAPH上，该模型实现的图片颜色恢复效果十分的好。原文地址：Let there be Color:项目地址：Automatic Image Colorization 摘要本文利用CNN提取图片全局先验信息(global priors)和局部图片特征信息(local image features)，并对特征进行融合，端对端(end to end)的对灰度图片进行自动上色。 图片语义信息：&emsp;视觉层： 即底层颜色，纹理，形状等等。&emsp;对象层： 属性特征，如某一对象某一时刻的状态&emsp;概念层： 最接近人类理解，如室内，室外，沙子，海水等 全局特征将反映：概念层信息，如室内室外，白天黑夜等等局部特征信息反映：局部材质，物体的位置信息等 色彩空间作者采用Lab颜色空间，L表示亮度，a，b表示颜色光谱绿-红和蓝-黄。Lab编码中有一个灰度层，颜色层变为两个，因此只需要预测两个通道。由图可以看出人们对亮度信息比较敏感。人眼中有94%的细胞由于探测亮度，6%的细胞用于探测颜色。因此我们将图片保存成灰度图即保留了图片大部分的信息，又节省空间。 网络结构网络结构大体由两部分组成。第一部分： low-level features 低特征提取，mid-level features 中特征提取，fusion layer 融合层，colorization network上色层组成。第二部分： low-level features 低特征提取，全局特征提取两部分组成。网络的输入为灰度图，第一部分输入是原图，由于第一部分只有卷积操作，因此对图片尺寸没有要求。第二部分输入是经过resize成224224大小的图片。包含全链接层，对输入大小有限制。*预测流程：将图片输入，经过低特征，中特征和全局特征的提取，一起来预测两个色彩图层即a和b，然后通过上采样，恢复到原图大小，与灰度图层L融合一起组成lab图片。 第一部分第一部分包含上色层，可以对图片进行上色，但是由于未加入全局的语义信息，所以效果不好。可以这样认为，根据全局特征得到的图片语义信息（室内或室外），利用全局语义信息进一步决定对图片的上色方案。 第二部分网络第二部分是一个标准的卷积神经网络，全局特征提取层输出是一个1*1*256的一个张量，通过融合层将语义信息加入第一部分网络中。整个网络的损失函数为：$$L(y^{color},y^{class}) = ||y^{color} - y^{color,*}||^2_{FRO} - \alpha (y_{l^{class}}^{color} - \log(\sum^{N}_{i = 0} exp(y_i^{class})))$$ 前半部分是一个预测颜色和真实颜色间的一个MSE Loss，后半部分是预测一个分类交叉熵loss。由于分类loss不影响上色，将$\alpha$设置为0，仅适用上色部分的loss。 融合层$$y_{u,v}^{fusion} = \sigma (b + W [y^{global},y^{mid}_{u,v}]^T)$$ 其中$y^{global}$是一个1*1*256的张量，b是一个$\frac{H}{8}*\frac{H}{8}*256$的一个长方体，将y与b头尾拼在一起，构成一个$\frac{H}{8}* \frac{H}{8}*512$的张量。 风格迁移将第二部分的输入换成一张其他风格的图片，图片类型要求相同，最终形成的图片的风格将发生改变。 网络特点 网络输入为多分辨率图片 网络中无池化层 上采样过程采用最近邻算法 所有的上色模型无法解决毛衣的上色问题，因为毛衣颜色不存在先验，是不确定的。如天空，海洋的颜色则是确定的。 PREFERENCE preference1]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模型性能评估指标概要]]></title>
    <url>%2F2019%2F01%2F23%2F%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%E6%A6%82%E8%A6%81%2F</url>
    <content type="text"><![CDATA[模型性能评价指标能够对模型预测结果性能好坏进行评价。以下列举了常用的模型评价指标。 AUC评价指标AUC（area under thr curve）指标常用来评估二分类模型的性能，指的是ROC曲线与x轴围成的面积。AUC不依赖于判决阀值。判别矩阵如下: 类型 正样本 负样本 预测为正 TP(真正例) FP(假正例) 预测为负 FN(假负例) TN(真负例) 随着阈值t的取值不同，有：真正率（正例预测为真/所有正样本）：$$TPR = \frac{TP}{TP+FN}$$假正率（负例预测为假/所有负样本）：$$FPR = \frac{FP}{FP+TN}$$因此TPR与FPR是关于t的一个函数： AUC即为如下曲线下的面积：$$AUC = \int_{t = 0}^{1} y(t) dx(t)$$AUC实际表现为把正样本排在负样本前面的概率。同时AUC对政府样本的比例不敏感。AUC越大表明模型区分正例和负例的能力越强，AUC常常依赖于具体的任务。 精确率：$$Pricision = \frac{TP}{TP+FP}$$精确度pricision指的是我判断为真的里头，确实为真的概率。 召回率：$$Recall = \frac{TP}{TP+FN}$$召回率recall指的是我判断是真的里头，确实为真的占样本所有为真的概率。 F1 score:$$\frac{1}{F_{1}} = \frac{1}{Precision}+\frac{1}{Recall}$$F1是precision和recall的调和均值， F1 score作为正负样本不均衡的评价方式.参考链接 mAP:mAP指mean average precision，即各个类别AP的平均值。 AP：指precision与recall曲线的下面部分。 对于IoU = 0.5:0.05:0.95分别计算mAP，随后平均得到最后的mAP：指的是将IoU从0.5一直递增到0.95，然后每一个IoU均计算一个AP值，然后对所有的AP值取平均，得到最终的mAP。 BenchMark，SOTA 与Baseline一个算法的benchmark指的是，它的性能已经被广泛研究，人们对它性能的表现形式、测量方法都非常熟悉，因此可以作为标准方法来衡量其他方法的好坏。state-of-the-art（SOTA）：能够称为SOTA的算法表明其性能在当前属于最佳性能。如果一个新算法但如果比不过SOTA，能比benchmark要好，且方法有一定创新，也是可以发表的。 baseline：表示比这个算法性能还差的基本上不能接受的，除非方法上有革命性的创新点，而且还有巨大的改进空间和超越benchmark的潜力，只是因为是发展初期而性能有限。所以baseline有一个自带的含义就是“性能起点”。 总结一下就是：benchmark是属于较好的水准，baseline则代表了及格线。]]></content>
      <categories>
        <category>模型评价</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[human head detect summary]]></title>
    <url>%2F2019%2F01%2F23%2Fhuman%20head%20detect%20summary%2F</url>
    <content type="text"><![CDATA[welcome to my blog,enter password to read. Incorrect Password! No content to display! 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]]></content>
      <categories>
        <category>比赛</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ 刷题常用数据及函数的语法记录]]></title>
    <url>%2F2019%2F01%2F21%2FC-%E5%88%B7%E9%A2%98%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E5%8F%8A%E5%87%BD%E6%95%B0%E7%9A%84%E8%AF%AD%E6%B3%95%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[字符串操作： int to string： &emsp;string a = to_string(int) string to int：&emsp; int a = stoi(string) char to int： &emsp; int a = char_b - 48 字符串中查找字符： 123456string a = "abcd";string b = "ab";int start = 0;int pos = a.find(b,start); // start可省略，默认从0开始//如果查找不存在返回string::nposif(a.find(b) == string::npos) cout&lt;&lt;"dont exist"; 截取子串： &emsp; string a = astring.substr(startpos,length) vector操作： vector 删除： vector_a.erase(iter_pos),vector_a.erase(iter_begin(),iter_end()) vector排序：匿名函数的形式 nums.sort(nums.begin(),nums.end(),[](int a,int b){return a&gt;b;}) unordered_map操作: unordered_map实现使用了哈希表，可以在$O(1)$时间复杂度访问到对应元素，缺点为要花费较高的空间复杂度。 map实现使用的对应结构为红黑树（类似平衡树），查找元素使用的复杂度为$O(\log n)$。 unordered_map声明： &emsp;unordered_map&lt;char,int&gt; map; unordered_map插入键值对： &emsp;map[&#39;a&#39;] = 1;,map.insert(make_pair(&#39;a&#39;,1)); unordered_map查找元素： &emsp; if(map.find(&#39;B&#39;) == map.end()){dont exist}， &emsp; if(map.count(&#39;B&#39;) == 0){dont exist} unordered_map移除元素： &emsp;map.erase(map.begin()), &emsp;map.erase(map.begin(),map.end()), &emsp;map.erase(&#39;A&#39;) 中值的取法： 防止整数溢出：int mid = left + (right-left)/2; 大数组开成全局变量： int weight[N][M]; 原因是计算机会将把虚拟内存空间分配给程序。虚拟内存空间分为栈空间和堆空间。所有开在函数内部的变量会开在栈里，所有开在静态变量，全局变量会开在堆里。C++默认栈空间大小为4M，所以一般将大数组开到全局变量中去。 异或的作用（^）： 用异或实现配偶：0^1=1，1^1=0 lowbit运算：给一个n快速找到二进制中最低的一个1，lowbit(100100) = 100 -&gt;树状数组的基本操作。 123int lowbit(int n)&#123; return (~n + 1) &amp; n; // return (-n)^n; 补码就是负数&#125; 位运算与底层的电路实现有关，无论什么操作都只用O(1)时间。 STL中的全排列操作：12while(next_permutation(A.begin(),A.end()))&#123; ... &#125; //从小到大产生排列组合，当排列组合全部产生结束时返回falseprev_permutation(A.begin(),A.end()); // 从大到小生成排列数，直接改变vector里头的值 sprintf() C 库函数 int sprintf(char str, const char format, …) 发送格式化输出到 str 所指向的字符串 12sprintf(str, &quot;Pi 的值 = %f&quot;, M_PI); // str = &quot;Pi 的值 = 3.141593&quot;sprintf(str,&quot;%02d:%02d&quot;,h,m); // %02d 指的是整数h的宽度为2，如果不够的话前面补0.(3-&gt;03) set用法： set是一个内部元素唯一的集合，定义：set&lt;vector&lt;int&gt;&gt; res;1234for(auto iter = res.begin();iter!=res.end();iter++) ...res.clear(); //删除所有的元素res.empty(); //判断是否为空集合res.rbegin() == res.end(); vector的用法： 初始化：vector&lt;int&gt; vec(size,0); 添加元素：vec.push_back(val);vec.insert(vec.begin(),val); 删除元素：vec.pop_back();vec.erase(vec.begin()) `vec.erase(vec.begin(), vec.begin()+3);` 查找：find(vec.begin(),vec.end(),val) != vec.end() 排序: 123456sort(vec.begin(),vec.end()); bool myfun(int a,int b)&#123;return a&lt;b; // 生序&#125;sort(vec.begin(),vec.end(),myfun); sort(vec.begin(),vec.end(),[](int a,int b)&#123;return a&lt;b;&#125;) lambda 表达式：1auto func = [c](int a,int b) &#123; return a &lt; b; &#125;; 其中c为表达式外边的变量，a,b为传入表达式的变量。 string 中find函数int pos = str.find(char,int begin = 0,int end = str.size()) //if(pos == string::npos) cant find it else return the index of char string 中的substrstring str = s.substr(begin,num)//表示从begin开始，共num个数 string str = s.substr(begin)//表示从begin开始到最后]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github 建站]]></title>
    <url>%2F2019%2F01%2F19%2FGithub-%E5%BB%BA%E7%AB%99%2F</url>
    <content type="text"><![CDATA[github上搭建一个博客网站（windows） 1. 前期准备 node.js: 2009年由Ryan推出的，基于javascript（负责解释并执行代码）与google 浏览器V8引擎（c++编写的一个超快的解释器）的一个后端服务器应用程序。旨在增大服务器并发连接的数量（非阻塞，时间驱动I/O）。 git:开源分布式版本控制系统。见链接 hexo:一个快速简洁的博客框架。见链接，hexo支持makdown，是一个生成静态网页，并将网页上传到服务器上的工具。 2. Github上创建一个registry Github上新建项目，项目必须要遵守格式：账户名.github.io，同时勾选Initialize this repository with a README。（eg：WenHuiZhou.github.io） 3. 下载安装node.js，以及git4. 安装hexo 命令行内输入指令：npm install -g hexo-cli&emsp;&emsp;npm (node package manager)：运行在node.js上的一个使用javascript写的类库管理器，npm内置于node.js中，作为node.js的包管理器。可以使用npm来查找安装一些库(nmp install jquery.js)。&emsp;&emsp;有时使用npm进行下载文件时经常出现网络上的问题，此时可以对npm换源。npm config set registry https://registry.npm.taobao.org 创建文件夹，作为hexo博客文件存储文件夹，输入指令。 123hexo init&lt;blog&gt;cd &lt;blog&gt;npm install 完成创建后hexo将生成如下文件： 正常使用中修改最多的文件夹为_config.yml，其中包括博客的基础配置以及模板信息。source为写文章所需的目录，如果要针对下载的模板修改，那么需要修改themes模板目录。 启动hexo：hexo g：hexo生成网页（generate），hexo s：hexo启动服务器server。 5 . hexo 连接 github 打开git bash，进入blog文件夹 配置用户名，以及邮箱输入：git config --global user.name WenHuiZhougit config --global user.email myemail 每次使用git进行commit时都需要用到用户名和邮箱记录。用于指定push到的github。 6. SSH密钥登陆： 利用密钥生成器制作一对密钥——一只公钥和一只私钥。将公钥添加到服务器的某个账户上，然后在客户端利用私钥即可完成认证并登录。 生成密钥对：ssh -keygen -t rsa -C &quot;myemail.com&quot;，将生成id_rsa 和 id_rsa.pub两个文件。 添加密钥对到ssh-agent：eval &quot;$(ssh-agent -s)&quot; 添加生成的SSH key到ssh-agent：ssh-add ~/.ssh/id_rsa 7 .设置github的ssh密匙 打开github setting，将添加ssh key，将id_rsa.pub内容复制进去即可。 在git bash上输入ssh -T git@github.com 此时返回 hi WenHuiZhou表明配置成功。 8 . 配置_config.yml文件 在_config.yml文件最后添加：deploy: type:git repository:git@github.com:WenHui-Zhou/WenHuiZhou.github.io.git branch: master repository地址可以从github上download那得到。 9. 在hexo上写博客 hexo new post &quot;blog name&quot;，hexo将会在source文件夹中生成.md文件，编辑.md文件写博客。 hexo s: 进入本地博客地址观察效果 hexo d -g: 将博客上传至github上 输入github上的访问地址：https://wenhui-zhou.github.io/即可博客网站。 10. 总结 使用hexo和github搭建了一个博客 使用hexo模板 maupassant对博客进行美化，见链接 该模板还需要做大量的个人定制工作，这是接下来要做的。 PREFERENCE reference1 reference2]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>netStation</tag>
      </tags>
  </entry>
</search>
