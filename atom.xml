<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>WenHuiZhou</title>
  
  <subtitle>perper（打起精神！）</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://wenhui-zhou.github.io/"/>
  <updated>2020-02-16T16:10:05.733Z</updated>
  <id>https://wenhui-zhou.github.io/</id>
  
  <author>
    <name>WenHuiZhou</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>线性判别函数</title>
    <link href="https://wenhui-zhou.github.io/2020/02/16/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0/"/>
    <id>https://wenhui-zhou.github.io/2020/02/16/线性判别函数/</id>
    <published>2020-02-16T13:53:23.000Z</published>
    <updated>2020-02-16T16:10:05.733Z</updated>
    
    <content type="html"><![CDATA[<p><strong>线性判别函数</strong>：用于分类的判别函数的参数形式已知，通过从样本来估计判别函数的参数。</p><a id="more"></a><h3 id="模式分类的途径"><a href="#模式分类的途径" class="headerlink" title="模式分类的途径"></a>模式分类的途径</h3><ul><li>估计类条件概率密度函数，然后通过贝叶斯得到后验概率，用于决策</li><li>直接估计后验概率：k-近邻分类器等</li><li>使用判别函数，直接决策</li></ul><h3 id="线性判别函数与决策面"><a href="#线性判别函数与决策面" class="headerlink" title="线性判别函数与决策面"></a>线性判别函数与决策面</h3><p><img src="/images/nlp/image-20200216222218691.png" alt="image-20200216222218691" style="zoom:40%;"></p><p><img src="/images/nlp/image-20200216222522730.png" alt="image-20200216222522730" style="zoom:40%;"></p><p><strong>多类情况-判别器</strong></p><ul><li>one-vs-all：构造C的二分类器，逐一比较</li><li>one-vs-one：两两配对，构造c/2个二分类器</li></ul><p><strong>增广性</strong></p><p>对线性判别函数采用其次增广表示，使得决策平面过原点，具有一些很好地分类性质。</p><p><img src="/images/nlp/image-20200216230032056.png" alt="image-20200216230032056" style="zoom:40%;"></p><h3 id="感知准则函数"><a href="#感知准则函数" class="headerlink" title="感知准则函数"></a>感知准则函数</h3><p><img src="/images/nlp/image-20200216230427121.png" alt="image-20200216230427121" style="zoom:40%;"></p><p><img src="/images/nlp/image-20200216231014083.png" alt="image-20200216231014083" style="zoom:40%;"></p><p>越靠近区域中间的解向量，越能对新的样本正确分类，可以对解区进行限制：</p><p><img src="/images/nlp/image-20200216233948027.png" alt="image-20200216233948027" style="zoom:40%;"></p><p>感知器准则：最小化错分样本</p><p><img src="/images/nlp/image-20200216234659412.png" alt="image-20200216234659412" style="zoom:40%;"></p><p><img src="/images/nlp/image-20200216234827162.png" alt="image-20200216234827162" style="zoom:40%;"></p><p><img src="/images/nlp/image-20200216235836881.png" alt="image-20200216235836881" style="zoom:40%;"></p><p><img src="/images/nlp/image-20200217000045647.png" alt="image-20200217000045647" style="zoom:40%;"></p><p><img src="/images/nlp/image-20200217000108761.png" alt="image-20200217000108761" style="zoom:40%;"></p><p><img src="/images/nlp/image-20200217000130566.png" alt="image-20200217000130566" style="zoom:40%;"></p><p><img src="/images/nlp/image-20200217000842488.png" alt="image-20200217000842488" style="zoom:40%;"></p><p><img src="/images/nlp/image-20200217000608368.png" alt="image-20200217000608368" style="zoom:40%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;线性判别函数&lt;/strong&gt;：用于分类的判别函数的参数形式已知，通过从样本来估计判别函数的参数。&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习方法" scheme="https://wenhui-zhou.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>Learn To Rank</title>
    <link href="https://wenhui-zhou.github.io/2020/02/14/Learn-To-Rank/"/>
    <id>https://wenhui-zhou.github.io/2020/02/14/Learn-To-Rank/</id>
    <published>2020-02-14T13:53:54.000Z</published>
    <updated>2020-02-16T10:57:18.734Z</updated>
    
    <content type="html"><![CDATA[<p>Learn To Rank是一种学习方法，通过训练模型来解决排序的问题，在信息检索，NLP，Data Mining领域有着很多的应用。</p><a id="more"></a><h3 id="排序问题"><a href="#排序问题" class="headerlink" title="排序问题"></a>排序问题</h3><p>在信息检索中，给定一个query，搜索引擎会召回（<strong>粗筛选</strong>）一系列相关的Documents（term匹配，keyword匹配，semantic匹配），之后对这些Documents排序，最后输出Top N的Documents。</p><p>排序问题即使用一个模型 f(q,d)来对该query下的documents进行排序，这个模型可以是人工设定一些参数的模型，也可以是用<strong>机器学习算法自动训练出来的模型</strong>。在Web Search领域，因为在Web Search 中，有很多信息可以用来<strong>确定query-doc pair的相关性</strong>，而另一方面，由于大量的搜索日志的存在，可以将<strong>用户的点击行为日志作为training data</strong>，使得通过机器学习自动得到排序模型成为可能。</p><p><strong>排序问题的核心在于找出query和doc之间的相关性，对相关性进行排序。</strong></p><p>learn to rank 是监督学习，分为train和test两个阶段：</p><p><img src="/images/nlp/image-20200214232441639.png" alt="image-20200214232441639"></p><h3 id="training-data-set-的生成"><a href="#training-data-set-的生成" class="headerlink" title="training data set 的生成"></a>training data set 的生成</h3><p>由于learning to rank是监督学习，因此对每一条记录都需要label。通常feature vector容易获取，而label实际上反映了query-doc pair的真实相关程度。通常有两种label的获取方式：</p><ul><li>人工标注，即对抽样出来作为training data的query-doc pair人为地进行相关程度的判断和标注，一般标注的相关程度分为5档：perfect，excellent，good，fair，bad。<ul><li>例如，query=“Microsoft”。document为Microsoft的官网是perfect；介绍Microsoft的wikipedia则是excellent；一篇将Microsoft作为其主要话题的网页则是good；一篇只是提到了Microsoft这个词的网页则是fair，而一篇跟Microsoft毫不相关的网页则是bad。人工标注的方法可以通过多人同时进行，最后以类似投票表决的方式决定一个query-doc pair的相关程度，这样可以相对减少各个人的观点不同带来的误差。</li></ul></li><li>通过搜索日志获取。搜索日志记录了人们在实际生活中的搜索行为和相应的点击行为，点击行为隐含了query-doc pair的相关性，所以可以被用来作为query-doc pair的相关程度的判断。一种最简单的方法就是利用同一个query下，不同doc的点击数的多少来作为它们相关程度的大小。</li></ul><p>通过搜索日志的方式获取的方法存在一些偏差，即用户偏向于点击位置靠前的doc，即便这个doc并不相关或者相关性不高。因此有一些tricky和general的方法用来处理这种“position bias”的偏差：</p><ol><li>当位置靠后的doc的点击数都比位置靠前的doc的点击数要高了，那么靠后的doc的相关性肯定要比靠前的doc的相关性大。</li><li>Joachims等人则提出了一系列去除bias的方法，例如 Click &gt; Skip Above, Last Click &gt; Skip Above, Click &gt; Earlier Click, Click &gt; Skip Previous, Click &gt; No Click Next等。</li><li>一个doc的点击数比另一个doc的点击数多，并不一定说明前者比后者更相关。但即使前者比后者位置靠前，两者的点击数相差5-10倍，这时候我们还是愿意相信前者更加相关。</li><li>click model，根据用户的点击信息对用户真正看到的doc进行“筛选”，进而能更准确地看出用户到底看到了哪些doc，没有看到哪些doc，更准确反应点击数/展示数（即展现CTR）来确定各个doc的相关性大小。</li></ol><h3 id="feature-的生成"><a href="#feature-的生成" class="headerlink" title="feature 的生成"></a>feature 的生成</h3><p>一般Learning to Rank的模型的feature分为两大类：relevance 和 importance（hotness），即query-doc pair 的相关性feature，和doc本身的热门程度的feature。两者中具有代表性的分别是 BM25 和 PageRank。</p><h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p>比较模型的输出结果，和真实结果（ground truth）之间的差异大小。用于Information Retrieval的排序衡量指标通常有：NDCG，MAP等。</p><p><strong>NDCG</strong></p><p>NDCG表示了从第1位doc到第k位doc的“归一化累积折扣信息增益值”，主要思想是相关性高且等级高的结果，值应该比较高。</p><ul><li>高关联度的结果比一般关联度的结果更影响最终的指标得分</li><li>有高关联度的结果出现在更靠前的位置的时候，指标会越高</li></ul><p><strong>CG：累计增益cumulative gain</strong></p><p>只考虑到了相关性的关联程度，没有考虑到位置的因素，是一个搜素结果相关性分数的总和。<br>$$<br>\mathrm{CG}_{\mathrm{p}}=\sum_{i=1}^{p} r e l_{i}<br>$$<br>rel表示i的相关性。</p><p><strong>折损累计增益（DCG）</strong>：</p><p>即对每一个CG的结果，除以一个折算值，目的是为了排名越靠前的结果影响力越大：<br>$$<br>\mathrm{DCG}_{\mathrm{p}}=\sum_{i=1}^{p} \frac{r e l_{i}}{\log _{2}(i+1)}=r e l_{1}+\sum_{i=2}^{p} \frac{r e l_{i}}{\log _{2}(i+1)}<br>$$<br><strong>归一化折损累计增益（NDCG）</strong></p><p>归一化之后的折损累积增益，由于搜索的词不同，因此返回的检索数量是不同的，DCG是一个累加值，因此没法针对两个不同的结果进行比较，需要求一个归一化之后的结果：<br>$$<br>\mathrm{nDCG}_{\mathrm{p}}=\frac{D C G_{p}}{I D C G_{p}}<br>$$<br>其中IDCG是理想情况下最大的DCG：<br>$$<br>\mathrm{IDCG}_{\mathrm{p}}=\sum_{i=1}^{|R E L|} \frac{2^{r e l_{i}}-1}{\log _{2}(i+1)}<br>$$<br>即从大到小，取前p个结果组成的集合。</p><p><strong>MAP</strong></p><p>对每个相关文档检索出准确率平均值的算术平均值。首先对每一个query计算一个AP：<br>$$<br>A P=\frac{\sum_{j=1}^{n_{i}} P(j) \cdot y_{i, j}}{\sum_{j=1}^{n_{i}} y_{i, j}}<br>$$<br>$y_{ij}$即每个doc的label（1和0），而每个query-doc pair的P值代表了到$d_{ij}$这个doc所在的位置为止的precision：<br>$$<br>P(j)=\frac{\sum_{k: \pi_{i}(k) \leq \pi_{i}(j)} y_{i, k}}{\pi_{i}(j)}<br>$$<br>其中pi表示排序中的位置。</p><h3 id="Formulation"><a href="#Formulation" class="headerlink" title="Formulation"></a>Formulation</h3><p>通过建立一个损失函数，即经验风险函数，通过最小化这个函数来达到模型训练的目的：<br>$$<br>\hat{R}(F)=\frac{1}{m} \sum_{i=1}^{m} L\left(F\left(\mathbf{x}_{i}\right), \mathbf{y}_{i}\right)<br>$$<br>由于上式优化函数不连续，因此我们寻求一个替代函数，通过优化次优函数得到次优解，替代方案有许多种，可以选择的方法有：</p><p><strong>pointwise loss</strong></p><p>平方误差：<br>$$<br>L^{\prime}(F(\mathbf{x}), \mathbf{y})=\sum_{i=1}^{n}\left(f\left(x_{i}\right)-y_{i}\right)^{2}<br>$$<br><strong>pairwise loss</strong></p><p>例如hinge loss，exponential loss，logistic loss等<br>$$<br>L^{\prime}(F(\mathbf{x}), \mathbf{y})=\sum_{i=1}^{n-1} \sum_{j=i+1}^{n} \phi\left(\operatorname{sign}\left(y_{i}-y_{j}\right), f\left(x_{i}\right)-f\left(x_{j}\right)\right)<br>$$<br><strong>listwise loss</strong><br>$$<br>L^{\prime}(F(\mathbf{x}), \mathbf{y})=\exp (-N D C G)<br>$$</p><h3 id="Learn-To-Rank"><a href="#Learn-To-Rank" class="headerlink" title="Learn To Rank"></a>Learn To Rank</h3><p><strong>pointwise：</strong>输入数据为单个doc以及query</p><p><strong>pairwise：</strong>输入数据为同一个query对应的两个doc，以及query</p><p><strong>listwise：</strong>输入数据为同一个query对应的若干doc，以及query</p><p>pointwise和pairwise方法将排序问题转化为classification，regression，ordinal classification等问题。而listwise方法则将一个ranking list作为一个instance来进行训练，其实会考虑每个query下所有doc之间的顺序关系。</p><p>这三种类型的Learning to Rank方法的具体算法一般有：</p><p>1) <strong>Pointwise</strong>: Subset Ranking, McRank, Prank, OC SVM</p><p>2) <strong>Pairwise</strong>: Ranking SVM, RankBoost, RankNet, GBRank, IR SVM, Lambda Rank, LambdaMart</p><p>3) <strong>Listwise</strong>: ListNet, ListMLE, AdaRank, SVM MAP, Soft Rank</p><p><strong>inference</strong></p><p>本文为学习笔记，参考如下网页：<a href="https://www.cnblogs.com/bentuwuying/p/6681943.html" target="_blank" rel="noopener">https://www.cnblogs.com/bentuwuying/p/6681943.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Learn To Rank是一种学习方法，通过训练模型来解决排序的问题，在信息检索，NLP，Data Mining领域有着很多的应用。&lt;/p&gt;
    
    </summary>
    
      <category term="webSearch" scheme="https://wenhui-zhou.github.io/categories/webSearch/"/>
    
    
  </entry>
  
  <entry>
    <title>OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</title>
    <link href="https://wenhui-zhou.github.io/2020/02/05/OpenPose-Realtime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/"/>
    <id>https://wenhui-zhou.github.io/2020/02/05/OpenPose-Realtime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/</id>
    <published>2020-02-05T08:27:15.000Z</published>
    <updated>2020-02-16T06:36:22.947Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章是2017年发表在CVPR上，作者开源了代码openpose，openpose代码完整，在推动人体识别，起到了巨大的作用。</p><a id="more"></a><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>人体姿态估计</strong>是理解视频图像的一个核心问题，一种比较简单的方法是two-step的框架，首先检测出图像中的人，然后解决人体姿态估计的问题。这种方法的问题在于，算法执行时间与图片中的人数呈正比，同时若第一步未检出行人，将会出现比较严重的漏检。</p><p>这篇文章提出一种<strong>bottom-up approach</strong>，这种方法检测出人体的部分，然后parsing成人体姿态的结果。这种方法执行时间与人数无关。</p><h3 id="architecture-网络框架"><a href="#architecture-网络框架" class="headerlink" title="architecture 网络框架"></a>architecture 网络框架</h3><p><img src="/images/nlp/image-20200205200758114.png" alt="image-20200205200758114" style="zoom:50%;"></p><ul><li>首先通过baseline10层的VGG-19网络，生成feature map</li><li>将feature map 分为两路，通过多层的CNN生成：<ul><li><strong>a set of Part Confidence Maps</strong> 身体部位的置信度</li><li><strong>a set of Part Affinity Fields (PAFs)</strong> 部位亲和场</li></ul></li><li><strong>Part Confidence Maps</strong>: a set of 2D confidence maps <strong>S</strong> for body part locations. Each joint location has a map.也就是说，每一个节点都对应了一张map，如果有25个节点的时候，就会有25张map。</li><li><strong>Part Affinity Fields (PAFs)</strong>: a set of 2D vector fields <strong>L</strong> which encodes the degree of association between parts.  生成parts之间的单位向量场。</li><li>Finally, the <strong>Confidence Maps</strong> and <strong>Part Affinity Fields</strong> are processed by a greedy algorithm to obtain the poses for each person in the image 联合置信图以及亲合场得出图像中每一个人的pose</li></ul><h3 id="confidence-maps"><a href="#confidence-maps" class="headerlink" title="confidence maps"></a>confidence maps</h3><p>置信图指的是一张2D表示的置信度图，可以定位到图像中关节点的像素上。</p><p>令$J$作为人体的的关节点总数，confidence map如下：<br>$$<br>\text { the set } S=\left(S_{1}, S_{2}, \ldots, S_{J}\right) \text { where } S_{j} \in R^{w \times h}, j \in 1 \ldots J<br>$$<br>总的来说，每一张map都对应一个节点，并且与输入的图片有着相同的size。</p><h3 id="Part-Affinity-Fields（PAFs）"><a href="#Part-Affinity-Fields（PAFs）" class="headerlink" title="Part Affinity Fields（PAFs）"></a>Part Affinity Fields（PAFs）</h3><p>PAF指的是流向量场，流向量场用来编码第一部分成对的关节点对，例如nose，neck，elbow等等：<br>$$<br>\text { the set } L=\left(L_{1}, L_{2}, \ldots, L_{C}\right) \text { where } L_{c} \in R^{w \times h \times 2}, c \in 1 \ldots C<br>$$<br>如果一个点在他的body part上（腿），那么这个点的值是一个2D单位向量，从起点joint指向终点的joint。</p><p><img src="/images/nlp/image-20200206214339000.png" alt="image-20200206214339000" style="zoom:40%;"></p><p>如上所述，整个流程分成两个步骤，第一个阶段生成PAF流向量场，第二个向量生成关节点密度场。</p><h3 id="Multi-Person-Parsing-using-PAFs"><a href="#Multi-Person-Parsing-using-PAFs" class="headerlink" title="Multi-Person Parsing using PAFs"></a>Multi-Person Parsing using PAFs</h3><p>从图像中找出人体姿态的步骤如下：</p><ul><li><strong>Step 1</strong>: Find <strong>all joints</strong> locations using the <strong>confidence maps</strong>.</li><li><strong>Step 2</strong>: Find which joints go together to <strong>form limbs (body parts)</strong> using the <strong>part affinity fields</strong> and joints in step 1.</li><li><strong>Step 3</strong>: <strong>Associate limbs that belong to the same person</strong> and get the final list of human poses.</li></ul><h3 id="如何生成人体limb"><a href="#如何生成人体limb" class="headerlink" title="如何生成人体limb"></a>如何生成人体limb</h3><ul><li>首先将生成的PAFs放大到输入的尺寸，</li><li>对于每一个limb类型，例如对wrist_elbow：<ul><li>从深度图中拿到所有的wrist和elbow的关节点的位置</li><li>对每一个起点peak和终点peak：<ul><li>用终点减去起点，然后归一化之后得到单位方向向量</li><li>每一对起点和终点之间的点，计算他们的PAFs的值</li><li>通过这中间所有点的PAFs值得平均值，计算当前limb connection的score</li><li>添加一个score来惩罚long distance：<code>min(0.5*paf_height/limb_dist - 1,0)</code></li><li>添加当前的limb连接到limb connecton candidate中</li></ul></li><li>对limb connection candidate进行排序</li><li>对于每一个候选连接，如果source和destination未被选中，则添加这个connection到最终的list当中</li></ul></li></ul><h3 id="pytorch-implementaton"><a href="#pytorch-implementaton" class="headerlink" title="pytorch implementaton"></a>pytorch implementaton</h3><p>giithub链接：https:/github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation</p><h3 id="preference"><a href="#preference" class="headerlink" title="preference"></a>preference</h3><ul><li><a href="https://blog.csdn.net/qq_14845119/article/details/98192997" target="_blank" rel="noopener">https:/towardsdatascience.com/cvpr-2017-openpose-realtime-multi-person-2d-pose-estimation-using-part-affinity-fields-f2ce18d720e8</a></li><li><a href="https://blog.csdn.net/qq_14845119/article/details/98192997" target="_blank" rel="noopener">https://blog.csdn.net/qq_14845119/article/details/98192997</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章是2017年发表在CVPR上，作者开源了代码openpose，openpose代码完整，在推动人体识别，起到了巨大的作用。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://wenhui-zhou.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯法(4)</title>
    <link href="https://wenhui-zhou.github.io/2020/01/30/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95-4/"/>
    <id>https://wenhui-zhou.github.io/2020/01/30/朴素贝叶斯法-4/</id>
    <published>2020-01-30T05:24:41.000Z</published>
    <updated>2020-02-02T07:34:23.207Z</updated>
    
    <content type="html"><![CDATA[<p>《统计学习方法》第4章 朴素贝叶斯法</p><p>朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法，对于给定的数据集，首先基于特征条件独立假设学习输入、输出的联合概率分布。对于给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。<br>$$<br>P\left(\omega_{i} | \mathbf{x}\right)=\frac{p\left(\mathbf{x} | \omega_{i}\right) P\left(\omega_{i}\right)}{p(\mathbf{x})}=\frac{p\left(\mathbf{x} | \omega_{i}\right) P\left(\omega_{i}\right)}{\sum_{i=1}^{c} p\left(\mathbf{x} | \omega_{j}\right) P\left(\omega_{j}\right)}<br>$$<br>将后验问题转化为先验的形式。</p><a id="more"></a><h3 id="贝叶斯用于模式分类"><a href="#贝叶斯用于模式分类" class="headerlink" title="贝叶斯用于模式分类"></a>贝叶斯用于模式分类</h3><p><img src="/images/nlp/image-20200130155020167.png" alt="image-20200130155020167" style="zoom:50%;"></p><p><strong>通过上述分类器设计的方法，拼凑出贝叶斯公式的右半部分，贝叶斯决策的关键在于得到一个准确的类先验概率</strong>。</p><h3 id="类先验的概率密度估计"><a href="#类先验的概率密度估计" class="headerlink" title="类先验的概率密度估计"></a>类先验的概率密度估计</h3><p>概率密度估计也是一个非常重要的研究方向，通常有三种方法：</p><ul><li>参数法：假定类先验的概率密度已知，如高斯分布</li><li>非参数法：parzen窗，k-nn等</li><li>半参数法：混合高斯分布（GM），期望最大化（EM）</li></ul><p><strong>高斯分布的一些知识点</strong></p><p>一维高斯分布：<br>$$<br>p(x)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left[-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}\right]<br>$$<br>混合高斯分布如下：</p><p><img src="/images/nlp/image-20200130161716499.png" alt="image-20200130161716499" style="zoom:50%;"></p><p><img src="/images/nlp/image-20200130162547374.png" alt="image-20200130162547374" style="zoom:50%;"></p><p>当数据分布符合高斯分布的时候，可以认为概率密度函数符合高斯分布：</p><p><img src="/images/nlp/image-20200130163925001.png" alt="image-20200130163925001" style="zoom:50%;"></p><p>从而得到类的后验概率的估计。</p><h3 id="离散变量贝叶斯决策"><a href="#离散变量贝叶斯决策" class="headerlink" title="离散变量贝叶斯决策"></a>离散变量贝叶斯决策</h3><p>此类问题例如问卷调查，医疗诊断等问题。</p><p><strong>独立二值特征</strong></p><p><img src="/images/nlp/image-20200131200632117.png" alt="image-20200131200632117" style="zoom:50%;"></p><p>二值特征的特点是两个特征之间相互独立，因此概率值为相乘：<br>$$<br>p^{x_i}_i(1-q_i)^{1-x_i}<br>$$<br><strong>复合模式分类</strong></p><p><img src="/images/nlp/image-20200131203920471.png" alt="image-20200131203920471" style="zoom:50%;"></p><h3 id="最大似然和贝叶斯参数估计"><a href="#最大似然和贝叶斯参数估计" class="headerlink" title="最大似然和贝叶斯参数估计"></a>最大似然和贝叶斯参数估计</h3><p><strong>参数估计</strong></p><p>给定分类器结构和函数形式，从而训练样本估计参数。使用最大似然（maximum likehood），贝叶斯估计（bayesian estimation）来估计模型的参数。</p><p><strong>最大似然估计</strong></p><p>最大似然估计的前提是默认知道了模型的形式。</p><p><img src="/images/nlp/image-20200131205146516.png" alt="image-20200131205146516" style="zoom:50%;"></p><p>最大似然估计的原理如上，样本间分布相互独立，概率密度的格式为：$p(x|w_i,\theta_i)$，通过估计参数$\theta$得出最后的结果，原理如下：</p><p><img src="/images/nlp/image-20200131212844576.png" alt="image-20200131212844576" style="zoom:50%;"></p><p>最大似然公式，通过最大求导令导数为零的方式来计算模型参数如下。</p><p><img src="/images/nlp/image-20200131215730451.png" alt="image-20200131215730451" style="zoom:50%;"></p><p>举个例子，概率密度函数形式为高斯混合函数：</p><p><img src="/images/nlp/image-20200131220410254.png" alt="image-20200131220410254" style="zoom:50%;"></p><p>通过求导数为0的方式，计算出模型的关键参数为 $\mu$ 以及 $\sum$ ，计算过程如下：</p><p><img src="/images/nlp/image-20200131222132022.png" alt="image-20200131222132022" style="zoom:50%;"></p><p><img src="/images/nlp/image-20200131222144463.png" alt="image-20200131222144463" style="zoom:50%;"></p><p><img src="/images/nlp/image-20200201151054278.png" alt="image-20200201151054278" style="zoom:60%;"></p><p><strong>贝叶斯参数估计</strong></p><p><img src="/images/nlp/image-20200131234433018.png" alt="image-20200131234433018" style="zoom:50%;"></p><p>主要原理是假设已经知道先验，用贝叶斯公式去估计模型的参数的后验分布，通常对于一些服从高斯分布的数据适用。</p><p><img src="/images/nlp/image-20200131234727931.png" alt="image-20200131234727931" style="zoom:50%;"></p><p>贝叶斯估计的步骤以及需要的条件。</p><p><img src="/images/nlp/image-20200201151321933.png" alt="image-20200201151321933" style="zoom:60%;"></p><p><strong>最大似然估计和贝叶斯估计的区别</strong></p><blockquote><p>最大似然估计和贝叶斯估计最大区别便在于估计的参数不同，最大似然估计要估计的参数θ被当作是固定形式的一个未知变量，然后我们结合真实数据通过最大化似然函数来求解这个固定形式的未知变量！</p></blockquote><blockquote><p>贝叶斯估计则是将参数视为是有某种已知先验分布的随机变量，意思便是这个参数他不是一个固定的未知数，而是符合一定先验分布如：随机变量θ符合正态分布等！那么在贝叶斯估计中除了类条件概率密度p(x|w)符合一定的先验分布，参数θ也符合一定的先验分布。我们通过贝叶斯规则将参数的先验分布转化成后验分布进行求解！</p></blockquote><h3 id="期望最大法EM"><a href="#期望最大法EM" class="headerlink" title="期望最大法EM"></a>期望最大法EM</h3><p><img src="/images/nlp/image-20200201153104416.png" alt="image-20200201153104416" style="zoom:50%;"></p><p>数据存在好数据以及bad数据，通过以及的参数，以及好的数据，共同来对缺失数据求期望。</p><p><img src="/images/nlp/image-20200201153235737.png" alt="image-20200201153235737" style="zoom:50%;"></p><p>步骤如上，首先计算预测函数的期望E。M步计算期望函数的最大化结果，直到模型性能提升程度小于阈值。</p><p>EM算法在高斯模型上的使用步骤：</p><p><img src="/images/nlp/image-20200201154032629.png" alt="image-20200201154032629" style="zoom:50%;"></p><p><img src="/images/nlp/image-20200201154122745.png" alt="image-20200201154122745" style="zoom:50%;"></p><h3 id="隐形马尔科夫模型"><a href="#隐形马尔科夫模型" class="headerlink" title="隐形马尔科夫模型"></a>隐形马尔科夫模型</h3><p>硬性马尔科夫模型是统计模型，用来描述一个含有未知参数的马尔科夫过程，其难点是从观测到的结果或参数中获得该过程的隐含参数，下面举个例子来说明：</p><p><img src="/images/nlp/image-20200201163743843.png" alt="image-20200201163743843" style="zoom:50%;"></p><p>我们有以上三种骰子，假设我们开始掷骰子，随机挑选骰子后记录得到的数字，最后得到的结果为一串3数字序列：1 6 3 5 2 7 3 5 2 4</p><p>这串数字叫做<strong>可见状态链</strong>，此外还有一个<strong>隐含的状态链</strong>，即选择骰子的序号：D6 D8 D8 D6 D4 D8 D6 D6 D4 D8。</p><p>马尔科夫链其实指的是这个隐含状态链，每个状态之间存在一个<strong>转换概率</strong>，例如D6后面下一个状态的概率都是1/3，其实我们也可以人为的设置骰子之间的转换概率。此外还有一个<strong>输出概率</strong>的概念，即在隐含状态和可见状态之间的输出概率。即D6骰子输出一个数字的概率是1/6，这个输出概率我们也是可以设置的。</p><p><img src="/images/nlp/image-20200201164743562.png" alt="image-20200201164743562" style="zoom:50%;"></p><p>通常我们在应用hhm的时候，这些状态信息是存在缺失的，因此hhm模型相关的算法主要分为三类：</p><ul><li><strong>知道骰子有几种（隐含状态数量），每种骰子是什么（转换概率），根据掷骰子掷出的结果（可见状态链），我想知道每次掷出来的都是哪种骰子（隐含状态链）</strong><ul><li>这个问题在语音识别领域称为解码问题，一种解法是求最大似然状态路径，即求一串骰子序列，该序列输出该观测结果的概率最大</li><li>第二种解法是求每次掷骰子是哪种骰子的概率。</li></ul></li><li><strong>知道骰子有几种（隐含状态数量），每种骰子是什么 （转换概率），根据掷骰子掷出的结果（可见状态链），我想知道掷出这个结果的概率。</strong> <ul><li>即根据模型去求解当前输出结果的概率大小，判断模型是否和数据相互匹配</li></ul></li><li><strong>知道骰子有几种（隐含状态数量），不知道每种骰子是什么（转换概率），观测到很多次掷骰子的结果 （可见状态链 ），我想反推出每种骰子是什么（转换概率）</strong><ul><li>这是个很常见的问题，根据可观测到的大量的可见状态链，去反推hhm模型中的参数，极可能是骰子的转换概率等</li></ul></li></ul><p><strong>破解骰子序列问题</strong></p><p><strong>求解最大似然路径问题：</strong>已知到骰子的结果序列，但是不知道用了哪种骰子，最简单的方法就是穷举从中选择最大概率序列。另一种选择方法为：<strong>viterbi algorithm</strong></p><p><strong>viterbi algorithm</strong></p><p>每次计算产生结果的骰子的序号时，选择最大概率的骰子作为当前位置上的骰子。我们发现，我们要求最大概率骰子序列时要做这么几件事情。首先，不管序列多长，要从序列长度为1算起，算序列长度为1时取到每个骰子的最大概率。然后，逐渐增加长度，每增加一次长度，重新算一遍在这个长度下最后一个位置取到每个骰子的最大概率。因为上一个长度下的取到每个骰子的最大概率都算过了，重新计算的话其实不难。当我们算到最后一位时，就知道最后一位是哪个骰子的概率最大了。然后，我们要把对应这个最大概率的序列从后往前推出来。<strong>只有算到最后，计算出最大的序列概率之后，才能确定序列上骰子是哪个。</strong></p><p><strong>谁动了骰子的问题</strong></p><p>当出现一段序列，你怀疑骰子被别人动过手脚，因此我们需要计算一下有问题骰子出现这段序列的概率是多少，正常的骰子出现这段序列的概率是多少。计算这段序列的概率就是所有结果的加和。计算这个问题的方法叫做<strong>forward algorithm</strong></p><p><strong>forward algorithm</strong></p><p>前向算法的计算方法为计算可能产生这个序列的所有的骰子的概率之和。例如这个三个骰子的情况，所有的概率之和计算如下：</p><p><img src="/images/nlp/image-20200202144838271.png" alt="image-20200202144838271" style="zoom:50%;"></p><p><strong>hhm总结</strong></p><p>以上两种方法，viterbi 以及forward算法一种计算序列上的最短路径，一种是计算产生所有的结果可能的骰子序列概率的加和算法。</p><h3 id="非参数方法"><a href="#非参数方法" class="headerlink" title="非参数方法"></a>非参数方法</h3><p><img src="/images/nlp/image-20200202150246277.png" alt="image-20200202150246277" style="zoom:50%;"></p><p>常用的密度估计算法如下：</p><p><img src="/images/nlp/image-20200202150336756.png" alt="image-20200202150336756" style="zoom:50%;"></p><p><strong>parzen window</strong></p><p><img src="/images/nlp/image-20200202151729629.png" alt="image-20200202151729629" style="zoom:50%;"></p><p>如上式，利用parzen window计算样本的概率密度分布，其中$h_n$为超参数。</p><p><img src="/images/nlp/image-20200202152404784.png" alt="image-20200202152404784" style="zoom:50%;"></p><p><strong>k近邻估计</strong></p><p><img src="/images/nlp/image-20200202152640531.png" alt="image-20200202152640531" style="zoom:50%;"></p><p>当收敛条件满足时，趋近于贝叶斯错误率。当k的个数为1时：</p><p><img src="/images/nlp//image-20200202152902374.png" alt="image-20200202152902374" style="zoom:50%;"></p><p><img src="/images/nlp/image-20200202153225602.png" alt="image-20200202153225602" style="zoom:50%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《统计学习方法》第4章 朴素贝叶斯法&lt;/p&gt;
&lt;p&gt;朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法，对于给定的数据集，首先基于特征条件独立假设学习输入、输出的联合概率分布。对于给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。&lt;br&gt;$$&lt;br&gt;P\left(\omega_{i} | \mathbf{x}\right)=\frac{p\left(\mathbf{x} | \omega_{i}\right) P\left(\omega_{i}\right)}{p(\mathbf{x})}=\frac{p\left(\mathbf{x} | \omega_{i}\right) P\left(\omega_{i}\right)}{\sum_{i=1}^{c} p\left(\mathbf{x} | \omega_{j}\right) P\left(\omega_{j}\right)}&lt;br&gt;$$&lt;br&gt;将后验问题转化为先验的形式。&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习方法" scheme="https://wenhui-zhou.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>k近邻法(3)</title>
    <link href="https://wenhui-zhou.github.io/2020/01/29/k%E8%BF%91%E9%82%BB%E6%B3%95-3/"/>
    <id>https://wenhui-zhou.github.io/2020/01/29/k近邻法-3/</id>
    <published>2020-01-29T05:08:45.000Z</published>
    <updated>2020-01-29T16:29:18.333Z</updated>
    
    <content type="html"><![CDATA[<p>《统计学习方法》第三章 k近邻法</p><a id="more"></a><h3 id="k近邻模型"><a href="#k近邻模型" class="headerlink" title="k近邻模型"></a>k近邻模型</h3><p>k近邻算法是一类基本的分类回归方法，假定给定一个训练数据集，其中实例类别已定，对新的实例，根据其k个最近邻的训练实例的类别通过多数表决等方式进行预测。其中k值得选择，距离度量以及分类决策规则是k近邻法的三个基本要素：</p><ul><li><p>优点</p><ul><li>精度高</li><li>对异常值不敏感</li><li>无数据输入假定</li></ul></li><li><p>缺点</p><ul><li>计算复杂度高</li><li>空间复杂度高</li></ul></li><li><p>适用数据范围</p><ul><li>数值型和标称型</li></ul></li></ul><p><strong>工作原理</strong></p><p>存在一个数据集，且数据集中样本与标签存在一一对应的关系，输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签作为新数据的分类类别。</p><p><strong>距离量度</strong></p><p>我们在确定两个向量是否属于同一类别的时候，我们需要利用一个距离量度来决定该实例的类别，距离定义方式有以下几种：</p><p>$L_p$距离：<br>$$<br>L_{p}\left(x_{i}, x_{j}\right)=\left(\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|^{p}\right)^{\frac{1}{p}}<br>$$<br>欧氏距离，L1以及L无穷距离：<br>$$<br>\begin{aligned} L_{2}\left(x_{i}, x_{j}\right) &amp;=\left(\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|^{2}\right)^{\frac{1}{2}} \ L_{1}\left(x_{i}, x_{j}\right) &amp;=\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right| \ L_{\infty}\left(x_{i}, x_{j}\right) &amp;=\max _{l}\left|x_{i}^{(l)}-x_{j}^{(l)}\right| \end{aligned}<br>$$<br><strong>k值选择</strong></p><ul><li><p>如果选择较小的K值</p><ul><li>学 习”的近似误差（approximation error)会减小，但 “学习”的估计误差（estimation error) 会增大</li><li>噪声敏感</li><li>K值的减小就意味着整体模型变得复杂，容易发生过 拟合</li></ul></li><li><p>如果选择较大的K值</p><ul><li>减少学习的估计误差，但缺点是学习的近似误差会增大.</li><li>K值的增大 就意味着整体的模型变得简单.</li></ul></li></ul><h3 id="KD-tree"><a href="#KD-tree" class="headerlink" title="KD tree"></a>KD tree</h3><p>实现k近邻法时，主要考虑的问题是如何对训练数据进行快速k近邻搜索，为了提高这种搜索的效率，可以考虑使用特殊的结构存储训练数据，以减少计算距离的次数，一个常用的存储结构就是kd tree。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《统计学习方法》第三章 k近邻法&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习方法" scheme="https://wenhui-zhou.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
</feed>
