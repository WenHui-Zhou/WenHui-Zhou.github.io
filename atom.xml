<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>WenHuiZhou</title>
  
  <subtitle>perper（打起精神！）</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://wenhui-zhou.github.io/"/>
  <updated>2020-01-29T16:29:18.333Z</updated>
  <id>https://wenhui-zhou.github.io/</id>
  
  <author>
    <name>WenHuiZhou</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>k近邻法(3)</title>
    <link href="https://wenhui-zhou.github.io/2020/01/29/k%E8%BF%91%E9%82%BB%E6%B3%95-3/"/>
    <id>https://wenhui-zhou.github.io/2020/01/29/k近邻法-3/</id>
    <published>2020-01-29T05:08:45.000Z</published>
    <updated>2020-01-29T16:29:18.333Z</updated>
    
    <content type="html"><![CDATA[<p>《统计学习方法》第三章 k近邻法</p><a id="more"></a><h3 id="k近邻模型"><a href="#k近邻模型" class="headerlink" title="k近邻模型"></a>k近邻模型</h3><p>k近邻算法是一类基本的分类回归方法，假定给定一个训练数据集，其中实例类别已定，对新的实例，根据其k个最近邻的训练实例的类别通过多数表决等方式进行预测。其中k值得选择，距离度量以及分类决策规则是k近邻法的三个基本要素：</p><ul><li><p>优点</p><ul><li>精度高</li><li>对异常值不敏感</li><li>无数据输入假定</li></ul></li><li><p>缺点</p><ul><li>计算复杂度高</li><li>空间复杂度高</li></ul></li><li><p>适用数据范围</p><ul><li>数值型和标称型</li></ul></li></ul><p><strong>工作原理</strong></p><p>存在一个数据集，且数据集中样本与标签存在一一对应的关系，输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签作为新数据的分类类别。</p><p><strong>距离量度</strong></p><p>我们在确定两个向量是否属于同一类别的时候，我们需要利用一个距离量度来决定该实例的类别，距离定义方式有以下几种：</p><p>$L_p$距离：<br>$$<br>L_{p}\left(x_{i}, x_{j}\right)=\left(\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|^{p}\right)^{\frac{1}{p}}<br>$$<br>欧氏距离，L1以及L无穷距离：<br>$$<br>\begin{aligned} L_{2}\left(x_{i}, x_{j}\right) &amp;=\left(\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|^{2}\right)^{\frac{1}{2}} \ L_{1}\left(x_{i}, x_{j}\right) &amp;=\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right| \ L_{\infty}\left(x_{i}, x_{j}\right) &amp;=\max _{l}\left|x_{i}^{(l)}-x_{j}^{(l)}\right| \end{aligned}<br>$$<br><strong>k值选择</strong></p><ul><li><p>如果选择较小的K值</p><ul><li>学 习”的近似误差（approximation error)会减小，但 “学习”的估计误差（estimation error) 会增大</li><li>噪声敏感</li><li>K值的减小就意味着整体模型变得复杂，容易发生过 拟合</li></ul></li><li><p>如果选择较大的K值</p><ul><li>减少学习的估计误差，但缺点是学习的近似误差会增大.</li><li>K值的增大 就意味着整体的模型变得简单.</li></ul></li></ul><h3 id="KD-tree"><a href="#KD-tree" class="headerlink" title="KD tree"></a>KD tree</h3><p>实现k近邻法时，主要考虑的问题是如何对训练数据进行快速k近邻搜索，为了提高这种搜索的效率，可以考虑使用特殊的结构存储训练数据，以减少计算距离的次数，一个常用的存储结构就是kd tree。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《统计学习方法》第三章 k近邻法&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习方法" scheme="https://wenhui-zhou.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>感知机(2)</title>
    <link href="https://wenhui-zhou.github.io/2020/01/29/%E6%84%9F%E7%9F%A5%E6%9C%BA-2/"/>
    <id>https://wenhui-zhou.github.io/2020/01/29/感知机-2/</id>
    <published>2020-01-28T16:09:19.000Z</published>
    <updated>2020-01-28T17:17:52.217Z</updated>
    
    <content type="html"><![CDATA[<p>《统计学习方法》第二章《感知机》</p><a id="more"></a><h3 id="感知机模型"><a href="#感知机模型" class="headerlink" title="感知机模型"></a>感知机模型</h3><p>感知机是一个二分类线性分类模型，输入为实例特征向量，输出为1或-1。感知器对应输出空间中将实例划分为正负两类。</p><ul><li>输入为实例的特征向量，输出为实例的类别，取+1和-1；</li><li>感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于判别模型；</li><li>导入基于误分类的损失函数；</li><li>利用梯度下降法对损失函数进行极小化；</li><li>感知机学习算法具有简单而易于实现的优点，分为原始形式和对偶形式；</li></ul><p>感知机函数如下：<br>$$<br>f(x)=\operatorname{sign}(w \cdot x+b)<br>$$<br>其中w为模型参数，b为偏置。</p><p><img src="../images/nlp/image-20200129004532357.png" alt="image-20200129004532357" style="zoom:50%;"></p><h3 id="感知机学习策略"><a href="#感知机学习策略" class="headerlink" title="感知机学习策略"></a>感知机学习策略</h3><p>通过定义一个损失函数，来达到感知机的学习的目的。我们选择误分点到分界点的距离作为损失函数，误分点到超平面的距离为：<br>$$<br>\frac{1}{|w|}\left|w \cdot x_{0}+b\right|<br>$$<br>误分点的性质为：<br>$$<br>-y_{i}\left(w \cdot x_{i}+b\right)&gt;0<br>$$<br>因此损失函数为所有误分点的距离集合：<br>$$<br>-\frac{1}{|w|} \sum_{x_{i} \in \mathcal{M}} y_{i}\left(w \cdot x_{i}+b\right)<br>$$<br>其中前面的系数可以忽略。</p><h3 id="感知机的学习算法"><a href="#感知机的学习算法" class="headerlink" title="感知机的学习算法"></a>感知机的学习算法</h3><p>通过随机梯度下降法，分别对w和b进行求导，得到下一步参数的更新：<br>$$<br>\nabla_{w} L(w, b)=-\sum_{x_{i} \in M} y_{i} x_{i} \quad \nabla_{b} L(w, b)=-\sum_{x_{i}, m} y_{i}<br>$$</p><p>$$<br>w \leftarrow w+\eta y_{i} x_{i} \quad b \leftarrow b+\eta y_{i}<br>$$</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>还有一些算法优化的可行性验证这里就忽略了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《统计学习方法》第二章《感知机》&lt;/p&gt;
    
    </summary>
    
      <category term="统计学习方法" scheme="https://wenhui-zhou.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>统计学习方法概述</title>
    <link href="https://wenhui-zhou.github.io/2020/01/26/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/"/>
    <id>https://wenhui-zhou.github.io/2020/01/26/统计学习方法概述/</id>
    <published>2020-01-26T05:42:16.000Z</published>
    <updated>2020-01-26T12:01:21.778Z</updated>
    
    <content type="html"><![CDATA[<p>第一章 《统计学习方法概论》</p><p>这一部分介绍了统计学习的一些基本概念，基本问题，基本方法等，作为统计学习的一个引入。</p><p>写《统计学习方法》这个系列博客的目的在于在找工作前，对机器学习方法重新温顾一下，在肺炎国难当前，争分夺秒，把这本书看完！</p><p>这本书在网上有着广泛的讨论，笔记，代码一应俱全，因此这个系列将会参考这些资料：</p><ul><li><a href="https://github.com/SmirkCao/Lihang" target="_blank" rel="noopener">https://github.com/SmirkCao/Lihang</a> （笔记）</li><li><a href="https://www.jiqizhixin.com/articles/2019-11-11-15" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2019-11-11-15</a> （课件）</li><li><a href="https://github.com/fengdu78/lihang-code" target="_blank" rel="noopener">https://github.com/fengdu78/lihang-code</a> （代码）</li></ul><a id="more"></a><h3 id="部分符号"><a href="#部分符号" class="headerlink" title="部分符号"></a>部分符号</h3><p>$H$: 希尔伯特空间</p><ul><li>希尔伯特空间 = 无限维+度量+线性+范数+内积 = 无限维 + 欧几里得空间</li></ul><h3 id="统计学习"><a href="#统计学习" class="headerlink" title="统计学习"></a>统计学习</h3><p><strong>特点</strong></p><p>基于数据构建概率统计模型，并运用该模型进行预测与分析的一门学科。是基于数据驱动的。</p><p><strong>对象</strong></p><p>对象是数据，提取数据特征，抽象出数据模型；</p><p>统计学习的一条基本假设是：同类数据具有一定的统计规律性</p><p><strong>目的</strong></p><p>通过构建模型对已知数据的概率建模，使得模型对未知数据进行准确的预测与分析。</p><p><strong>方法</strong></p><p>统计学习方法包含三要素：模型，策略，算法</p><h3 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h3><p><strong>基本概念</strong></p><p><strong>输入空间、特征空间、输出空间</strong></p><p>输入空间指输入的数据的集合；特征空间指所有特征向量所存在的空间；输出空间指输出的集合所在的空间。</p><p><strong>联合概率分布</strong></p><p>监督学习假设输入与输出遵循联合概率分布$P(X,Y)$函数。</p><p><strong>假设空间</strong></p><p>监督学习的目的在于学习一个由输入到输出的一个映射。映射就是有模型来表示的。假设空间就是所有的映射的集合，我们的目的就是找到最好的一个映射。</p><p><strong>问题的形式化</strong></p><p>通过成对的数据样本，学习一个函数映射，即条件概率分布（决策函数）$P(Y|X)$，该函数描述了输入与输出随机变量之间的映射关系。</p><h3 id="统计学习三要素"><a href="#统计学习三要素" class="headerlink" title="统计学习三要素"></a>统计学习三要素</h3><p>方法 = 模型 + 策略 + 算法</p><p><strong>模型</strong></p><p>在监督学习中，<strong>模型就是所要学习的条件概率分布或决策函数</strong>，模型的假设空间包含所有可能的条件概率函数。</p><p><strong>策略</strong></p><p>引入损失函数来度量模型的性能好坏，预测效果越好的模型损失函数的值越小。</p><p>损失函数的期望：<br>$$<br>R_{\mathrm{exp}}(f)=E_{P}[L(Y, f(X))]=\int_{x \times y} L(y, f(x)) P(x, y) \mathrm{d} x \mathrm{d} y<br>$$<br>上式是模型$f(x)$关于联合分布$P(X,Y)$的平均意义下的损失，称为风险函数或期望损失。</p><p><strong>经验风险最小化与结构风险最小化</strong></p><p>经验风险指在训练集上的损失的平均值，经验风险最小化模型：<br>$$<br>\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)<br>$$<br>当样本容量很小的时候，模型优化经验风险最小化会导致模型的<strong>过拟合</strong>发生。</p><p><strong>结构风险最小化</strong> structure risk minimization，为防止过拟合提出的策略，等价于正则化（regularization），加入正则化项regularizer，或罚项 penalty term：<br>$$<br>R_{\operatorname{srm}}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)<br>$$<br><strong>算法</strong></p><p>算法指模型训练过程中的优化策略。如果最优化问题有显式的解析式，那么这个最优化问题就比较简单，但是通常解析解不存在，这就需要用数值计算的方法去求解。统计学习方法可以利用已有的最优化算法进行模型的优化。</p><h3 id="模型评估与模型选择"><a href="#模型评估与模型选择" class="headerlink" title="模型评估与模型选择"></a>模型评估与模型选择</h3><p>当我们一味追求训练数据上的拟合能力的时候，往往将导致模型发生过拟合。<img src="/images/nlp/image-20200126191036412.png" alt="image-20200126191036412" style="zoom:50%;"></p><p>因此在选择模型的时候，需要在模型的复杂度与预测的误差上做出最优的选择。</p><h3 id="正则化与交叉验证"><a href="#正则化与交叉验证" class="headerlink" title="正则化与交叉验证"></a>正则化与交叉验证</h3><p>模型选择的典型方法是正则化，正则化是结构化方向最小化策略的实现，模型越复杂正则化值就会越大：<br>$$<br>L(w)=\frac{1}{N} \sum_{i=1}^{N}\left(f\left(x_{i} ; w\right)-y_{i}\right)^{2}+\frac{\lambda}{2}|w|^{2}<br>$$<br><strong>交叉验证</strong></p><p>交叉验证的思想是将数据进行切分，分成训练集，测试集以及验证集等，例如留一验证法等。</p><h3 id="泛化能力"><a href="#泛化能力" class="headerlink" title="泛化能力"></a>泛化能力</h3><p>泛化能力指模型对未知数据的预测结果，可以通过比较两个函数的泛化误差的上界来比较两种方法的好坏。</p><h3 id="生成模型与判别模型"><a href="#生成模型与判别模型" class="headerlink" title="生成模型与判别模型"></a>生成模型与判别模型</h3><p>生成模型表示了给定输入X产生Y的生成关系，判别模型给定X，输出X的类别。</p><h3 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h3><p>分类问题即对输入进行分类，分类的评价指标是准确率和召回率。</p><ul><li>TP  true positive</li><li>FN  false negative</li><li>FP  false positive</li><li>TN  true negative</li></ul><p>$$<br>P = \frac{TP}{TP+FP}<br>$$</p><p>$$<br>R = \frac{TP}{TP + FN}<br>$$</p><p>F1 值：<br>$$<br>F_1 = \frac{2TP}{2TP + FP + FN}<br>$$</p><h3 id="标注问题"><a href="#标注问题" class="headerlink" title="标注问题"></a>标注问题</h3><p>输入：观测序列</p><p>输出：标记序列或状态序列</p><p>如对一个长句子的标注。</p><h3 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h3><p>回归问题数据是连续的，同时常常使用著名的最小二乘法来求解。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本章概要的介绍了一下统计学习的各个方法，如果深入到深入学习来说，可以看出来有很多东西没有涉及到，但是对于机器学习基础来说，足够了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;第一章 《统计学习方法概论》&lt;/p&gt;
&lt;p&gt;这一部分介绍了统计学习的一些基本概念，基本问题，基本方法等，作为统计学习的一个引入。&lt;/p&gt;
&lt;p&gt;写《统计学习方法》这个系列博客的目的在于在找工作前，对机器学习方法重新温顾一下，在肺炎国难当前，争分夺秒，把这本书看完！&lt;/p&gt;
&lt;p&gt;这本书在网上有着广泛的讨论，笔记，代码一应俱全，因此这个系列将会参考这些资料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/SmirkCao/Lihang&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/SmirkCao/Lihang&lt;/a&gt; （笔记）&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.jiqizhixin.com/articles/2019-11-11-15&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.jiqizhixin.com/articles/2019-11-11-15&lt;/a&gt; （课件）&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/fengdu78/lihang-code&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/fengdu78/lihang-code&lt;/a&gt; （代码）&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="统计学习方法" scheme="https://wenhui-zhou.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>cs224N word vector I</title>
    <link href="https://wenhui-zhou.github.io/2020/01/09/cs224N-word-vector-I/"/>
    <id>https://wenhui-zhou.github.io/2020/01/09/cs224N-word-vector-I/</id>
    <published>2020-01-09T06:29:54.000Z</published>
    <updated>2020-01-12T16:30:57.842Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章是CS224N winter2019的第一次课的内容，主要针对词的表示，词向量生成的一些方法，从最早到最新，列举了这些算法以及应用。</p><a id="more"></a><h3 id="NLP任务的特殊性"><a href="#NLP任务的特殊性" class="headerlink" title="NLP任务的特殊性"></a>NLP任务的特殊性</h3><p>说道NLP任务的特殊性，其实是针对图像任务来说的。对于图像任务来说，我们所看到的场景通常直接反映了图像中所包含的内涵，同时图像上像素点通常是连续的，对于一个可微分系统求最优化的过程（深度学习的内涵就是构建一个可微分的系统），是天然切合的。</p><p>我们知道NLP最小的单元是词，由众多的词组成句子，形成语义。<strong>对于一个词来说，它在形状上不表示任何的含义，而是指代一种抽象的含义</strong>，因此在理解词含义的时候，我们需要结合上下文，结合语境，诸如LSTM，transformer这种能够获得较长上下文信息的结果在NLP中起到巨大的作用。此外，还存在着大量的一词多义的情形，因此具体的语境，上下文对理解NLP任务显得至关重要。</p><p>另一个显著的不同在于图像色彩在变化上是连续的，而NLP任务中，每个词之间是离散的，不存在连续的关系。因此我们需要对每次词进行适当的编码，转化成连续的词向量的形式。通过这种方式最优化我们的模型，得到NLP任务的解。</p><h3 id="word-vector"><a href="#word-vector" class="headerlink" title="word vector"></a>word vector</h3><p>如何用向量表示词，一个重要的指标是词向量能够反映词之间的相似性和相异性。例如苹果和巧克力，应当比苹果和梨之间的距离要大。下面是自己中词的表示方式：</p><ul><li><strong>同义词的字典：</strong>一个比较直观的解决方法是建立一个同义词的字典，将所有的同义词归类，用字典的方式来表示词，但是这种方式有缺点，我们无法涵盖所有的单词的含义，需要人去维护字典。</li><li><strong>one-hot方式：</strong>采用onehot方式，奖励一个大小为vocab size的向量，在该词的位置为1，其他为0。这种方式的缺点是，需要维护一个巨大的矩阵，同时每个 词之间都是独立的，无法表示不同词的相似性。</li><li><strong>一个词的含义由它周围的词决定：</strong>即一个词的含义应该与他的上下文（fix-window）所决定的。这种利用周围的词来表示当前的词的方式称为词嵌入word-embedding，它实际上是一种分布。</li></ul><p><img src="/images/nlp//image-20200112210153920.png" alt="image-20200112210153920" style="zoom:40%;"></p><h3 id="word-vector-1"><a href="#word-vector-1" class="headerlink" title="word vector"></a>word vector</h3><p>2013年由Mikolov提出，提出利用周围周围的词来预测当前的词：</p><ul><li>一个很大的语料库</li><li>每一个词表示表示成 一个词向量</li><li>遍历句子中的每一个词c</li><li>利用每一个c周围的词（fix-window），来计算c的相似性</li><li>通过迭代，最大化预测c的概率</li></ul><p><img src="/images/nlp//image-20200112214809724.png" alt="image-20200112214809724" style="zoom:50%;"></p><p>构造损失函数如下：</p><p><img src="/images/nlp//image-20200112215013622.png" alt="image-20200112215013622" style="zoom:40%;"></p><p>利用贝叶斯公式，利用前后m个位置的词来预测当前位置词的概率。构造一个负log似然函数，即交叉熵函数，通过最小化交叉熵损失，预测正确的概率最大。</p><p>其中每个词概率的计算p：</p><p><img src="/images/nlp//image-20200112215844822.png" alt="image-20200112215844822" style="zoom:40%;"></p><p>因此对于每一个词来说，同时有两个向量来表示：</p><p><img src="/images/nlp//image-20200112224851890.png" alt="image-20200112224851890" style="zoom:40%;"></p><p>在词的含义由周围词决定的思想下，有着两种模型的变种，一种是CBOW，另一种是skip-gram。</p><h3 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h3><p>Continuous Bag of Words Model,连续的词包模型，将句子挖空，利用其它词来预测当前的词。每个词学习两个向量：</p><ul><li>v：when the word is in the context</li><li>u：when the word is in the center</li></ul><p><img src="/images/nlp//image-20200112230514347.png" alt="image-20200112230514347" style="zoom:50%;"></p><p>如上，我们输入一个长度为字典词数长度的词（可以使onehot形式），然后学习一个从输入到隐变量的一个映射，然后在学一个隐变量到输出的映射。输入时句子中出了挖空的那个单词。最后通过softmax变成概率之后，最优化交叉熵损失，得到最优的结果。</p><h3 id="skip-gram-model"><a href="#skip-gram-model" class="headerlink" title="skip gram model"></a>skip gram model</h3><p>这个模型的思路和CBOW相反，利用一个单词去预测其他周围位置的单词，网路结构如下：</p><p><img src="/images/nlp//image-20200112231300222.png" alt="image-20200112231300222" style="zoom:50%;"></p><p>输入为一个词的onehot，学习两个映射过程，最终得到其周围fix-window内的其他词的预测结果，其过程与CBOW类似。</p><p>上述的两个模型中，均是三层结构，隐藏层维度为我们希望的词向量的长度，从输入到隐藏层之间的映射参数即为词典长度x词向量长度的一个矩阵，即我们所要的wordvec。</p><h3 id="negative-sampling"><a href="#negative-sampling" class="headerlink" title="negative sampling"></a>negative sampling</h3><p>在训练神经网络时，每当接受一个训练样本，然后调整所有神经单元权重参数，来使神经网络预测更加准确。换句话说，每个训练样本都将会调整所有神经网络中的参数。<br> 我们词汇表的大小决定了我们skip-gram 神经网络将会有一个非常大的权重参数，并且所有的权重参数会随着数十亿训练样本不断调整。</p><p>negative sampling  每次让一个训练样本仅仅更新一小部分的权重参数，从而降低梯度下降过程中的计算量。<br> 如果 vocabulary 大小为1万时， 当输入样本 ( “fox”, “quick”) 到神经网络时， “ fox” 经过 one-hot 编码，在输出层我们期望对应 “quick” 单词的那个神经元结点输出 1，其余 9999 个都应该输出 0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们为 negative word.   negative sampling 的想法也很直接 ，将随机选择一小部分的 negative words，比如选 10个 negative words 来更新对应的权重参数。（选择onehot中为0的一部分数参与模型的更新。）</p><p>在论文中作者指出指出对于小规模数据集，建议选择 5-20 个 negative words，对于大规模数据集选择 2-5个 negative words.</p><p>如果使用了 negative sampling 仅仅去更新positive word- “quick” 和选择的其他 10 个negative words 的结点对应的权重，共计 11 个输出神经元，相当于每次只更新 300 x 11 = 3300 个权重参数。对于 3百万 的权重来说，相当于只计算了千分之一的权重，这样计算效率就大幅度提高。</p><p><strong>如何选择负样本</strong></p><p>一个词是否会被选择为负样本与其出现的频率有关，出现的频次越高，越容易被当成负样本：</p><p><img src="/images/nlp//image-20200112233855262.png" alt="image-20200112233855262" style="zoom:50%;"></p><p>经过实验，当这个数为3/4的时候，模型表现最好。</p><h3 id="hierarchical-softmax"><a href="#hierarchical-softmax" class="headerlink" title="hierarchical softmax"></a>hierarchical softmax</h3><p>层次softmax是另一种优化模型计算的方法，他的思路借鉴了huffman树的思想，出现越是频繁的树，他出现的位置越浅。哈夫曼树每一个叶子节点表示一个类别，每个非叶子节点需要做一次二分类，走左边后走右边的概率用逻辑回归来表示：</p><p><img src="/images/nlp//image-20200113002246268.png" alt="image-20200113002246268" style="zoom:50%;"></p><p>如何降低复杂度呢，当k为词个数时,h为维度，可以将复杂度降到 $O(hlog_2(k))$。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>以上便是wordvec的表示方法，即解决了NLP的一个最基本，最重要的问题，词的表示。我们利用CBOW，skip-gram等模型，在优化模型的过程中，得到了词到向量的一个映射关系，这个关系就是wordvec的一个映射矩阵，有了这个矩阵我们就可以将词转化成向量。此时向量是根据其前后的单词而产生的，因此此位置的单词是根据前后的词而产生。下面一篇post将介绍wordvec更深层的东西，wordsence。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章是CS224N winter2019的第一次课的内容，主要针对词的表示，词向量生成的一些方法，从最早到最新，列举了这些算法以及应用。&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="https://wenhui-zhou.github.io/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>2019！2020！</title>
    <link href="https://wenhui-zhou.github.io/2020/01/01/2019%EF%BC%812020%EF%BC%81/"/>
    <id>https://wenhui-zhou.github.io/2020/01/01/2019！2020！/</id>
    <published>2020-01-01T03:34:18.000Z</published>
    <updated>2020-01-01T03:35:58.848Z</updated>
    
    <content type="html"><![CDATA[<p>2019年的最后几天，我在想给这一年打上一个什么样的标签，让我十几二十年后还能想起来这一年。确实是，我记不清过去的二十几年，唯一的感触剩下时间匆匆。</p><p>2019年1月份，和xu在北大三教的咖啡厅搭了这个网站，与此同时进行的还有刷算法题，准备着一些东西。再往前推几天，我匆匆完成了研一上最后的考试，连夜收拾行李进城。和yingying在雁栖湖有很多美好的回忆，可惜那些事情发生在2018年了，2018年对我来说是煎熬的一年，有一种引力阻止我思考里头究竟发生了什么事情。不过最令我开心的是在2018年结束的时候，我收获了很多宝贵的友谊。</p><p>所以我想，我不能在失去2019年了。</p><p>让我努力回忆几件事情，把一年的生活串起来。</p><p><strong>第一件事情</strong>，2019年新年在2月5号，过完春节我提前了两周时间回到雁栖湖。北京大雪后不久，校园里寂静的景象让我震惊，整个雁栖湖被雪裹住，我拖着行李箱，踏出一条雪路。接下来一个月我把时间安排的满满当当，这段时间，自我怀疑、喝鸡汤、再次怀疑循环往复。这段经历给我的启发很多，我愿意把它写出来：</p><ul><li>制定一个目标每时每刻努力着，这个过程是非常充实且令人满足的。结果反而是其次</li><li>目标明确的好处是知道自己要干什么，坏处是焦虑越来越严重</li></ul><p><strong>第二件事，</strong>我开始慢慢窥探自己的性格。事情讲起来很琐碎，我意识到自己在很多不确定的事情面前，显得唯唯诺诺，不敢承担责任。和老板聊到这件事情让我感触颇深，也对老板充满了感激。我在此总结一下导师给我的帮助：一个人自信与否在于他看待问题的角度，林彪认为一场战役七层胜算则可以打，粟裕有四层胜算便胸有成竹，侃侃而谈。</p><p>所以我大可不必这样的担心，也由此我摆脱了大部分的焦虑。后来才意识到，突破自己是困难的，很大一部分原因是无法找到问题的关键。此刻我很需要这样的信心，我也感到自己有了长足的进步。给人以正能量，给人以信心！</p><p>另一方面是自我意识的觉醒，顺从应当是我从小到大的习惯，当我发现我讨厌这样的自己，我会刻意去做一些改变，我会愿意决定一些事情，事先想好事情的应对方案。在这件事情上我还不够好。</p><p><strong>第三件事，</strong>生活需要理想，信仰。如果物质追求成为生活的目标时，生活是沉重且乏味的。而后读了《乔布斯传》，发现在物质之上，有着一群人，在追逐着梦想，渴望改变世界，追求着精神上更美好的东西。这对我的冲击是巨大的，我被这些浅显的道理击中，在回头看时，发现这些东西对我是多么的重要。想起来一次去参观腾讯，晚上8点和朋友坐班车回来，看到眼前拥堵的公路，熙熙攘攘的车子仿佛看到了日后无数个上下班的日日夜夜 ，备受打击。但是回头一想，你每天为着心中的理想奋斗，这是多么振奋人心的事情啊！</p><p>嗯，人生是伟大的理想，是诗和远方。</p><p><strong>最后</strong>，以上三件事情能够大致的勾勒出2019的轮廓，也许到我人生快结束的时候，也会有三件事情，勾勒我的一生。毕竟和时间比起来我们都太渺小。2019年很多时候在忙碌，但是真正对我有推动作用的事情却很少，最后还希望记录一些感悟：</p><ul><li>没有一个夜晚允许我们有一点软弱</li><li>永远不要掉队</li><li>人生应当有更高的追求和理想</li><li>相信积累的力量</li><li>阅读能够拯救我的内心，让我体会到真正的快乐</li><li>清醒的认识自己</li><li>展现积极的一面，永远是自己给别人信心</li></ul><p><strong>最后的最后，2020，happy new year!</strong></p><p><strong>2019/12/31</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;2019年的最后几天，我在想给这一年打上一个什么样的标签，让我十几二十年后还能想起来这一年。确实是，我记不清过去的二十几年，唯一的感触剩下时间匆匆。&lt;/p&gt;
&lt;p&gt;2019年1月份，和xu在北大三教的咖啡厅搭了这个网站，与此同时进行的还有刷算法题，准备着一些东西。再往前推几
      
    
    </summary>
    
    
  </entry>
  
</feed>
