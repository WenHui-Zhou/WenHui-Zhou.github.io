<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>WenHuiZhou</title>
  
  <subtitle>perper（打起精神！）</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://wenhui-zhou.github.io/"/>
  <updated>2020-01-26T11:52:47.395Z</updated>
  <id>https://wenhui-zhou.github.io/</id>
  
  <author>
    <name>WenHuiZhou</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>统计学习方法概述</title>
    <link href="https://wenhui-zhou.github.io/2020/01/26/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0/"/>
    <id>https://wenhui-zhou.github.io/2020/01/26/统计学习方法概述/</id>
    <published>2020-01-26T05:42:16.000Z</published>
    <updated>2020-01-26T11:52:47.395Z</updated>
    
    <content type="html"><![CDATA[<p>第一章 《统计学习方法概论》</p><p>这一部分介绍了统计学习的一些基本概念，基本问题，基本方法等，作为统计学习的一个引入。</p><p>写《统计学习方法》这个系列博客的目的在于在找工作前，对机器学习方法重新温顾一下，在肺炎国难当前，争分夺秒，把这本书看完！</p><p>这本书在网上有着广泛的讨论，笔记，代码一应俱全，因此这个系列将会参考这些资料：</p><ul><li><a href="https://github.com/SmirkCao/Lihang" target="_blank" rel="noopener">https://github.com/SmirkCao/Lihang</a> （笔记）</li><li><a href="https://www.jiqizhixin.com/articles/2019-11-11-15" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2019-11-11-15</a> （课件）</li><li><a href="https://github.com/fengdu78/lihang-code" target="_blank" rel="noopener">https://github.com/fengdu78/lihang-code</a> （代码）</li></ul><a id="more"></a><h3 id="部分符号"><a href="#部分符号" class="headerlink" title="部分符号"></a>部分符号</h3><p>$H$: 希尔伯特空间</p><ul><li>希尔伯特空间 = 无限维+度量+线性+范数+内积 = 无限维 + 欧几里得空间</li></ul><h3 id="统计学习"><a href="#统计学习" class="headerlink" title="统计学习"></a>统计学习</h3><p><strong>特点</strong></p><p>基于数据构建概率统计模型，并运用该模型进行预测与分析的一门学科。是基于数据驱动的。</p><p><strong>对象</strong></p><p>对象是数据，提取数据特征，抽象出数据模型；</p><p>统计学习的一条基本假设是：同类数据具有一定的统计规律性</p><p><strong>目的</strong></p><p>通过构建模型对已知数据的概率建模，使得模型对未知数据进行准确的预测与分析。</p><p><strong>方法</strong></p><p>统计学习方法包含三要素：模型，策略，算法</p><h3 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h3><p><strong>基本概念</strong></p><p><strong>输入空间、特征空间、输出空间</strong></p><p>输入空间指输入的数据的集合；特征空间指所有特征向量所存在的空间；输出空间指输出的集合所在的空间。</p><p><strong>联合概率分布</strong></p><p>监督学习假设输入与输出遵循联合概率分布$P(X,Y)$函数。</p><p><strong>假设空间</strong></p><p>监督学习的目的在于学习一个由输入到输出的一个映射。映射就是有模型来表示的。假设空间就是所有的映射的集合，我们的目的就是找到最好的一个映射。</p><p><strong>问题的形式化</strong></p><p>通过成对的数据样本，学习一个函数映射，即条件概率分布（决策函数）$P(Y|X)$，该函数描述了输入与输出随机变量之间的映射关系。</p><h3 id="统计学习三要素"><a href="#统计学习三要素" class="headerlink" title="统计学习三要素"></a>统计学习三要素</h3><p>方法 = 模型 + 策略 + 算法</p><p><strong>模型</strong></p><p>在监督学习中，<strong>模型就是所要学习的条件概率分布或决策函数</strong>，模型的假设空间包含所有可能的条件概率函数。</p><p><strong>策略</strong></p><p>引入损失函数来度量模型的性能好坏，预测效果越好的模型损失函数的值越小。</p><p>损失函数的期望：<br>$$<br>R_{\mathrm{exp}}(f)=E_{P}[L(Y, f(X))]=\int_{x \times y} L(y, f(x)) P(x, y) \mathrm{d} x \mathrm{d} y<br>$$<br>上式是模型$f(x)$关于联合分布$P(X,Y)$的平均意义下的损失，称为风险函数或期望损失。</p><p><strong>经验风险最小化与结构风险最小化</strong></p><p>经验风险指在训练集上的损失的平均值，经验风险最小化模型：<br>$$<br>\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)<br>$$<br>当样本容量很小的时候，模型优化经验风险最小化会导致模型的<strong>过拟合</strong>发生。</p><p><strong>结构风险最小化</strong> structure risk minimization，为防止过拟合提出的策略，等价于正则化（regularization），加入正则化项regularizer，或罚项 penalty term：<br>$$<br>R_{\operatorname{srm}}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)<br>$$<br><strong>算法</strong></p><p>算法指模型训练过程中的优化策略。如果最优化问题有显式的解析式，那么这个最优化问题就比较简单，但是通常解析解不存在，这就需要用数值计算的方法去求解。统计学习方法可以利用已有的最优化算法进行模型的优化。</p><h3 id="模型评估与模型选择"><a href="#模型评估与模型选择" class="headerlink" title="模型评估与模型选择"></a>模型评估与模型选择</h3><p>当我们一味追求训练数据上的拟合能力的时候，往往将导致模型发生过拟合。<img src="../images/nlp/image-20200126191036412.png" alt="image-20200126191036412" style="zoom:50%;"></p><p>因此在选择模型的时候，需要在模型的复杂度与预测的误差上做出最优的选择。</p><h3 id="正则化与交叉验证"><a href="#正则化与交叉验证" class="headerlink" title="正则化与交叉验证"></a>正则化与交叉验证</h3><p>模型选择的典型方法是正则化，正则化是结构化方向最小化策略的实现，模型越复杂正则化值就会越大：<br>$$<br>L(w)=\frac{1}{N} \sum_{i=1}^{N}\left(f\left(x_{i} ; w\right)-y_{i}\right)^{2}+\frac{\lambda}{2}|w|^{2}<br>$$<br><strong>交叉验证</strong></p><p>交叉验证的思想是将数据进行切分，分成训练集，测试集以及验证集等，例如留一验证法等。</p><h3 id="泛化能力"><a href="#泛化能力" class="headerlink" title="泛化能力"></a>泛化能力</h3><p>泛化能力指模型对未知数据的预测结果，可以通过比较两个函数的泛化误差的上界来比较两种方法的好坏。</p><h3 id="生成模型与判别模型"><a href="#生成模型与判别模型" class="headerlink" title="生成模型与判别模型"></a>生成模型与判别模型</h3><p>生成模型表示了给定输入X产生Y的生成关系，判别模型给定X，输出X的类别。</p><h3 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h3><p>分类问题即对输入进行分类，分类的评价指标是准确率和召回率。</p><ul><li>TP  true positive</li><li>FN  false negative</li><li>FP  false positive</li><li>TN  true negative</li></ul><p>$$<br>P = \frac{TP}{TP+FP}<br>$$</p><p>$$<br>R = \frac{TP}{TP + FN}<br>$$</p><p>F1 值：<br>$$<br>F_1 = \frac{2TP}{2TP + FP + FN}<br>$$</p><h3 id="标注问题"><a href="#标注问题" class="headerlink" title="标注问题"></a>标注问题</h3><p>输入：观测序列</p><p>输出：标记序列或状态序列</p><p>如对一个长句子的标注。</p><h3 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h3><p>回归问题数据是连续的，同时常常使用著名的最小二乘法来求解。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本章概要的介绍了一下统计学习的各个方法，如果深入到深入学习来说，可以看出来有很多东西没有涉及到，但是对于机器学习基础来说，足够了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;第一章 《统计学习方法概论》&lt;/p&gt;
&lt;p&gt;这一部分介绍了统计学习的一些基本概念，基本问题，基本方法等，作为统计学习的一个引入。&lt;/p&gt;
&lt;p&gt;写《统计学习方法》这个系列博客的目的在于在找工作前，对机器学习方法重新温顾一下，在肺炎国难当前，争分夺秒，把这本书看完！&lt;/p&gt;
&lt;p&gt;这本书在网上有着广泛的讨论，笔记，代码一应俱全，因此这个系列将会参考这些资料：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/SmirkCao/Lihang&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/SmirkCao/Lihang&lt;/a&gt; （笔记）&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.jiqizhixin.com/articles/2019-11-11-15&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.jiqizhixin.com/articles/2019-11-11-15&lt;/a&gt; （课件）&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/fengdu78/lihang-code&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/fengdu78/lihang-code&lt;/a&gt; （代码）&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="统计学习方法" scheme="https://wenhui-zhou.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>cs224N word vector I</title>
    <link href="https://wenhui-zhou.github.io/2020/01/09/cs224N-word-vector-I/"/>
    <id>https://wenhui-zhou.github.io/2020/01/09/cs224N-word-vector-I/</id>
    <published>2020-01-09T06:29:54.000Z</published>
    <updated>2020-01-12T16:30:57.842Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章是CS224N winter2019的第一次课的内容，主要针对词的表示，词向量生成的一些方法，从最早到最新，列举了这些算法以及应用。</p><a id="more"></a><h3 id="NLP任务的特殊性"><a href="#NLP任务的特殊性" class="headerlink" title="NLP任务的特殊性"></a>NLP任务的特殊性</h3><p>说道NLP任务的特殊性，其实是针对图像任务来说的。对于图像任务来说，我们所看到的场景通常直接反映了图像中所包含的内涵，同时图像上像素点通常是连续的，对于一个可微分系统求最优化的过程（深度学习的内涵就是构建一个可微分的系统），是天然切合的。</p><p>我们知道NLP最小的单元是词，由众多的词组成句子，形成语义。<strong>对于一个词来说，它在形状上不表示任何的含义，而是指代一种抽象的含义</strong>，因此在理解词含义的时候，我们需要结合上下文，结合语境，诸如LSTM，transformer这种能够获得较长上下文信息的结果在NLP中起到巨大的作用。此外，还存在着大量的一词多义的情形，因此具体的语境，上下文对理解NLP任务显得至关重要。</p><p>另一个显著的不同在于图像色彩在变化上是连续的，而NLP任务中，每个词之间是离散的，不存在连续的关系。因此我们需要对每次词进行适当的编码，转化成连续的词向量的形式。通过这种方式最优化我们的模型，得到NLP任务的解。</p><h3 id="word-vector"><a href="#word-vector" class="headerlink" title="word vector"></a>word vector</h3><p>如何用向量表示词，一个重要的指标是词向量能够反映词之间的相似性和相异性。例如苹果和巧克力，应当比苹果和梨之间的距离要大。下面是自己中词的表示方式：</p><ul><li><strong>同义词的字典：</strong>一个比较直观的解决方法是建立一个同义词的字典，将所有的同义词归类，用字典的方式来表示词，但是这种方式有缺点，我们无法涵盖所有的单词的含义，需要人去维护字典。</li><li><strong>one-hot方式：</strong>采用onehot方式，奖励一个大小为vocab size的向量，在该词的位置为1，其他为0。这种方式的缺点是，需要维护一个巨大的矩阵，同时每个 词之间都是独立的，无法表示不同词的相似性。</li><li><strong>一个词的含义由它周围的词决定：</strong>即一个词的含义应该与他的上下文（fix-window）所决定的。这种利用周围的词来表示当前的词的方式称为词嵌入word-embedding，它实际上是一种分布。</li></ul><p><img src="/images/nlp//image-20200112210153920.png" alt="image-20200112210153920" style="zoom:40%;"></p><h3 id="word-vector-1"><a href="#word-vector-1" class="headerlink" title="word vector"></a>word vector</h3><p>2013年由Mikolov提出，提出利用周围周围的词来预测当前的词：</p><ul><li>一个很大的语料库</li><li>每一个词表示表示成 一个词向量</li><li>遍历句子中的每一个词c</li><li>利用每一个c周围的词（fix-window），来计算c的相似性</li><li>通过迭代，最大化预测c的概率</li></ul><p><img src="/images/nlp//image-20200112214809724.png" alt="image-20200112214809724" style="zoom:50%;"></p><p>构造损失函数如下：</p><p><img src="/images/nlp//image-20200112215013622.png" alt="image-20200112215013622" style="zoom:40%;"></p><p>利用贝叶斯公式，利用前后m个位置的词来预测当前位置词的概率。构造一个负log似然函数，即交叉熵函数，通过最小化交叉熵损失，预测正确的概率最大。</p><p>其中每个词概率的计算p：</p><p><img src="/images/nlp//image-20200112215844822.png" alt="image-20200112215844822" style="zoom:40%;"></p><p>因此对于每一个词来说，同时有两个向量来表示：</p><p><img src="/images/nlp//image-20200112224851890.png" alt="image-20200112224851890" style="zoom:40%;"></p><p>在词的含义由周围词决定的思想下，有着两种模型的变种，一种是CBOW，另一种是skip-gram。</p><h3 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h3><p>Continuous Bag of Words Model,连续的词包模型，将句子挖空，利用其它词来预测当前的词。每个词学习两个向量：</p><ul><li>v：when the word is in the context</li><li>u：when the word is in the center</li></ul><p><img src="/images/nlp//image-20200112230514347.png" alt="image-20200112230514347" style="zoom:50%;"></p><p>如上，我们输入一个长度为字典词数长度的词（可以使onehot形式），然后学习一个从输入到隐变量的一个映射，然后在学一个隐变量到输出的映射。输入时句子中出了挖空的那个单词。最后通过softmax变成概率之后，最优化交叉熵损失，得到最优的结果。</p><h3 id="skip-gram-model"><a href="#skip-gram-model" class="headerlink" title="skip gram model"></a>skip gram model</h3><p>这个模型的思路和CBOW相反，利用一个单词去预测其他周围位置的单词，网路结构如下：</p><p><img src="/images/nlp//image-20200112231300222.png" alt="image-20200112231300222" style="zoom:50%;"></p><p>输入为一个词的onehot，学习两个映射过程，最终得到其周围fix-window内的其他词的预测结果，其过程与CBOW类似。</p><p>上述的两个模型中，均是三层结构，隐藏层维度为我们希望的词向量的长度，从输入到隐藏层之间的映射参数即为词典长度x词向量长度的一个矩阵，即我们所要的wordvec。</p><h3 id="negative-sampling"><a href="#negative-sampling" class="headerlink" title="negative sampling"></a>negative sampling</h3><p>在训练神经网络时，每当接受一个训练样本，然后调整所有神经单元权重参数，来使神经网络预测更加准确。换句话说，每个训练样本都将会调整所有神经网络中的参数。<br> 我们词汇表的大小决定了我们skip-gram 神经网络将会有一个非常大的权重参数，并且所有的权重参数会随着数十亿训练样本不断调整。</p><p>negative sampling  每次让一个训练样本仅仅更新一小部分的权重参数，从而降低梯度下降过程中的计算量。<br> 如果 vocabulary 大小为1万时， 当输入样本 ( “fox”, “quick”) 到神经网络时， “ fox” 经过 one-hot 编码，在输出层我们期望对应 “quick” 单词的那个神经元结点输出 1，其余 9999 个都应该输出 0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们为 negative word.   negative sampling 的想法也很直接 ，将随机选择一小部分的 negative words，比如选 10个 negative words 来更新对应的权重参数。（选择onehot中为0的一部分数参与模型的更新。）</p><p>在论文中作者指出指出对于小规模数据集，建议选择 5-20 个 negative words，对于大规模数据集选择 2-5个 negative words.</p><p>如果使用了 negative sampling 仅仅去更新positive word- “quick” 和选择的其他 10 个negative words 的结点对应的权重，共计 11 个输出神经元，相当于每次只更新 300 x 11 = 3300 个权重参数。对于 3百万 的权重来说，相当于只计算了千分之一的权重，这样计算效率就大幅度提高。</p><p><strong>如何选择负样本</strong></p><p>一个词是否会被选择为负样本与其出现的频率有关，出现的频次越高，越容易被当成负样本：</p><p><img src="/images/nlp//image-20200112233855262.png" alt="image-20200112233855262" style="zoom:50%;"></p><p>经过实验，当这个数为3/4的时候，模型表现最好。</p><h3 id="hierarchical-softmax"><a href="#hierarchical-softmax" class="headerlink" title="hierarchical softmax"></a>hierarchical softmax</h3><p>层次softmax是另一种优化模型计算的方法，他的思路借鉴了huffman树的思想，出现越是频繁的树，他出现的位置越浅。哈夫曼树每一个叶子节点表示一个类别，每个非叶子节点需要做一次二分类，走左边后走右边的概率用逻辑回归来表示：</p><p><img src="/images/nlp//image-20200113002246268.png" alt="image-20200113002246268" style="zoom:50%;"></p><p>如何降低复杂度呢，当k为词个数时,h为维度，可以将复杂度降到 $O(hlog_2(k))$。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>以上便是wordvec的表示方法，即解决了NLP的一个最基本，最重要的问题，词的表示。我们利用CBOW，skip-gram等模型，在优化模型的过程中，得到了词到向量的一个映射关系，这个关系就是wordvec的一个映射矩阵，有了这个矩阵我们就可以将词转化成向量。此时向量是根据其前后的单词而产生的，因此此位置的单词是根据前后的词而产生。下面一篇post将介绍wordvec更深层的东西，wordsence。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章是CS224N winter2019的第一次课的内容，主要针对词的表示，词向量生成的一些方法，从最早到最新，列举了这些算法以及应用。&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="https://wenhui-zhou.github.io/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>2019！2020！</title>
    <link href="https://wenhui-zhou.github.io/2020/01/01/2019%EF%BC%812020%EF%BC%81/"/>
    <id>https://wenhui-zhou.github.io/2020/01/01/2019！2020！/</id>
    <published>2020-01-01T03:34:18.000Z</published>
    <updated>2020-01-01T03:35:58.848Z</updated>
    
    <content type="html"><![CDATA[<p>2019年的最后几天，我在想给这一年打上一个什么样的标签，让我十几二十年后还能想起来这一年。确实是，我记不清过去的二十几年，唯一的感触剩下时间匆匆。</p><p>2019年1月份，和xu在北大三教的咖啡厅搭了这个网站，与此同时进行的还有刷算法题，准备着一些东西。再往前推几天，我匆匆完成了研一上最后的考试，连夜收拾行李进城。和yingying在雁栖湖有很多美好的回忆，可惜那些事情发生在2018年了，2018年对我来说是煎熬的一年，有一种引力阻止我思考里头究竟发生了什么事情。不过最令我开心的是在2018年结束的时候，我收获了很多宝贵的友谊。</p><p>所以我想，我不能在失去2019年了。</p><p>让我努力回忆几件事情，把一年的生活串起来。</p><p><strong>第一件事情</strong>，2019年新年在2月5号，过完春节我提前了两周时间回到雁栖湖。北京大雪后不久，校园里寂静的景象让我震惊，整个雁栖湖被雪裹住，我拖着行李箱，踏出一条雪路。接下来一个月我把时间安排的满满当当，这段时间，自我怀疑、喝鸡汤、再次怀疑循环往复。这段经历给我的启发很多，我愿意把它写出来：</p><ul><li>制定一个目标每时每刻努力着，这个过程是非常充实且令人满足的。结果反而是其次</li><li>目标明确的好处是知道自己要干什么，坏处是焦虑越来越严重</li></ul><p><strong>第二件事，</strong>我开始慢慢窥探自己的性格。事情讲起来很琐碎，我意识到自己在很多不确定的事情面前，显得唯唯诺诺，不敢承担责任。和老板聊到这件事情让我感触颇深，也对老板充满了感激。我在此总结一下导师给我的帮助：一个人自信与否在于他看待问题的角度，林彪认为一场战役七层胜算则可以打，粟裕有四层胜算便胸有成竹，侃侃而谈。</p><p>所以我大可不必这样的担心，也由此我摆脱了大部分的焦虑。后来才意识到，突破自己是困难的，很大一部分原因是无法找到问题的关键。此刻我很需要这样的信心，我也感到自己有了长足的进步。给人以正能量，给人以信心！</p><p>另一方面是自我意识的觉醒，顺从应当是我从小到大的习惯，当我发现我讨厌这样的自己，我会刻意去做一些改变，我会愿意决定一些事情，事先想好事情的应对方案。在这件事情上我还不够好。</p><p><strong>第三件事，</strong>生活需要理想，信仰。如果物质追求成为生活的目标时，生活是沉重且乏味的。而后读了《乔布斯传》，发现在物质之上，有着一群人，在追逐着梦想，渴望改变世界，追求着精神上更美好的东西。这对我的冲击是巨大的，我被这些浅显的道理击中，在回头看时，发现这些东西对我是多么的重要。想起来一次去参观腾讯，晚上8点和朋友坐班车回来，看到眼前拥堵的公路，熙熙攘攘的车子仿佛看到了日后无数个上下班的日日夜夜 ，备受打击。但是回头一想，你每天为着心中的理想奋斗，这是多么振奋人心的事情啊！</p><p>嗯，人生是伟大的理想，是诗和远方。</p><p><strong>最后</strong>，以上三件事情能够大致的勾勒出2019的轮廓，也许到我人生快结束的时候，也会有三件事情，勾勒我的一生。毕竟和时间比起来我们都太渺小。2019年很多时候在忙碌，但是真正对我有推动作用的事情却很少，最后还希望记录一些感悟：</p><ul><li>没有一个夜晚允许我们有一点软弱</li><li>永远不要掉队</li><li>人生应当有更高的追求和理想</li><li>相信积累的力量</li><li>阅读能够拯救我的内心，让我体会到真正的快乐</li><li>清醒的认识自己</li><li>展现积极的一面，永远是自己给别人信心</li></ul><p><strong>最后的最后，2020，happy new year!</strong></p><p><strong>2019/12/31</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;2019年的最后几天，我在想给这一年打上一个什么样的标签，让我十几二十年后还能想起来这一年。确实是，我记不清过去的二十几年，唯一的感触剩下时间匆匆。&lt;/p&gt;
&lt;p&gt;2019年1月份，和xu在北大三教的咖啡厅搭了这个网站，与此同时进行的还有刷算法题，准备着一些东西。再往前推几
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>GPT</title>
    <link href="https://wenhui-zhou.github.io/2019/12/29/GPT/"/>
    <id>https://wenhui-zhou.github.io/2019/12/29/GPT/</id>
    <published>2019-12-29T13:44:32.000Z</published>
    <updated>2019-12-29T14:09:08.121Z</updated>
    
    <content type="html"><![CDATA[<p>GPT在bert出现之前就与我们见面了，它利用了transformer的结构，在众多的任务中，取得了比较好的成绩。GPT的核心思想是 <strong>通过无标签的文本去训练生成语言模型</strong>，根据具体的NLP任务，利用新的数据进行finetune（和bert简直一模一样）。</p><a id="more"></a><h3 id="模型的结构"><a href="#模型的结构" class="headerlink" title="模型的结构"></a>模型的结构</h3><p>模型的结构是使用了多层的单向transformer结构，如下图：</p><p><img src="/images/nlp/v2-fb7be8467a0231607f3f2e4ace92534e_hd.jpg" alt="img"></p><p>GPT即gerneral pre-training 通用预训练语言模型，是一种利用transformer作为特征抽取器，基于语言模型进行训练的预训练语言模型。因此GPT有两个重点，一个是语言模型，另一个是transformer。</p><p><strong>语言模型</strong></p><p>一个语言模型通常可以理解成一句话出现的概率 p(W) 的计算。语言模型利用语料进行训练，训练的目的就是：<strong>计算某个句子出现的概率。</strong>一个句子的概率的计算方式如下：</p><p>对于一个有T个词按顺序构成的句子，P(W)实际上求解的是字符串的<strong>联合概率</strong>，利用贝叶斯公式，链式分解如下：<br>$$<br>\begin{aligned}<br>P\left(W_{1}^{T}\right) &amp;=P\left(\mathrm{w}_{1}, w_{2}, \ldots, w_{T}\right) \\<br>&amp;=P\left(w_{1}\right) P\left(w_{2} | w_{1}\right) P\left(w_{3} | w_{1}, w_{2}\right) \ldots P\left(w_{T} | w_{1}, w_{2}, \ldots w_{T-1}\right)<br>\end{aligned}<br>$$<br>从上面可以看出来，一个统计语言模型可以表示成给定前面的词，求后面的一个词出现的条件概率。当我们在求P(w)的时候，我们就已经建立了一个模型，这里的诸多条件概率就是模型的参数。GPT预训练过程就是利用语料，构造训练数据，利用上述语言模型，不断预测，学习参数的过程。</p><p><strong>GPT结构</strong></p><p>GPT的结构由12个transformer组成，和bert的一个最大的不同在于，他的transformer是单向的，输入为文本token和position信息结合一起后embedding。输入经过12个transformer结果处理之后，到输出层，经过一个lienar层之后，使用softmax进行初始化，随后使得似然最大化（交叉熵的负数最大），得到最后的结果：<br>$$<br>P\left(y | x^{1}, \ldots, x^{m}\right)=\operatorname{softmax}\left(h_{l}^{m} W_{y}\right)<br>$$<br>其中$h_l^m$表示12个transformer输出的结果，W表示linear层的参数，最大化似然，即得到最终的结果：<br>$$<br>L_{2}(\mathcal{C})=\sum_{(x, y)} \log P\left(y | x^{1}, \ldots, x^{m}\right)<br>$$<br><strong>如何使用GPT</strong></p><p>GPT训练过程分为两步，第一步在一个非常大的数据集上进行数据的无监督训练，第二阶段，我们在一个很小的数据集上对模型进行finetune，使用有监督的方法解决特定方向的问题。通过大量的实验，作者发现，只要经过很小的fintune就可以应用到很多不同的任务上。</p><p>第一步使用无监督的方式学习语言本身存在的相关性，训练方式是单向的，模型从左到有，利用已经出现的词，来预测下一个位置上的词。和bert不同，bert使用mask技术，能够学到语言中双向的特征。</p><p>之所以使用unsupervised learning的原因是，标注大量的数据的成本是很高的，同时人工标注的难度也很大。使用无监督的方式可以学到数据之间的相关性。</p><p><strong>GPT和下游任务的结合</strong></p><p><img src="/images/nlp/gpt.jpg" alt="img"></p><p>使用GPT作为pretraining模型，第一步将数据转化成序列结构，如上图输入的组合方式，然后通过修改输出部分的结构，达到网络在多个特定任务上的应用：</p><ul><li>For text classification, we can directly fine-tune the model.</li><li>For textual entailment, we concatenate the premise and hypothesis token sequences, with a delimiter token in between.</li><li>For similarity tasks, since there is no inherent ordering of the two sentences being compared, the input sequence is modified to contain both possible sentence orderings. Each input sequence is processed independently to produce two sequence representations, which are finally added element-wise before being fed into the linear output layer.</li><li>For question answering and commonsense reasoning, we are given a context document zz, a question qq, and a set of possible answers akak. We concatenate the document context and question with each possible answer as [start;z;q;delim;ak;extract]. Each of these sequences are processed independently and then normalized via a softmax layer to produce an output distribution over possible answers.</li></ul><p>上面主要介绍的是输入的组合方式。</p><p><img src="/images/nlp/difference.jpg" alt=""></p><p><strong>bert下游任务</strong></p><p><img src="/images/nlp/fine-tuning.jpg" alt=""></p><h3 id="GPT-2"><a href="#GPT-2" class="headerlink" title="GPT-2"></a>GPT-2</h3><p>GPT-2 是GPT-1的升级版本，它在文本生成上有着惊艳的表现，其<strong>生成的文本在上下文连贯性和情感表达上</strong>都超过了人们对目前阶段语言模型的预期。</p><p>GPT-2有着海量的训练数据，GPT-2结构采用了只有解码器的transformer架构，接下来我们将一起探索GPT-2的应用以及在生成任务中的奥秘。</p><p>语言模型的作用是根据已有的句子的一部分来预测下一个单词是什么。GPT-2使用transformer解码器模块构建，而bert则是通过transformer编码器模块构建，他们之间一个关键的不同在于GPT-2一次只输出一个单词，新的单词产生之后就被添加到单词序列中，作为下一个单词预测的输入。这种机制叫做自回归（auto-regression），诸如TransformerXL和XLNet都是用了自回归，XLNet同时还找到了一种能够同时兼顾前后的上下文信息的方法。</p><p>GPT-2对transformer模块进行了改造，仅仅选择了Decoder的部分：</p><p><img src="/images/nlp/640-1577377624300.webp" alt=""></p><p>即只允许看到当前预测位置左边的单词，transformer结构只允许片段的长度为512，而这种结构可以支持最长1024个单词序列。</p><p><strong>GPT-2的工作流程</strong></p><p>训练一个GPT-2模型，最简单的方法就是让他随机工作。我们随机选择一个单词作为起始单词让它根据我们提供得词生成一段样本（即生成交互式条件样本）。</p><p>此时模型的输入只有一个词，然后经过若干层的transformer，得到一个向量输出。然后将这个向量与词汇表中的每一个单词计算一个概率，我们选择概率最高的一个单词作为下一个单词。GPT-2的词汇表中有50000个单词。</p><p>有时选择top1的方法会出现问题，有时模型可能会陷入一直推荐同一个词的循环中，因此我们在选择下一个词的时候，通常我们从topk个词中按概率随机选择一个（torch.distribution.categorical.Categorical）。</p><p>接下来我们将新增的单词添加在序列的尾部，作为预测下一个单词的输入，GPT-2网络的参数保留了对第一个单词的解释，然后根据这些信息来生成第二个单词。在向后生成的过程中，GPT-2已生成的单词将不会被改变。</p><h3 id="深入GPT-2内部"><a href="#深入GPT-2内部" class="headerlink" title="深入GPT-2内部"></a>深入GPT-2内部</h3><p><strong>输入编码</strong></p><p>GPT-2输入从词嵌入矩阵中查找单词对应的嵌入向量word embedding，该矩阵也是模型训练的一部分：</p><p><img src="/images/nlp/640-1577377615340.webp" alt=""></p><p>模型的大小和词向量的长度有关，词向量长度最小为768。此外我们还需要对输入序列的每一个位置都对应一个位置编码，这些编码矩阵也是训练参数的一部分：</p><p><img src="/images/nlp/640.webp" alt=""></p><p>因此在准备输入数据的时候，我们随机找到一个起始单词，然后根据起始单词，去vocab中转化成词向量，然后加上第一个位置上的位置编码：</p><p><img src="/images/nlp/640-1577377765194.webp" alt=""></p><p>随后序列进入transformer中，transformer中的自注意力机制将每个词与序列中的相关性学习到，体现了transformer超强的特征提取能力。</p><p>对于自注意力机制来说，存在三个矩阵，Key，Query，Value，首先将这三个矩阵与输入相乘，然后得到K，Q，V，可以认为接下来就是找到相互匹配的K，Q（query和key的匹配对），即softmax的值最高，然后乘以V，得到当前词对序列中其他词的相关性的大小：</p><p><img src="/images/nlp/640-1577378601211.webp" alt="img"></p><p><strong>模型输出</strong></p><p>当最后一个transformer输出向量之后，这个向量本质上就会预测词的词向量。我们将这个词向量乘上词嵌入矩阵，得到一个长度为词汇表长度的向量。这个向量每一个位置就表示当前预测位置上的词对词汇表中该位置词的相似概率。</p><p><img src="/images/nlp/640-1577378813143.webp" alt="img"></p><p>然后我们现在top-k（40）个概率，然后从topk中，按照概率大小，选择出一个单词作为当前预测的单词。</p><p><img src="/images/nlp/640-1577378918869.webp" alt="img"></p><p>重复上述的过程，直到生成1024个词，遇到终止符。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;GPT在bert出现之前就与我们见面了，它利用了transformer的结构，在众多的任务中，取得了比较好的成绩。GPT的核心思想是 &lt;strong&gt;通过无标签的文本去训练生成语言模型&lt;/strong&gt;，根据具体的NLP任务，利用新的数据进行finetune（和bert简直一模一样）。&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="https://wenhui-zhou.github.io/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>finetune-诗句生成</title>
    <link href="https://wenhui-zhou.github.io/2019/12/29/finetune-%E8%AF%97%E5%8F%A5%E7%94%9F%E6%88%90/"/>
    <id>https://wenhui-zhou.github.io/2019/12/29/finetune-诗句生成/</id>
    <published>2019-12-29T13:35:47.000Z</published>
    <updated>2019-12-29T13:42:42.141Z</updated>
    
    <content type="html"><![CDATA[<p>原先的诗句生成模型仅仅利用了bert language model，而没有去利用诗句语料进行finetune，尽管language model生成的结果已经令人比较满意了。但是我们想知道在特定的数据集上进行finetune的话，结果是否能得到提升，于是这篇post主要完成这个工作：continue training。</p><a id="more"></a><h3 id="pytorch-多卡分布式训练"><a href="#pytorch-多卡分布式训练" class="headerlink" title="pytorch 多卡分布式训练"></a>pytorch 多卡分布式训练</h3><p>关于分布式训练，通常可以使用<strong>DataLoader</strong>，这个wrapper可以方便使用多张卡，而进程只有一个，唯一的问题是这个方法只能满足一台计算机上GPU的通信，对需要使用多个机器，多个GPU的任务无能为力。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = nn.DataParallel(net)</span><br></pre></td></tr></table></figure><p>以上代码将整个网络分布到多个GPU上。pytorch定义的网络模型参数默认放在GPU 0上，所以dataparallel的时候，实质上是把训练参数从gpu靠背到其他的gpu上同时训练。此时dataloader加载数据的时候，batch_size需要设置成原来batch的n倍，n为gpu的数量。</p><p>如果我们要使用多个机器上的GPU，pytorch依然提供了办法：</p><ul><li><code>torch.utils.parallel.DistributedDataParallel</code>方法：与dataloader类似，用来实现多机多卡分布训练，他可实现在不同机器的多个模型拷贝之间的平均梯度</li><li><code>torch.utils.data.distributed.DistributedSampler</code> 方法：在多机多卡的情况下，每个卡读取的数据显然是不同的，dataparallel的做法是直接将batch切分到不同的卡上。对于多机来说，直接进行数据传输将会耗费很多时间，于是使用distributedSampler，确保每一个dataloader只会load到整个数据集的一个特定子集，避免不同进程之间数据重复</li></ul><p><strong>使用方法</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataloader,Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.distributed <span class="keyword">import</span> DistributedSampler</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel</span><br><span class="line"></span><br><span class="line">dataset = your_dataset()</span><br><span class="line">datasample =DistributedSampler(dataset,num_replicas=world_size,rank = rank)</span><br><span class="line">dataloader = Dataloader(dataset,batch_size=batch_size_per_gpu,sampler = datasampler)</span><br><span class="line">model = your_model()</span><br><span class="line">model = DistributedDataParallel(model,device_ids = [local_rank],output_device=local_rank)</span><br></pre></td></tr></table></figure><p>在设置dataloader的batch-size的时候，只需要设置单卡的batch-size即可。world_size指进程总数，就是卡的数量，rank是进程编号，local_rank指本地序号。要想使用DistributedDataParallel就需要先完成多进程的初始化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.distributed.init_process_group()</span><br></pre></td></tr></table></figure><p><img src="/images/nlp/image-20191224122842203.png" alt=""></p><p><img src="/images/nlp/image-20191224122817322.png" alt="image-20191224122817322"></p><p><img src="/images/nlp/image-20191224123807016.png" alt="image-20191224123807016"></p><p><img src="/images/nlp/image-20191224123750444.png" alt="image-20191224123750444"></p><p><img src="/images/nlp/image-20191224124839172.png" alt="image-20191224124839172"></p><p><strong>梯度积累</strong></p><p>梯度累加的步骤如下：</p><ol><li>获取loss，输入图像和标签，通过infer计算得到预测值，计算损失函数</li><li><code>loss.backward()</code> 反向传播，计算当前梯度</li><li>多次循环1-2，不清空梯度，使梯度累加在已有的梯度上</li><li>梯度累加了一定的次数以后，先<code>optimizer.step()</code>根据累计的梯度更新网络参数，然后通过<code>optimizer.zero_grad()</code> 清空过往的梯度，为下一波梯度累加做准备</li></ol><p>梯度累加总结来说就是每次获取一个batch吼不清空梯度，而是累加到一定程度的时候去清空梯度，变相的相当于扩大了batchsize，同时可以避免计算多个损失函数时，存储多个计算图。</p><p><strong>pytorch采样器（dataloader）</strong></p><p>pytorch在加载数据的时候提供了一个sampler模块，这个模块用来对数据进行采样，常用的采样器有<code>RandomSampler</code>，当dataloader中shuffle的参数为true的时候，系统会自动调用这个采样器。dataloader默认使用的采样器为sequentialSampler，即按顺序来进行采样。sampler组织好数据的下标后，在dataloader中将数据取出来。</p><p><strong>pytorch and apex</strong></p><p>pytorch在分布式训练上存在着一些问题：</p><ul><li>混合精度训练难以收敛：pytorch可以方面得将模型转换成fp16，但是训练batchnorm层的时候，又需要转成f32，导致了混合精度，难以优化的问题。</li><li>bn同步的问题，bn同步能够极大的加快模型的收敛，精度也会有所提升，原生的方法一直未能较好地解决</li></ul><p>apex是NVIDIA维护的一个支持</p><p><strong>bert优化器</strong></p><p>bert常使用的优化器有BertAdam，AdamW，FusedAdam（和BertAdam类似，当用到apex时配套当做优化器使用）。Bert的优化器和传统的Adam优化器有什么不同呢，主要的不同有以下两点：</p><ul><li>bertAdam/AdamW 能够固定权重衰减，可用于微调模型</li><li>bertAdam/AdamW 不会对偏差bias进行补偿</li></ul><p><strong>bert loss function</strong></p><p>bert损失函数主要由两部分组成，第一部分来自Mask-LM的单词级别分类任务，另一部分是句子级别的分类任务。通过联合学习，使bert学习到语言中token级别，以及句子级别的语义信息，具体的损失函数如下：</p><p>第一部分的损失函数是mask部分的词的一个多分类问题，词典的大小为分类的大小。第二部分的损失函数是是否是下一句的二分类问题，两个分类函数结合作为最后的loss.</p><p><strong>bert激活函数：gelu</strong></p><p>gelu：高斯误差线性单元，是一种高性能的神经网络激活函数，GELU的非线性变换是一种符合预期的随机正则变换方式：</p><p><img src="/images/nlp/image-20191224220030781.png" alt="image-20191224220030781"></p><p><strong>总结</strong></p><p>总结一下上面的工作，我基本上了解了生成诗句的一套流程，对bert的结构也有了比较多的了解。总体来说，代码比较容易看懂，这比起C++的代码，简直轻松很多。</p><p>还可以梳理一下bert的脉络，研究一下whole word marking的实现方法。明后两天把这个事情做完。其他在按计划慢慢推进！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原先的诗句生成模型仅仅利用了bert language model，而没有去利用诗句语料进行finetune，尽管language model生成的结果已经令人比较满意了。但是我们想知道在特定的数据集上进行finetune的话，结果是否能得到提升，于是这篇post主要完成这个工作：continue training。&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="https://wenhui-zhou.github.io/categories/NLP/"/>
    
    
  </entry>
  
</feed>
