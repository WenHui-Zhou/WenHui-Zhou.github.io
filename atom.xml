<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>WenHuiZhou</title>
  
  <subtitle>perper（打起精神！）</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://wenhui-zhou.github.io/"/>
  <updated>2019-10-16T04:53:17.349Z</updated>
  <id>https://wenhui-zhou.github.io/</id>
  
  <author>
    <name>WenHuiZhou</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>如何读论文</title>
    <link href="https://wenhui-zhou.github.io/2019/10/16/%E5%A6%82%E4%BD%95%E8%AF%BB%E8%AE%BA%E6%96%87/"/>
    <id>https://wenhui-zhou.github.io/2019/10/16/如何读论文/</id>
    <published>2019-10-16T02:14:49.000Z</published>
    <updated>2019-10-16T04:53:17.349Z</updated>
    
    <content type="html"><![CDATA[<p>对于近期在读论文上效率比较低，读完收获比较小的问题，我从知乎上找到了一些比较好的读论文，做笔记的方法，在这里记录一下。</p><a id="more"></a><h2 id="读论文"><a href="#读论文" class="headerlink" title="读论文"></a>读论文</h2><h4 id="筛选论文，确定是否值得读"><a href="#筛选论文，确定是否值得读" class="headerlink" title="筛选论文，确定是否值得读"></a>筛选论文，确定是否值得读</h4><blockquote><p>feel free to stop reading the article at any point</p></blockquote><ol><li>拿到一篇论文先看<strong>论文题目，keywords</strong>，若你不感兴趣，you stop</li><li>阅读 <strong>abstract</strong>，你可以快速地了解整篇文章。</li><li><p>阅读 <strong>conclusion</strong>，从结论中你可以看出来这篇文章是否和你研究的问题相关。</p></li><li><p>阅读 论文 <strong>图片，表格，标题</strong>，从这些地方你可以花很少的时间，搞清楚作者是如何做这项工作的</p></li></ol><p>到这里，如果你认为这篇文章还可以继续的话，你就可以接着往下进行了。</p><h4 id="精度论文"><a href="#精度论文" class="headerlink" title="精度论文"></a>精度论文</h4><ol><li>阅读 <strong>introduction</strong>，该部分你将读到整个文章的背景，以及作者做这篇文章的主要目的。</li><li>文章的最重要的核心是 <strong>the result and the discussion</strong>，你应该在这上面花主要的时间，如果你觉得差不多了，你就可以停止了。</li><li>但是你如果觉得这篇文章和你的工作的相关性比较大的话，那你就应该dig extremely into the <strong>experience section</strong>。在这一部分你就可以清楚的知道作者是如何做这件事的。</li><li>当你阅读完这篇这篇论文的时候，适当的做些笔记，这些笔记将会在你未来的研究中为你省下很多的时间。</li></ol><h4 id="读论文的一些tips"><a href="#读论文的一些tips" class="headerlink" title="读论文的一些tips"></a>读论文的一些tips</h4><ol><li>参考文献中信息很多，可以花时间找一些参考文献中的文章是否和你当前的问题相关，减少调研的工作量。</li><li>关注近五年的文献。</li><li>关注核心期刊、会议以及一些学科大牛。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于近期在读论文上效率比较低，读完收获比较小的问题，我从知乎上找到了一些比较好的读论文，做笔记的方法，在这里记录一下。&lt;/p&gt;
    
    </summary>
    
    
      <category term="tool" scheme="https://wenhui-zhou.github.io/tags/tool/"/>
    
  </entry>
  
  <entry>
    <title>resume detail 2019/10/15</title>
    <link href="https://wenhui-zhou.github.io/2019/10/15/resume-detail/"/>
    <id>https://wenhui-zhou.github.io/2019/10/15/resume-detail/</id>
    <published>2019-10-15T06:29:43.000Z</published>
    <updated>2019-10-16T05:59:55.417Z</updated>
    
    <content type="html"><![CDATA[<script src="//cdn.bootcss.com/jquery/1.11.3/jquery.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="welcome to my blog,enter password to read." />    <label for="pass">welcome to my blog,enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">U2FsdGVkX19ZdXvzzL7/HetZMd7W+3b5zq6TxPIfZICFtmemVpxUD8+2zyFGlxwAcxh+22F2Mki2c9gaHZmOsRMYt8FY0rkyU2EQzQiMP321LLCkGALEcc9wyN3m6Hi8AYaXNFnkGp3FxmT9f8ohoNMkyoY2BB78AFC5GUEPVhU=</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      welcome to my blog,enter password to read.
    
    </summary>
    
    
      <category term="项目总结" scheme="https://wenhui-zhou.github.io/tags/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>DeepMVS:Learning Multi-view Stereopsis</title>
    <link href="https://wenhui-zhou.github.io/2019/10/14/DeepMVS-Learning-Multi-view-Stereopsis/"/>
    <id>https://wenhui-zhou.github.io/2019/10/14/DeepMVS-Learning-Multi-view-Stereopsis/</id>
    <published>2019-10-14T06:48:27.000Z</published>
    <updated>2019-10-14T08:40:17.192Z</updated>
    
    <content type="html"><![CDATA[<p>该篇论文通过一系列的图像，生成这些图像所对应的深度信息。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;该篇论文通过一系列的图像，生成这些图像所对应的深度信息。&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="3D重建" scheme="https://wenhui-zhou.github.io/tags/3D%E9%87%8D%E5%BB%BA/"/>
    
  </entry>
  
  <entry>
    <title>State of Art on 3D Reconstruction with RGB-D Cameras 三维重建综述</title>
    <link href="https://wenhui-zhou.github.io/2019/10/10/State-of-Art-on-3D-Reconstruction-with-RGB-D-Cameras-%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E7%BB%BC%E8%BF%B0/"/>
    <id>https://wenhui-zhou.github.io/2019/10/10/State-of-Art-on-3D-Reconstruction-with-RGB-D-Cameras-三维重建综述/</id>
    <published>2019-10-10T02:31:17.000Z</published>
    <updated>2019-10-14T01:57:37.838Z</updated>
    
    <content type="html"><![CDATA[<p>这篇论文是发表在欧洲计算机图形学协会2018上的一篇综述文章，下面将精读这篇论文，对其中的重点内容进行记录。</p><a id="more"></a><h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><p>近些年来，基于结构光（structure light）和TOF（time of flight）方法的深度相机得到了大规模的商用，很多基于RGB-D数据的三维重建算法达到了很好的重建效果。一些具有创新性的方法得到了发展、一些基于RGB-D用于还原3D结构的方法、一些基于RGB-D研究物体其他属性的方法（材料，反射模型）也相继被提出。</p><h4 id="RGB-D-cameras-and-their-characteristics"><a href="#RGB-D-cameras-and-their-characteristics" class="headerlink" title="RGB-D cameras and their characteristics"></a>RGB-D cameras and their characteristics</h4><p>目前，深度距离检测上存在两种方法，一种为三角测距（triangulation），另一种为TOF（time of flight），三角测距可以是被动式（立体视觉），也可以是主动式（结构光）。stereo vision方法计算两张不同角度的照片的差异，而结构光同样是发射红外线，通过分析红外线的扭曲程度三角测量的方式得到深度信息。TOF方法则是通过发射红外光，通过测量接收到反射光的时间来判断物体的深度信息。</p><h4 id="static-Scene-Reconstruction（静态场景重建）"><a href="#static-Scene-Reconstruction（静态场景重建）" class="headerlink" title="static Scene Reconstruction（静态场景重建）"></a>static Scene Reconstruction（静态场景重建）</h4><p> 在线的静态场景重建直接相关的技术有SLAM（simultaneous Localization and Mapping），这个技术主要的关注点在于在未知环境中机器人的导航，主要针对离散稀疏的点云建模。另一方面，静态稠密点云的重建也引起很大的关注。</p><p>在线重建的发展使得一些如kinect Fusion算法，possion surface reconstruction（柏松重建算法）的研究成为一个热门的方向。</p><h4 id="静态场景重建的基础pipeline"><a href="#静态场景重建的基础pipeline" class="headerlink" title="静态场景重建的基础pipeline"></a>静态场景重建的基础pipeline</h4><p>第一步：深度图的预处理，噪声的消除，外部（outlier）信息的移除这些处理方法会首先对RGB-D数据进行处理。</p><p>第二步：从输入的深度图序列中提取出额外的信息，存储起来。</p><p>第三步：相机位姿的估计以及转换矩阵T的估计</p><p>第四步：深度图的融合，将所有计算出的点融合到模型M中。</p><h4 id="深度图的预处理"><a href="#深度图的预处理" class="headerlink" title="深度图的预处理"></a>深度图的预处理</h4><p>深度图中噪声的长生由多种因素影响，常用的方法有使用双边滤波的方式（bilateral filter）来过滤噪声，此外对于一些特定的模型，姿态估计等等方法也会被使用。</p><h4 id="camera-pose-Estimate"><a href="#camera-pose-Estimate" class="headerlink" title="camera pose Estimate"></a>camera pose Estimate</h4><p>对每一张RGB-D图像，计算6-DOF pose T。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇论文是发表在欧洲计算机图形学协会2018上的一篇综述文章，下面将精读这篇论文，对其中的重点内容进行记录。&lt;/p&gt;
    
    </summary>
    
      <category term="论文阅读" scheme="https://wenhui-zhou.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="3D重建" scheme="https://wenhui-zhou.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/3D%E9%87%8D%E5%BB%BA/"/>
    
    
  </entry>
  
  <entry>
    <title>3D重建论文阅读</title>
    <link href="https://wenhui-zhou.github.io/2019/10/08/3D%E9%87%8D%E5%BB%BA%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>https://wenhui-zhou.github.io/2019/10/08/3D重建论文阅读/</id>
    <published>2019-10-08T02:38:39.000Z</published>
    <updated>2019-10-14T06:47:21.195Z</updated>
    
    <content type="html"><![CDATA[<p>本篇博客的主要目的是为了记录所读的有关于三维重建的文章，对每篇文章的insight进行简要的总结。</p><a id="more"></a><h3 id="State-of-Art-on-3D-Reconstruction-with-RGB-D-Cameras"><a href="#State-of-Art-on-3D-Reconstruction-with-RGB-D-Cameras" class="headerlink" title="State of Art on 3D Reconstruction with RGB-D Cameras"></a>State of Art on 3D Reconstruction with RGB-D Cameras</h3><p>该论文是发表在eurographics 欧洲计算机图形学协会2018上，对当前的RGB-D图像三维重建进行了一个综述整理。</p><p>这是明天的任务。</p><hr><h3 id="Underwater-3-D-Scene-Reconstruction-Using-Kinect-v2-Based-on-Physical-Models-for-Refraction-and-Time-of-Flight-Correction"><a href="#Underwater-3-D-Scene-Reconstruction-Using-Kinect-v2-Based-on-Physical-Models-for-Refraction-and-Time-of-Flight-Correction" class="headerlink" title="Underwater 3-D Scene Reconstruction Using Kinect v2 Based on Physical Models for Refraction and Time of Flight Correction"></a>Underwater 3-D Scene Reconstruction Using Kinect v2 Based on Physical Models for Refraction and Time of Flight Correction</h3><p>该论文被2017年IEEE access收录，论文主要的思路是搭建一个防水装置，将kinect v2放入水中，利用kinect v2来采集RGB图像以及深度图像。然后通过<strong>水下数据采集，相机矫正，噪声过滤，TOF矫正，反射矫正</strong>等步骤恢复深度数据，最后通过kinect Fusion等到三维重建后的效果。</p><ul><li><p>数据获取采集部分采用加入防水外壳的kinect v2。</p></li><li><p>水下滤波部分，在kinect fusion算法中，针对空气中的滤波采用bilinear filter，水下环境复杂，作者采用5 x 5的median中值滤波。</p></li><li>kinect TOF矫正，由于在水下红外线的传播速度与空气中传播的速度不同，因此需要对检测到的深度信息进行矫正。水中传播的距离需要根据水中的红外线传播的速度进行修正。</li><li>水下折射矫正，kinect v2捕捉到的图像、深度信息在水下存在一定程度上的偏移，因此需要进行水下的折射矫正。</li></ul><p>在三维恢复性能比较方面，作者采用物体的三维模型或者激光采集到的三维数据作为ground truth进行对比，得出性能的优劣。</p><p>2019/10/8</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇博客的主要目的是为了记录所读的有关于三维重建的文章，对每篇文章的insight进行简要的总结。&lt;/p&gt;
    
    </summary>
    
      <category term="3D重建" scheme="https://wenhui-zhou.github.io/categories/3D%E9%87%8D%E5%BB%BA/"/>
    
    
  </entry>
  
</feed>
