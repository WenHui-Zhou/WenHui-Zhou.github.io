<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>WenHuiZhou</title>
  
  <subtitle>perper（打起精神！）</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://wenhui-zhou.github.io/"/>
  <updated>2020-06-08T15:10:01.192Z</updated>
  <id>https://wenhui-zhou.github.io/</id>
  
  <author>
    <name>WenHuiZhou</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>面试试题准备</title>
    <link href="https://wenhui-zhou.github.io/2020/06/06/%E9%9D%A2%E8%AF%95%E8%AF%95%E9%A2%98%E5%87%86%E5%A4%87/"/>
    <id>https://wenhui-zhou.github.io/2020/06/06/面试试题准备/</id>
    <published>2020-06-06T13:22:20.000Z</published>
    <updated>2020-06-08T15:10:01.192Z</updated>
    
    <content type="html"><![CDATA[<p>面试之前的一些问题总结</p><a id="more"></a><h3 id="牛客网线上笔试"><a href="#牛客网线上笔试" class="headerlink" title="牛客网线上笔试"></a>牛客网线上笔试</h3><p><strong>数据的读入：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#数据输入格式为：</span></span><br><span class="line"><span class="comment"># 5</span></span><br><span class="line"><span class="comment"># 1 2</span></span><br><span class="line"><span class="comment"># 5 3</span></span><br><span class="line"><span class="comment"># 4 6</span></span><br><span class="line"><span class="comment"># 7 5</span></span><br><span class="line"><span class="comment"># 9 0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">num = sys.stdin.readline().strip()</span><br><span class="line">num = int(num)</span><br><span class="line">data = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num):</span><br><span class="line">  line = sys.stdin.readline().strip()</span><br><span class="line">  <span class="keyword">if</span> line == <span class="string">''</span>:</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">  line = [int(val) <span class="keyword">for</span> val <span class="keyword">in</span> line.split(<span class="string">' '</span>)]</span><br><span class="line">  data.append(line)</span><br><span class="line"><span class="comment">## 成功读到data</span></span><br></pre></td></tr></table></figure><p><strong>最终答案需要print输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(result)</span><br></pre></td></tr></table></figure><h3 id="collections库的用法"><a href="#collections库的用法" class="headerlink" title="collections库的用法"></a>collections库的用法</h3><p><strong>counter</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">count = Counter([<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>,<span class="string">'b'</span>,<span class="string">'a'</span>])</span><br><span class="line">print(count)</span><br><span class="line"><span class="comment"># Counter(&#123;'b': 3, 'a': 2, 'c': 1, 'd': 1&#125;)</span></span><br><span class="line">count.items()</span><br><span class="line"><span class="comment"># Out[6]: dict_items([('a', 2), ('b', 3), ('c', 1), ('d', 1)])</span></span><br><span class="line">list(count.elements())</span><br><span class="line"><span class="comment"># Out[10]: ['a', 'a', 'b', 'b', 'b', 'c', 'd']</span></span><br><span class="line">count.most_common()</span><br><span class="line"><span class="comment"># [('b', 3),('a', 2), ('c', 1), ('d', 1)]</span></span><br></pre></td></tr></table></figure><p><strong>defaultdict</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">dicto = defaultdict(list) <span class="comment"># 不需要判断元素是否已经存在key中，存入参数为values对应的类型</span></span><br><span class="line">dicto[<span class="string">'lei'</span>]</span><br><span class="line"><span class="comment"># dicto:&#123;'lei':[]&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;面试之前的一些问题总结&lt;/p&gt;
    
    </summary>
    
      <category term="面试准备" scheme="https://wenhui-zhou.github.io/categories/%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87/"/>
    
    
  </entry>
  
  <entry>
    <title>手势识别</title>
    <link href="https://wenhui-zhou.github.io/2020/06/04/%E6%89%8B%E5%8A%BF%E8%AF%86%E5%88%AB/"/>
    <id>https://wenhui-zhou.github.io/2020/06/04/手势识别/</id>
    <published>2020-06-04T13:17:50.000Z</published>
    <updated>2020-06-05T08:48:54.430Z</updated>
    
    <content type="html"><![CDATA[<script src="//cdn.bootcss.com/jquery/1.11.3/jquery.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="welcome to my blog,enter password to read." />    <label for="pass">welcome to my blog,enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      welcome to my blog,enter password to read.
    
    </summary>
    
      <category term="面试准备" scheme="https://wenhui-zhou.github.io/categories/%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87/"/>
    
    
  </entry>
  
  <entry>
    <title>常见的目标检测网络</title>
    <link href="https://wenhui-zhou.github.io/2020/05/27/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C/"/>
    <id>https://wenhui-zhou.github.io/2020/05/27/常见的目标检测网络/</id>
    <published>2020-05-27T03:17:51.000Z</published>
    <updated>2020-06-08T03:29:03.826Z</updated>
    
    <content type="html"><![CDATA[<p>总结常见的目标检测网络，持续更新，文章要写很长。</p><p>[TOC]</p><a id="more"></a><h3 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h3><p>目标检测任务可以理解为目标的识别和定位，识别指的是判断物体的类别，定位指的是确定物体的位置。</p><p>目前的目标检测模型分为两类，一类是two-stage，这一类的典型代表是RCNN，Fast-RCNN，Faster-RCNN家族。他们的特点是检测精度高，识别错误率低，但是速度较慢，不能满足实时场景的需求。另一类是one-stage，他们典型的网络有yolo，SSD，yolo2并且准确率能够和two-stage基本持平。</p><h3 id="RCNN-2014"><a href="#RCNN-2014" class="headerlink" title="RCNN(2014)"></a>RCNN(2014)</h3><p>RCNN详解：<a href="https://perper.site/2019/02/11/RCNN详解/" target="_blank" rel="noopener">https://perper.site/2019/02/11/RCNN%E8%AF%A6%E8%A7%A3/</a></p><p><strong>大致流程：</strong></p><ul><li>首先通过select search算法选择出图像中可能出现目标的box，最终得到2000个框<ul><li>将图像过分割出2k-3k个框</li><li>对框进行合并（颜色相近，梯度相近等）</li><li>输出曾经有出现过的框</li></ul></li><li>将输出的候选框依次输入到卷积网络中进行网络训练</li><li>将cf7层得到4096维特征输出到SVM中，训练N个SVM二分类器，对每个框进行二分类</li><li>边框回归：边框回归主要学习四个参数分别是x，y的偏移和w，h的缩放。通过优化二模和参数二次项，得到四个变换的参数，然后将参数乘以相应的边得到最终的边框位置（位移和缩放）</li></ul><p>RCNN的流程如下图：</p><p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200528011717296.png" alt="image-20200528011717296" style="zoom:50%;"></p><p>该网络的主要缺陷在于：</p><ul><li>候选框由select search算法产生，需要花费很长时间</li><li>对2k个候选框，均需要从头做卷积，存在很多的冗余计算</li><li>模型确定的情况只能接受固定大小的输入</li><li>网络训练过程分段，过程繁琐</li></ul><h3 id="Fast-RCNN-2015"><a href="#Fast-RCNN-2015" class="headerlink" title="Fast-RCNN(2015)"></a>Fast-RCNN(2015)</h3><p>Fast-RCNN详解：<a href="https://perper.site/2019/02/14/Fast-RCNN详解/" target="_blank" rel="noopener">https://perper.site/2019/02/14/Fast-RCNN%E8%AF%A6%E8%A7%A3/</a></p><p><strong>大致流程：</strong></p><ul><li>select search 算法提取2k个候选框</li><li>将图片整体输入卷积网络中，得到整个图片的feature map</li><li>通过RoI projection（通过感受野的映射公式）将2k个region proposal 映射到feature map</li><li>通过RoI pooling变成一样大，然后经过全连接层等接入softmax分类，已经bbox边框回归</li></ul><p><strong>RoI pooling</strong></p><p>RoI pooling将传入不同大小的region proposal划分层h x w个网格，对每个网格做max pooling，因此得到的输出大小是一致的。</p><p>RoI pooling反向传播过程中，原则是，max value那个位置梯度为1，其他位置为0。由于不同的region可能存在重叠的部分，因此重叠部分的梯度是每个region 梯度的求和。</p><p><strong>Loss</strong></p><p>分类部分，Fast RCNN使用的是softmax + 交叉熵：</p><p>softmax：<br>$$<br>P_{i}=\frac{e^{V_{i}}}{\sum_{i}^{C} e^{V_{i}}}<br>$$<br>交叉熵：<br>$$<br>L = -\sum(plog(p))<br>$$</p><p>边框回归使用smooth L1 loss代替原来的L2loss：<br>$$<br>\operatorname{smooth}_{L_{1}}(x)=\left{\begin{array}{lr}0.5 x^{2} &amp; \text { if }|x|&lt;1 \ |x|-0.5 &amp; \text { otherwise }\end{array}\right.<br>$$<br><strong>网络流程：</strong></p><p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200528111556483.png" alt="image-20200528111556483" style="zoom:50%;"></p><p><strong>缺点：</strong></p><ul><li>select search 算法提取候选框效率太低了（CPU上），每张图耗时3s</li></ul><h3 id="Faster-RCNN-2016"><a href="#Faster-RCNN-2016" class="headerlink" title="Faster RCNN(2016)"></a>Faster RCNN(2016)</h3><p>Faster RCNN详解：<a href="https://perper.site/2019/02/14/Faster-RCNN详解/" target="_blank" rel="noopener">https://perper.site/2019/02/14/Faster-RCNN%E8%AF%A6%E8%A7%A3/</a></p><p><strong>大致流程：</strong></p><ul><li>将图片输入到卷积网络中，生成feature map</li><li>将feature map输入RPN层（region proposal network）中，输出proposal region</li><li>提取出proposal region内的feature map 输入到RoI pooling层，输出固定维度的向量</li><li>对该向量进行softmax分类以及边框回归</li></ul><p><strong>RPN层</strong></p><p>RPN即region proposal network，输入feature map，输出为候选框的位置。RPN网络有两个分支。一个分支是softmax进行前后景的分类。另一支是边框回归（同RCNN），修正region proposal的位置。</p><p>Faster rcnn最后产生300个proposal region进行下一步边框位置的精修以及类别的判别。</p><p><strong>前后景分类</strong></p><p>在feature map的每一个像素点上，生成9个长宽比不同的anchor作为初始的anchor，然后第一个分支用softmax对每一个边框判断是前景还是背景。对于GT来说，前背景anchor标签分配的规则是：</p><ul><li>与某个GT的IoU最大的为前景</li><li>与任意GT IoU大于0.7为前景</li><li>与任意GT的IoU小于0.3为负类</li></ul><p>Faster RCNN在速度上提升很多，但是每秒仅能处理5张图片，因此one stage算法脱颖而出。</p><h3 id="Yolo-V1-2016"><a href="#Yolo-V1-2016" class="headerlink" title="Yolo V1(2016)"></a>Yolo V1(2016)</h3><p>Yolo V1 详解：<a href="https://perper.site/2019/03/07/YOLO-V1-详解/" target="_blank" rel="noopener">https://perper.site/2019/03/07/YOLO-V1-%E8%AF%A6%E8%A7%A3/</a></p><p><strong>大致流程：</strong></p><ul><li>将图片resize之后，划分成7x7的网格，每个网格负责检测两个bounding box以及一个confidence。每个网络输出还带着一个one hot，表示类别的检测</li><li>将图片resize后，输入网络，最终输出7x7x2 + 2 + 7x7xclasses的向量</li><li>将网络预测出的含有对象的框做一个NMS非极大值抑制，剔除掉目标的重复识别</li></ul><p><strong>一些注意点</strong></p><ul><li><p>GT的anchor的confidence是否为1，判断标准是一个对象的中心是否落在该anchor上</p></li><li><p>对于每一个对象，用对象中心的落在的网格来预测这个对象的边框，该边框的confidence即为1</p></li><li>边框位置在网络中使用网格边长进行归一化，因此数值大小都比较小</li><li>网络的损失函数的设计目标就是让坐标（x,y,w,h），confidence，classification 这个三个方面达到很好的平衡，共包含三个部分，坐标，置信度，已经类别分类。</li></ul><p><strong>缺点</strong></p><ul><li>yolo v1对相邻的目标检测效果不好，原因是每个网格只有两个anchor，并且只能检测一个类别，限制了密集对象的检测，以及小目标的检测</li><li>yolo v1难以检测异常长款比的目标</li><li>损失函数中大检测框与小检测框出现的误差是同等对待的，但是实际中，误差对小检测框的影响远大于大检测框。（出现在小检测框的误差通常无法容忍，比如位置偏差很大，但是大检测框影响不是那么明显）</li></ul><p>Yolo v1是第一个one stage的目标检测算法，他的速度能够达到每秒45张图片。</p><h3 id="Yolo-V2"><a href="#Yolo-V2" class="headerlink" title="Yolo V2"></a>Yolo V2</h3><p>Yolo V2详解：<a href="https://perper.site/2019/03/08/YOLO-V2-V3详解/" target="_blank" rel="noopener">https://perper.site/2019/03/08/YOLO-V2-V3%E8%AF%A6%E8%A7%A3/</a></p><p>Yolo V2和v1在网络结构上相同，使用同一个框架，他的主要贡献是在网络的优化上做了很多工作。</p><p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200529101019105.png" alt="image-20200529101019105" style="zoom:50%;"></p><p>每一点都可能是一个考点，好好复习一下。</p><h4 id="BatchNorm"><a href="#BatchNorm" class="headerlink" title="BatchNorm"></a><strong>BatchNorm</strong></h4><ul><li>有助于网络反向传播过程中梯度消失和梯度爆炸问题</li><li>降低网络对超参数的敏感性</li><li>BN对batch归一化，起到一定的正则化作用，获得更好的收敛速度</li><li>yolo v2中，在卷积后加入batchnorm替代dropout，效果提升2.4%</li></ul><p><strong>需要 BatchNorm 原因</strong></p><ul><li>在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化。因此深层次网络需要不断的调整来适应输入数据的变化，导致了学习效率的降低。</li><li>当我们使用一些类似sigmoid，tanh这种激活函数时，由于参数大小变大，导致梯度容易陷入梯度饱和区，网络难以收敛。</li></ul><p><strong>BatchNorm 操作</strong></p><ul><li>对每一层的参数特征进行归一化处理，将特征变换到0，方差1</li><li>对归一化后的参数做一次线性变换，$Z 1=Y Z+\beta$ ,使得变换后的参数不会丢失太多信息</li></ul><h4 id="高分辨率图像微调模型"><a href="#高分辨率图像微调模型" class="headerlink" title="高分辨率图像微调模型"></a>高分辨率图像微调模型</h4><p>yolo v2由于分类的数据集比含边框标注的数据集来的大，同时分类数据集分辨率比较低，作者使用高分辨率图像对模型进行微调，先是用224图像做分类，然后扩大到448，最后使用448的含边框的样本进行训练。</p><h4 id="使用先验框"><a href="#使用先验框" class="headerlink" title="使用先验框"></a>使用先验框</h4><p>yolo v1在检测目标时，由于目标尺度变换范围大，因此在精确定位方面难以得到准确的定位。Yolo v2采用了Faster RCNN的RPN思路，在每个grid预先设定一组不同大小和宽高比的边框（9个），来覆盖整个图像的不同位置和多种尺度，对特征图（feature map）进行卷积来预测每个位置的边界框以及置信度（是否含有物体）。</p><p>v2输出feature map大小比v1要大，达到13 x 13。因此v2候选框13x13x9（v1为98）,因此 anchor的检测精度略为降低（map降低0.2），但是yolo v2的召回率大大提升，因为每张图的候选框大大增加了。（提升7%）。</p><h4 id="聚类提取先验框尺度"><a href="#聚类提取先验框尺度" class="headerlink" title="聚类提取先验框尺度"></a>聚类提取先验框尺度</h4><p>对grid预设一组先验框大小，之前都是手工设定。Yolo v2对测试集中的先验框进行聚类分析，以寻找最可能匹配样本的边框尺寸。</p><p>作者采用kmean进行聚类，选用box与聚类中心box之间的IOU值作为距离指标，IoU越大，相互距离越小，最终作者选择了五个先验框作为聚类的类别数，他的效果和人工选择9个框的效果一致。</p><h4 id="约束预测边框的位置"><a href="#约束预测边框的位置" class="headerlink" title="约束预测边框的位置"></a>约束预测边框的位置</h4><p>yolov2 采用不同于RPN的边框回归的方法，yolov2回归的目标是预测边界框中心点相对于对应cell左上角位置的相对偏移。</p><h4 id="使用darknet-19作为backbone"><a href="#使用darknet-19作为backbone" class="headerlink" title="使用darknet-19作为backbone"></a>使用darknet-19作为backbone</h4><p>作者使用了自己训练的darknet-19来提取图片特征，darknet 模型参数小，运算速度快（快了33%）</p><h4 id="使用数据多尺度训练"><a href="#使用数据多尺度训练" class="headerlink" title="使用数据多尺度训练"></a>使用数据多尺度训练</h4><p>由于YOLOv2模型中只有卷积层和池化层，为了增强模型的鲁棒性，YOLOv2采用了多尺度输入训练策略，具体来说就是在训练过程中每间隔一定的iterations之后改变模型的输入图片大小。YOLOv2的下采样总步长为32，输入图片大小选择一系列为32倍数的值。</p><h3 id="Yolo-V3"><a href="#Yolo-V3" class="headerlink" title="Yolo V3"></a>Yolo V3</h3><p>yolo v3在v2的基础上进一步对网络进行优化，主要的改进有调整了网络结构，加入了残差模块。利用多尺度检测对对象进行检测，利用logistics替代softmax进行分类。</p><p><strong>新的网络结构Darknet-53</strong></p><p>加深了backbone的深度，加入了卷积、BN、leaky Relu等组件。</p><p><strong>多尺度检测</strong></p><p>Yolo V3在三个不同尺度的特征图上对对象进行检测，能够检测到更加细粒度的特征。分别在1/32，1/16，1/8，克服网络难以检测细粒度的对象。</p><p><strong>对象分类由softmax改成logistics</strong></p><p>预测对象类别时不使用softmax，改成使用logistic的输出进行预测。这样能够支持多标签对象（比如一个人有Woman 和 Person两个标签）。</p><h3 id="one-stage-与-two-stage-主要区别"><a href="#one-stage-与-two-stage-主要区别" class="headerlink" title="one stage 与 two stage 主要区别"></a>one stage 与 two stage 主要区别</h3><p><strong>two stage 算法</strong>首先通过启发式方法，或RPN层，产生一系列稀疏候选框，然后对候选框进行分类与回归，速度比较慢，精度比较高。</p><p><strong>one stage 算法</strong>则是在图片不同位置进行密集采样，抽样时选择不同的长宽比，然后CNN提取特征后，直接进行分类与回归，速度比较快，精度比较低，正负样本不均衡。</p><h3 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h3><p>SSD详解：<a href="https://perper.site/2019/03/10/SSD-M2Det-详解/" target="_blank" rel="noopener">https://perper.site/2019/03/10/SSD-M2Det-%E8%AF%A6%E8%A7%A3/</a></p><p><strong>大致流程：</strong></p><ul><li>将图像输入到网络中</li><li>选择卷积的中间层，如在38x38，19x19上的每个位置，选择对应横纵比不同的anchor4个，最后共有8732个候选框</li><li>将候选框进行NMS之后，输入loss中，loss由位置损失和置信度损失共同组成</li></ul><p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200530131928769.png" alt="image-20200530131928769" style="zoom:50%;"></p><p>SSD有两个重要的特点：</p><ul><li>提取不同尺度的特征图来做检测，大尺度的特征检测较小物体，小尺度特征检测较大物体</li><li>SSD采用不同尺度和长宽比的先验框，先验框的计算和feature map的大小有关</li></ul><p><strong>Loss</strong></p><p>SSD的loss由位置loss以及分类softmax组成。位置loss与Faster RCNN相同。</p><h3 id="RetinaNet"><a href="#RetinaNet" class="headerlink" title="RetinaNet"></a>RetinaNet</h3><p>retinanet详解：<a href="https://perper.site/2019/05/16/RetinaNet-原理记录/" target="_blank" rel="noopener">https://perper.site/2019/05/16/RetinaNet-%E5%8E%9F%E7%90%86%E8%AE%B0%E5%BD%95/</a></p><p><a href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4" target="_blank" rel="noopener">https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4</a></p><p><strong>大致流程：</strong></p><ul><li>retinanet使用resnet作为backbone，提取图像的feature map</li><li>将feature map输入三层的金字塔结构，并做特征融合（层对层）</li><li>随后在每一层上划分proposal anchor，anchor的大小和层数存在对应关系</li><li>将这些proposal输入类别分类和边框回归中，得到最终的结果</li></ul><p><strong>retinanet金字塔结构</strong></p><p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200601102057512.png" alt="image-20200601102057512" style="zoom:50%;"></p><p>即resnet最后三层C3,C4,C5生成p3,p4,p5，随后p5生成p6,p7。在每一层feature map的anchor大小的设置如下：</p><p>在设置anchor的时候，作者选用了一下几种设置：</p><p>anchor-size = [32, 64, 128, 256, 512] 对应P3～P7</p><p>anchor—scale = [2 x x0 ，2 x x(1/3 )，2 x x (2/3)]</p><p>anchor-wh = [1:2 ，1 ，2:1]</p><p>每一层anchor的大小为anchor-size 乘以 anchor-scale。然后使用三种长宽比，每一层，每一个位置得到九种大小的anchor。随后对这些位置的anchor进行边框回归以及类别的回归。</p><p><strong>focal loss</strong></p><p>one stage 算法在选择候选框的时候通常在每个像素位置上选择9个anchor，因此候选框的数量非常的多，大概有10万个，但是传统的two stage算法提取出的候选框的个数，通常在2k个。one stage算法选择的候选框绝大多数是一些easy sample，即可以置信度很高的认定为背景类别。这一类样本对网络训练产生影响：</p><ul><li>网络训练是easy negativate 样本对loss不起作用，网络收敛很慢</li><li>easy negativate样本数量过多，因此在loss收敛的过程中，真正的收敛方向会被覆盖掉，导致模型精度下降</li></ul><p>因此本文在cross entropy的基础上，提出了focal loss。</p><p>原始交叉熵loss：<br>$$<br>\operatorname{CE}(p, y)=\left{\begin{array}{ll}-\log (p) &amp; \text { if } y=1 \ -\log (1-p) &amp; \text { otherwise }\end{array}\right.<br>$$<br>focal loss：<br>$$<br>\mathrm{FL}\left(p_{\mathrm{t}}\right)=-\left(1-p_{\mathrm{t}}\right)^{\gamma} \log \left(p_{\mathrm{t}}\right)<br>$$<br>即对置信度高的样本分配比较小的loss权重，对难学的样本（置信度低）分配比较高的梯度。</p><p>实际使用是使用了一个参数a变量作为调节：<br>$$<br>\mathrm{FL}\left(p_{\mathrm{t}}\right)=-\alpha_{\mathrm{t}}\left(1-p_{\mathrm{t}}\right)^{\gamma} \log \left(p_{\mathrm{t}}\right)<br>$$<br>常见网络的anchor数量：</p><ul><li><a href="https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89" target="_blank" rel="noopener">YOLOv1</a>: 98 boxes</li><li><a href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65" target="_blank" rel="noopener">YOLOv2</a>: ~1k</li><li><a href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" target="_blank" rel="noopener">OverFeat</a>: ~1–2k</li><li><a href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11" target="_blank" rel="noopener">SSD</a>: ~8–26k</li><li><strong>RetinaNet: ~100k</strong>.</li></ul><p><strong>分类以及边框回归</strong></p><p>分类以及回归模块网络结构相同，参数不共享，回归模块只会回归1k top score的边框，置信度threshhold设置为0.05。检测得到anchor之后，使用NMS进行过滤，NMS的阈值为0.5。</p><h3 id="MobileNet-2017"><a href="#MobileNet-2017" class="headerlink" title="MobileNet (2017)"></a>MobileNet (2017)</h3><p>mobilenet详解：<a href="https://perper.site/2019/03/03/MobileNets-详解/" target="_blank" rel="noopener">https://perper.site/2019/03/03/MobileNets-%E8%AF%A6%E8%A7%A3/</a></p><p>mobileNet的核心改进在于将传统CNN结构进行近似的转换，直接降低了网络的运算量以及参数量。</p><p><strong>深度可分离卷积</strong></p><p>深度可分离卷积即将传统的卷积分解成<strong>深度卷积</strong>与<strong>逐点卷积</strong>的组合。</p><p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200601222900682.png" alt="image-20200601222900682" style="zoom:50%;"></p><p> 接下来通过计算两种卷积减小的计算量来比较两种卷积的差别：</p><p><strong>普通卷积</strong></p><p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200601223850349.png" alt="image-20200601223850349" style="zoom:50%;"></p><p>计算量：<br>$$<br>D_k <em> D_k </em> D_w <em> D_H </em> M<em>N<br>$$<br>参数量：<br>$$<br>D_k </em> D_k <em> N<br>$$<br><em>*深度可分离卷积</em></em></p><p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200601224309892.png" alt="image-20200601224309892" style="zoom:50%;"></p><p>计算量：<br>$$<br>D_k <em> D_k</em>D_w<em>D_H</em>M + D_w<em>D_H</em>N<em>M<br>$$<br>参数量：<br>$$<br>D_k</em>D_k<em>M + M</em>N<br>$$<br>对比两者的参数量以及计算量，深度分离卷积均下降为原来的：<br>$$<br>\frac{1}{N}+\frac{1}{D_{K}^{2}}<br>$$<br><strong>网络结构的改变</strong></p><p>下面以一个简单的例子说明对CNN结构的分解：</p><p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200601225407817.png" alt="image-20200601225407817" style="zoom:50%;"></p><p>将conv改成了Depthwidth和1X1的conv，新增了一层BN层，特别的ReLU6指的是：</p><p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200601225529721.png" alt="image-20200601225529721" style="zoom:50%;"><br>$$<br>\operatorname{ReLU}(6)=\min (\max (0, x), 6)<br>$$<br>作者为ReLU设置了一个最大值6，作者认为非线性激活函数在低精度计算下具有更高的鲁棒性。</p><p><strong>width multipler</strong></p><p>引入一个参数$\alpha$，用于缩放通道的数量，均匀的对网络进行减负，即参数量乘以M修改成乘以$\alpha M$，$N$ 变成 $\alpha N$。</p><p><strong>resolution Multiplier </strong></p><p>引入参数$\rho$，用来缩小图片卷积得到的分辨率、因此得到的参数量也小了$\rho$。</p><p><strong>损失函数</strong></p><p>网络的损失函数使用的是softmax + 交叉熵loss。</p><h3 id="mobileNet-V2-2018"><a href="#mobileNet-V2-2018" class="headerlink" title="mobileNet V2(2018)"></a>mobileNet V2(2018)</h3><p>mobileNet v2详解：<a href="https://perper.site/2019/03/04/MobileNet-V2-详解/" target="_blank" rel="noopener">https://perper.site/2019/03/04/MobileNet-V2-%E8%AF%A6%E8%A7%A3/</a></p><h4 id="mobileNet-v2特点"><a href="#mobileNet-v2特点" class="headerlink" title="mobileNet v2特点"></a>mobileNet v2特点</h4><ul><li>v1在处理低精度数据时，depthwise convolution导致特征退化，卷积核特征边为0。v2引入linear bottleneck结构，去掉部分relu结构</li><li>v1直筒型结构印象网络性能，因此v2引入short skip connection（inverted residual）</li></ul><h4 id="linear-bottleneck"><a href="#linear-bottleneck" class="headerlink" title="linear bottleneck"></a>linear bottleneck</h4><p><strong>原因：</strong>对低维度做ReLU运算，很容易造成信息的丢失。而在高维度进行ReLU运算的话，信息的丢失则会很少，即卷积核中很多为0的项。</p><p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200602135934414.png" alt="image-20200602135934414" style="zoom:40%;"></p><p>原因为低纬度数据经过ReLU后，逆操作还原回来的数据信息丢失严重。</p><p>因此作者提出，将最后一个relu6换成linear结构，即linear bottleneck。</p><p>修改结构后有如下的变化：</p><p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200602154310452.png" alt="image-20200602154310452" style="zoom:40%;"></p><p>左边是mobilenet原始的结构，右边是转换后，将最后一个relu6换成linear，stride=1时添加shortcut，stride=2不添加shortcut。</p><h4 id="inverted-residuals"><a href="#inverted-residuals" class="headerlink" title="inverted residuals"></a>inverted residuals</h4><p>V1是一个直筒型结构，V2希望利用数据的多尺度特征，因此引入了shortcut结构：</p><p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200602153027233.png" alt="image-20200602153027233" style="zoom:40%;"></p><p>mobileNet V2 先进行了升维然后进行降维，与resnet中shortcut结构相反，因此称为inverted residuals。</p><p>由于深度卷积本身无法改变特征的维度，而低纬度的特征效果不好，因此先通过一个1x1的结构进行升维，V2结构中，第一个1x1的结构是一个升维结构，即增加特征图的通道数，然后DW之后，通过一个1x1的结构进行降维。</p><h3 id="moblieNet-V3-2019"><a href="#moblieNet-V3-2019" class="headerlink" title="moblieNet V3(2019)"></a>moblieNet V3(2019)</h3><p>mobileNet V3使用了 <strong>神经结构搜索NAS</strong>来完成V3的结构优化。</p><p><strong>mobileNet V3中的技术</strong></p><ul><li>神经结构搜索实现的网络框架</li><li>引入V1中的深度可分离卷积</li><li>引入V2中的linear以及倒残差</li><li>引入squeeze and excitation轻量级注意力机制</li><li>使用一种新的激活函数h-swish</li><li>修改v2网络最后阶段，保留更高维度数据</li></ul><p><strong>h-swish激活函数</strong><br>$$<br>\mathrm{h}-\operatorname{swish}[x]=x \frac{\operatorname{ReLU} 6(x+3)}{6}<br>$$<br><strong>网络结构搜索NAS</strong></p><p>资源受限NAS：用于计算和参数量受限情况下，搜索网络来优化各个块</p><p>netAdapt：用于每一个模块确定之后，网络层微调每一层卷积核的数量，称为层级搜索</p><h3 id="anchor-free系列"><a href="#anchor-free系列" class="headerlink" title="anchor free系列"></a>anchor free系列</h3><h3 id="CornerNet"><a href="#CornerNet" class="headerlink" title="CornerNet"></a>CornerNet</h3><p>CornerNet将传统的anchor-base的检测方法，过渡到anchor free的方法上，文中认为anchor有一些缺点：</p><ul><li>anchor过多，出现样本不均衡的问题</li><li>anchor box 带来了大量的超参数</li></ul><p>网络结构如下：</p><p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200605221207921.png" alt="image-20200605221207921" style="zoom:50%;"></p><p>网络由三部分组成，分别是hourglass，top left Corner prediction，bottom right Corner prediction。</p><p><strong>大致步骤：</strong></p><ul><li>将图片输入 hourglass 用于提取图片特征</li><li>得到特征，经过Corner prediction module预测角点的位置，首先经过Corner pooling 结构，然后输出三个特征，分别是Corner的heatmap，角点的embedding，用于匹配另一个角点，offerses：卷积过程中出现的偏差</li><li>随后通过优化loss，得到预测效果好的网络</li></ul><p><strong>corner pooling：</strong></p><p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200605222258791.png" alt="image-20200605222258791" style="zoom:50%;"></p><p>pooling的方式如上，每一个位置的值取向右，向下的最大值作为当前的值。该pooling方式有效的解释：</p><ol><li>每个顶点只与边界框的两条边相关，所以corner 更容易提取。2</li><li>顶点更有效提供离散的边界空间</li></ol><p><strong>Prediction Module</strong></p><p>prediction module的第一个输出是<strong>heatmaps</strong>，heatmaps包含C个通道（C是目标的类别，没有背景类别），每个channel是二进制掩膜，表示相应类别的角点位置。对于每个角点，只有一个ground-truth，其他位置都是负样本。在训练过程，模型减少负样本，在每个ground-truth角点设定半径r区域内都是正样本，因为r内的角点，都可以产生合适的边框。（论文中正负边框IoU为0.7）</p><p>定点的损失：<br>$$<br>L_{d e t}=\frac{-1}{N} \sum_{c=1}^{C} \sum_{i=1}^{H} \sum_{j=1}^{W}\left{\begin{array}{cc}\left(1-p_{c i j}\right)^{\alpha} \log \left(p_{c i j}\right) &amp; \text { if } y_{c i j}=1 \ \left(1-y_{c i j}\right)^{\beta}\left(p_{c i j}\right)^{\alpha} \log \left(1-p_{c i j}\right) &amp; \text { otherwise }\end{array}\right.<br>$$<br>其中$p_{cij}$ 表示i,j位置上的类别为c的概率，其中$y_{cij}$ 表示GT的i,j位置上半径为r的高斯值。</p><p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200605225958982.png" alt="image-20200605225958982" style="zoom:50%;"></p><p><strong>offsets</strong></p><p>模型在卷积过程中造成的位置偏移，一些取整等操作造成的，模型通过最优化一个smooth L1来优化：<br>$$<br>L_{o f f}=\frac{1}{N} \sum_{k=1}^{N} \operatorname{SmoothL} 1 \operatorname{Loss}\left(\boldsymbol{o}_{k}, \hat{\boldsymbol{o}}_{k}\right)<br>$$<br><strong>embedding</strong></p><p>模型输出的第三部分，是左上角Corner和右下角Corner相互识别的标识，损失函数通过最小化同一个框的距离，最大化不同框的距离来达到目的。<br>$$<br>\begin{array}{l}L_{p u l l}=\frac{1}{N} \sum_{k=1}^{N}\left[\left(e_{t_{k}}-e_{k}\right)^{2}+\left(e_{b_{k}}-e_{k}\right)^{2}\right] \ L_{p u s h}=\frac{1}{N(N-1)} \sum_{k=1}^{N} \sum_{j=1}^{N} \max \left(0, \Delta-\left|e_{k}-e_{j}\right|\right)\end{array}<br>$$<br>$e_k$ 表示两个点的均值，$e_{tk}$ 表示左上角，$e_{lk}$表示右下角，第一个式子将同一个框的角拉近。第二个式子将不同框的角拉远。</p><p>总体的loss为：<br>$$<br>L=L_{d e t}+\alpha L_{p u l l}+\beta L_{p u s h}+\gamma L_{o f f}<br>$$</p><h3 id="FCOS"><a href="#FCOS" class="headerlink" title="FCOS"></a>FCOS</h3><p>FCOS一种one stage的anchor free目标检测算法，类似于语义分割，通过逐像素预测的方式来解决目标检测的问题。</p><p>剔除了anchor，避免了anchor各种超参的计算，IoU的计算等。</p><p><strong>网络结构</strong></p><p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200606100058559.png" alt="image-20200606100058559" style="zoom:50%;"></p><p><strong>逐像素回归</strong></p><p>anchor base模型，对IoU大于阈值的边框，进行边框回归，回归的规则是预测框中点，到GT边的距离。</p><p>FCOS则是对目标框中，所有的点都做边框回归，每个点回归到box的上下左右四个距离：<br>$$<br>\begin{array}{l}l^{<em>}=x-x_{0}^{(i)}, \quad t^{</em>}=y-y_{0}^{(i)} \ r^{<em>}=x_{1}^{(i)}-x, \quad b^{</em>}=y_{1}^{(i)}-y\end{array}<br>$$<br><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200606104920048.png" alt="image-20200606104920048" style="zoom:50%;"></p><p>对于目标框重叠的情况，作者选择的是小样本的框进行回归。</p><p>不同feature map上的点和GT进行比较之前需要将feature map每个点映射到原图上，转换关系为：</p><p> <strong>(floor(s/2) + x*s, floor(s/2) + y*s)</strong> ，s为stride。</p><p>FCOS对每一个像素直接回归box，例如使用coco数据，最终网络的输出为4维的box，FCOS训练了C个二分类网络，最终得到4 + C维的输出。</p><p>损失函数如下：<br>$$<br>\begin{aligned} L\left(\left{\boldsymbol{p}_{x, y}\right},\left{\boldsymbol{t}_{x, y}\right}\right) &amp;=\frac{1}{N_{\text {pos }}} \sum_{x, y} L_{\text {cls }}\left(\boldsymbol{p}_{x, y}, c_{x, y}^{<em>}\right) \ &amp;+\frac{\lambda}{N_{\text {pos }}} \sum_{x, y} \mathbb{1}_{\left{c_{x, y}^{</em>}&gt;0\right}} L_{\text {reg }}\left(\boldsymbol{t}_{x, y}, \boldsymbol{t}_{x, y}^{*}\right) \end{aligned}<br>$$<br>第一项为focal loss损失用于像素的分类，第二项为IoU 损失，用于像素点的回归。</p><p>由于FCOS利用逐像素进行回归，因此有大量的正样本参与训练，相比于anchor base方法，不存在正负样本不均衡的问题。</p><p><strong>多尺度识别</strong></p><p>网络将特征输入特征金字塔中（与retinaNet相同），每一级的特征都做边框回归，特征分类等。这样在feature map比较深的特征上可以检测大目标，比较浅的feature map上可以检测比较小的目标。然后通过一定规则放大到正常图像的尺度。有效提升重叠目标的检测性能。</p><p>作者通过一个超参数m来限制框的回归范围，如果一个框：<br>$$<br>\max \left(l^{<em>}, t^{</em>}, r^{<em>}, b^{</em>}\right)&gt;m_{i} \text { or } \max \left(l^{<em>}, t^{</em>}, r^{<em>}, b^{</em>}\right)&lt;m_{i-1}<br>$$<br>即视为是负样本，否则是正样本。对负样本不进行回归。这样的目的是对一些偏离中心很远的box忽略掉。</p><p><strong>center-ness</strong></p><p>作者发现fpn层预测的结果比retinanet差2%，原因是FCOS在远离目标中心预测了很多低质量的框，</p><p>因此作者在分类中多引入一个分支，对框的距离进行约束，即center-ness：<br>$$<br>\text { centerness }^{<em>}=\sqrt{\frac{\min \left(l^{</em>}, r^{<em>}\right)}{\max \left(l^{</em>}, r^{<em>}\right)} \times \frac{\min \left(t^{</em>}, b^{<em>}\right)}{\max \left(t^{</em>}, b^{*}\right)}}<br>$$<br>在训练的过程中我们会约束中center-ness的值，使得其接近于0，使得分布在目标位置边缘的低质量框能够尽可能的靠近中心。在最终使用该网络的过程中，非极大值抑制(NMS)就可以轻松滤除这些低质量的边界框，提高检测性能。</p><p><strong>总结</strong></p><ul><li>把图片输入backbone中，得到feature map</li><li>将feature map输入FPN层，得到每一层P3-P7的feature map</li><li>对P3-P7的feature map进行box的位置的回归以及分类</li><li>box regression部分，对GT内部的点，直接回归到box，计算四个参数即点到四条边的距离，同时对每一个点预测一个类别，最终输出有4 + C个维度，对一些偏离中心的边框进行舍弃</li><li>classification部分，一个分支去分类，使用focal loss，另一个分支使用center mess对边框位置进行约束</li></ul><p><strong>IoU loss</strong></p><p>直接将网络的IoU作为loss进行网络的训练，相比于L2优化边框，优点为：</p><ul><li>IoU loss将位置信息作为一个整体进行训练，而L2损失却把它们当作互相独立的四个变量进行训练，因此IoU损失能得到更为准确的训练效果；</li><li>输入任意样本，IoU的值均介于[0, 1]之间，这种自然的归一化损失使模型具有更强的处理多尺度图像的能力。</li></ul><h3 id="Segmentation-部分"><a href="#Segmentation-部分" class="headerlink" title="Segmentation 部分"></a>Segmentation 部分</h3><h3 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h3><p>FCN详解： <a href="https://perper.site/2019/02/20/语义分割系列-FCN详解/" target="_blank" rel="noopener">https://perper.site/2019/02/20/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%B3%BB%E5%88%97-FCN%E8%AF%A6%E8%A7%A3/</a></p><p>FCN作为最早的分割网络，是分割领域的开山之作。</p><p><strong>大致流程</strong></p><ul><li>FCN将CNN分类网络中最后几层全连接层，改成了卷积层，即全卷积</li><li>将图像输入网络中，将不同pooling阶段的特征进行融合，得到网络最终的输出</li><li>网络最终输出C+1个channel的 feature map， 优化网络损失，得到每一个像素位置上的类别信息</li></ul><p><strong>网络结构</strong></p><p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200607224612659.png" alt="image-20200607224612659" style="zoom:40%;"></p><p>网络选择了8x，16x，32x这三种尺度的上采样输出进行网络的优化，分别对这三层上采样，然后将得到的feature map进行优化，最终最优的结构为8x。</p><p><strong>上采样</strong></p><p>讲一个2x2的feature map上采样成4x4的feature map，卷积核的大小为3x3，首先将2x2加上2个padding，变成6x6，随后做3x3的卷积，即可。这一步操作使得网络的输出与输入图像大小一致。含步长的卷积可以视为对feature map各个元素之间进行填充。</p><p><strong>损失函数</strong></p><p>网络采用交叉熵损失函数，对每个位置进行回归。</p><p>segmentation领域碰撞出了两个最重要的设计：U-shape Structure 和 Dilation Conv，据此形成当下语义分割领域网络设计最常见的两大派系：1）U-shape 联盟以 RefineNet、GCN、DFN 等算法为代表；2）Dilation 联盟以 PSPNet、Deeplab 系列方法为代表。</p><h3 id="DeepLab"><a href="#DeepLab" class="headerlink" title="DeepLab"></a>DeepLab</h3><p>加油！</p><h3 id="关键点检测部分网络"><a href="#关键点检测部分网络" class="headerlink" title="关键点检测部分网络"></a>关键点检测部分网络</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;总结常见的目标检测网络，持续更新，文章要写很长。&lt;/p&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习总结" scheme="https://wenhui-zhou.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    
    
  </entry>
  
  <entry>
    <title>常见数据结构</title>
    <link href="https://wenhui-zhou.github.io/2020/05/26/%E5%B8%B8%E8%A7%81%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    <id>https://wenhui-zhou.github.io/2020/05/26/常见数据结构/</id>
    <published>2020-05-26T14:57:20.000Z</published>
    <updated>2020-05-27T03:16:55.375Z</updated>
    
    <content type="html"><![CDATA[<ul><li>array</li><li>链表</li><li>hash map</li><li>set</li><li>红黑树</li></ul><h3 id="array"><a href="#array" class="headerlink" title="array"></a>array</h3><p>数组又叫顺序表，在内存中存储空间是连续的，允许用户对其进行插入，删除，访问和替换等等。Python中的列表是由对其它对象的引用组成的连续数组。</p><p>append复杂度为O(1)，insert复杂度为O(n)，sort复杂度：O(nlogn)。</p><h3 id="链表"><a href="#链表" class="headerlink" title="链表"></a>链表</h3><p>链表数据存储空间未必是连续的，在插入或删除的时候，只需要改变指针的指向，其他都是不变的。链表的空间不需要提前分配，链表只能顺序访问，无法实现随机访问。</p><h3 id="hash-map"><a href="#hash-map" class="headerlink" title="hash map"></a>hash map</h3><p>哈希表的关键思路在于建立存储对象和地址之间的联系，这个联系即哈希函数。通过哈希函数算出对象的地址。dict类似对key进行了hash,然后再对hash生成一个红黑树进行查找，其查找复杂其实是O(logn)，O(1)是理想情况。</p><p>建立hash map的过程哈希值可能存在冲突，可用的解决方案是：链式地址法，开放定址法，线行探查法，平方探查法等。</p><h3 id="set"><a href="#set" class="headerlink" title="set"></a>set</h3><p>set本质上是一颗红黑树</p><h3 id="红黑树"><a href="#红黑树" class="headerlink" title="红黑树"></a>红黑树</h3><p>红黑树是一棵自平衡二叉查找树，<strong>二叉查找树</strong>满足左子树节点小于根节点，右子树节点大于根节点。</p><p><strong>红黑树的定义和性质：</strong></p><ul><li>红黑树每个节点要么是红色，要么是黑色</li><li>根节点是黑色</li><li>子节点是黑色</li><li>红色节点两个子节点都是黑色的</li><li>如果一个节点存在一个子黑节点，那么一定会有两个黑色子节点</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;array&lt;/li&gt;
&lt;li&gt;链表&lt;/li&gt;
&lt;li&gt;hash map&lt;/li&gt;
&lt;li&gt;set&lt;/li&gt;
&lt;li&gt;红黑树&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;array&quot;&gt;&lt;a href=&quot;#array&quot; class=&quot;headerlink&quot; title=&quot;
      
    
    </summary>
    
      <category term="深度学习总结" scheme="https://wenhui-zhou.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    
    
  </entry>
  
  <entry>
    <title>序列化RNN系列</title>
    <link href="https://wenhui-zhou.github.io/2020/05/21/%E5%BA%8F%E5%88%97%E5%8C%96RNN%E7%B3%BB%E5%88%97/"/>
    <id>https://wenhui-zhou.github.io/2020/05/21/序列化RNN系列/</id>
    <published>2020-05-21T05:52:45.000Z</published>
    <updated>2020-05-21T13:55:46.728Z</updated>
    
    <content type="html"><![CDATA[<h3 id="为什么需要RNN"><a href="#为什么需要RNN" class="headerlink" title="为什么需要RNN"></a>为什么需要RNN</h3><p>当我们遇到一些数据是序列的，长度不定的，数据的先后，顺序，是存在相互影响的语义的。对于这类问题，因此就出现了RNN这种结构，能够能够的提取序列数据的特征。</p><h3 id="RNN结构"><a href="#RNN结构" class="headerlink" title="RNN结构"></a>RNN结构</h3><p><img src="/images/nlp/image-20200521141220698.png" alt="image-20200521141220698" style="zoom:50%;"></p><p>最简单的RNN的结构如上所示，左边是一个RNN单元，右边是将这个单元展示后得到的网络。最早的激活函数使用tanh，该网络的特殊之处在于，下一个阶段网络的输入由上一阶段的输出以及x共同组成，用公式表示如下：<br>$$<br>\begin{array}{l}O_{t}=g\left(V \cdot S_{t}\right) \ S_{t}=f\left(U \cdot X_{t}+W \cdot S_{t-1}\right)\end{array}<br>$$</p><h3 id="RNN的优点"><a href="#RNN的优点" class="headerlink" title="RNN的优点"></a>RNN的优点</h3><ol><li>RNN可以记录时间序列上的信息，对于序列数据，前后语义有着相互联系的场景比较适用。</li><li>RNN可以处理文本，语音这些数据，数据的输出长度可以是不定的。</li></ol><h3 id="RNN的缺点"><a href="#RNN的缺点" class="headerlink" title="RNN的缺点"></a>RNN的缺点</h3><ol><li><p>梯度消失和梯度爆炸问题，当对RNN进行梯度求导的时候，得到的表达式是参数的一个连乘形式，任意时刻对$W_s$求偏导如下：<br>$$<br>\frac{\partial L_{t}}{\partial W_{x}}=\sum_{k=0}^{t} \frac{\partial L_{t}}{\partial O_{t}} \frac{\partial O_{t}}{\partial S_{t}}\left(\prod_{j=k+1}^{t} \frac{\partial S_{j}}{\partial S_{j-1}}\right) \frac{\partial S_{k}}{\partial W_{x}}<br>$$<br>随着网络加深，连乘项越来越多，将S用tanh激活函数带入，下面表达式可变为：<br>$$<br>\prod_{j=k+1}^{t} \frac{\partial S_{j}}{\partial S_{j-1}}= \prod_{j=k+1}^{t} \tanh ^{\prime} W_{s}<br>$$<br>即一个参数累乘的形式，当网络足够深的时候，如果参数小于一，则会出现梯度消失的问题，如果参数大于1，多次连乘的结果将导致梯度爆炸。</p></li><li><p>RNN网络难以训练，并且如果使用的是tanh或者relu激活函数，它无法处理非常长的序列。</p></li></ol><p>通过上面可以发现，只要解决了掉偏导公式中参数连乘的哪一项就可以解决梯度问题，LSTM就是按照这个思路，将这一项变成0或者1。</p><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>LSTM即long short Term memory，LSTM的结构比普通的RNN要复杂一些，由三个门结构组成，分别是遗忘门，输入门，输出门：</p><p><img src="/images/nlp/image-20200521155345276.png" alt="image-20200521155345276" style="zoom:50%;"></p><p>首先是<strong>遗忘门</strong>，对输入的数据做一些选择性的遗忘，控制是否遗忘由sigmoid决定。其次是<strong>输入门</strong>，利用sigmoid对输入数据进行取舍，tanh对输入数据赋予权重。<strong>输出门</strong>：利用sigmoid对输入进行取舍，然后用tanh对数据进行加权，得到下一个输入。</p><p>（通过sigmoid后的特征，最后通过一个乘法加入到网络中）</p><h3 id="为什么LSTM能够解决梯度消失问题"><a href="#为什么LSTM能够解决梯度消失问题" class="headerlink" title="为什么LSTM能够解决梯度消失问题"></a>为什么LSTM能够解决梯度消失问题</h3><p>接在RNN的后面分析，LSTM梯度求导过程每一项中也存在一个累乘项，但是LSTM这个累乘项在LSTM中为0或者为1，因此有效避免了累乘导致的梯度消失问题。</p><p>传统RNN梯度计算如下：<br>$$<br>\frac{\partial L_{3}}{\partial W_{s}}=\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial W_{s}}+\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial S_{2}} \frac{\partial S_{2}}{\partial W_{s}}+\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial S_{2}} \frac{\partial S_{2}}{\partial S_{1}} \frac{\partial S_{1}}{\partial W_{s}}<br>$$<br>LSTM中有表达式：<br>$$<br>\prod_{j=k+1}^{t} \frac{\partial S_{j}}{\partial S_{j-1}}=\prod_{j=k+1}^{t} \tanh ^{\prime} \sigma\left(W_{f} X_{t}+b_{f}\right) \approx 0 | 1<br>$$<br>因此LSTM:<br>$$<br>\frac{\partial L_{3}}{\partial W_{s}}=\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial W_{s}}+\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{2}}{\partial W_{s}}+\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{1}}{\partial W_{s}}<br>$$<br>梯度中不存在累乘项，因此可以克服梯度消失和梯度爆炸的问题。</p><h3 id="LSTM具有记忆功能"><a href="#LSTM具有记忆功能" class="headerlink" title="LSTM具有记忆功能"></a>LSTM具有记忆功能</h3><p>由于LSTM每次计算都有参考到上一时刻的LSTM状态，每一步决策均使用到了上一次的中间结果，因此具有记忆功能。</p><h3 id="LSTM具记忆时间长"><a href="#LSTM具记忆时间长" class="headerlink" title="LSTM具记忆时间长"></a>LSTM具记忆时间长</h3><p>由于LSTM将连乘项转化为1或者0，因此有效解决了梯度爆炸和梯度消失的问题，可以保存距离当前位置比较远的位置的信息，因此LSTM具有记忆时间长的功能。</p><h3 id="LSTM存在的问题"><a href="#LSTM存在的问题" class="headerlink" title="LSTM存在的问题"></a>LSTM存在的问题</h3><p>无法并行运算，LSTM计算效率太低。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;为什么需要RNN&quot;&gt;&lt;a href=&quot;#为什么需要RNN&quot; class=&quot;headerlink&quot; title=&quot;为什么需要RNN&quot;&gt;&lt;/a&gt;为什么需要RNN&lt;/h3&gt;&lt;p&gt;当我们遇到一些数据是序列的，长度不定的，数据的先后，顺序，是存在相互影响的语义的。对于这类问
      
    
    </summary>
    
      <category term="面试准备" scheme="https://wenhui-zhou.github.io/categories/%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87/"/>
    
    
  </entry>
  
</feed>
