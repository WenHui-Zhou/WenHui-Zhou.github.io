<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="cheer up"><meta name="baidu-site-verification" content="9pSIuwCbvi"><meta name="google-site-verification" content="YzcCTjF6VoVlNAtL37_S4vFjzFwYTAFZzD51Il2IGKY"><title>cs224N word vector I | WenHuiZhou</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">cs224N word vector I</h1><a id="logo" href="/.">WenHuiZhou</a><p class="description">perper（打起精神！）</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">cs224N word vector I</h1><div class="post-meta">Jan 9, 2020<span> | </span><span class="category"><a href="/categories/NLP/">NLP</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" href="/2020/01/09/cs224N-word-vector-I/#vcomment"><span class="valine-comment-count" data-xid="/2020/01/09/cs224N-word-vector-I/"></span><span> 条评论</span></a><div class="post-content"><p>这篇文章是CS224N winter2019的第一次课的内容，主要针对词的表示，词向量生成的一些方法，从最早到最新，列举了这些算法以及应用。</p>
<a id="more"></a>
<h3 id="NLP任务的特殊性"><a href="#NLP任务的特殊性" class="headerlink" title="NLP任务的特殊性"></a>NLP任务的特殊性</h3><p>说道NLP任务的特殊性，其实是针对图像任务来说的。对于图像任务来说，我们所看到的场景通常直接反映了图像中所包含的内涵，同时图像上像素点通常是连续的，对于一个可微分系统求最优化的过程（深度学习的内涵就是构建一个可微分的系统），是天然切合的。</p>
<p>我们知道NLP最小的单元是词，由众多的词组成句子，形成语义。<strong>对于一个词来说，它在形状上不表示任何的含义，而是指代一种抽象的含义</strong>，因此在理解词含义的时候，我们需要结合上下文，结合语境，诸如LSTM，transformer这种能够获得较长上下文信息的结果在NLP中起到巨大的作用。此外，还存在着大量的一词多义的情形，因此具体的语境，上下文对理解NLP任务显得至关重要。</p>
<p>另一个显著的不同在于图像色彩在变化上是连续的，而NLP任务中，每个词之间是离散的，不存在连续的关系。因此我们需要对每次词进行适当的编码，转化成连续的词向量的形式。通过这种方式最优化我们的模型，得到NLP任务的解。</p>
<h3 id="word-vector"><a href="#word-vector" class="headerlink" title="word vector"></a>word vector</h3><p>如何用向量表示词，一个重要的指标是词向量能够反映词之间的相似性和相异性。例如苹果和巧克力，应当比苹果和梨之间的距离要大。下面是自己中词的表示方式：</p>
<ul>
<li><strong>同义词的字典：</strong>一个比较直观的解决方法是建立一个同义词的字典，将所有的同义词归类，用字典的方式来表示词，但是这种方式有缺点，我们无法涵盖所有的单词的含义，需要人去维护字典。</li>
<li><strong>one-hot方式：</strong>采用onehot方式，奖励一个大小为vocab size的向量，在该词的位置为1，其他为0。这种方式的缺点是，需要维护一个巨大的矩阵，同时每个 词之间都是独立的，无法表示不同词的相似性。</li>
<li><strong>一个词的含义由它周围的词决定：</strong>即一个词的含义应该与他的上下文（fix-window）所决定的。这种利用周围的词来表示当前的词的方式称为词嵌入word-embedding，它实际上是一种分布。</li>
</ul>
<p><img src="/images/nlp//image-20200112210153920.png" alt="image-20200112210153920" style="zoom:40%;"></p>
<h3 id="word-vector-1"><a href="#word-vector-1" class="headerlink" title="word vector"></a>word vector</h3><p>2013年由Mikolov提出，提出利用周围周围的词来预测当前的词：</p>
<ul>
<li>一个很大的语料库</li>
<li>每一个词表示表示成 一个词向量</li>
<li>遍历句子中的每一个词c</li>
<li>利用每一个c周围的词（fix-window），来计算c的相似性</li>
<li>通过迭代，最大化预测c的概率</li>
</ul>
<p><img src="/images/nlp//image-20200112214809724.png" alt="image-20200112214809724" style="zoom:50%;"></p>
<p>构造损失函数如下：</p>
<p><img src="/images/nlp//image-20200112215013622.png" alt="image-20200112215013622" style="zoom:40%;"></p>
<p>利用贝叶斯公式，利用前后m个位置的词来预测当前位置词的概率。构造一个负log似然函数，即交叉熵函数，通过最小化交叉熵损失，预测正确的概率最大。</p>
<p>其中每个词概率的计算p：</p>
<p><img src="/images/nlp//image-20200112215844822.png" alt="image-20200112215844822" style="zoom:40%;"></p>
<p>因此对于每一个词来说，同时有两个向量来表示：</p>
<p><img src="/images/nlp//image-20200112224851890.png" alt="image-20200112224851890" style="zoom:40%;"></p>
<p>在词的含义由周围词决定的思想下，有着两种模型的变种，一种是CBOW，另一种是skip-gram。</p>
<h3 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h3><p>Continuous Bag of Words Model,连续的词包模型，将句子挖空，利用其它词来预测当前的词。每个词学习两个向量：</p>
<ul>
<li>v：when the word is in the context</li>
<li>u：when the word is in the center</li>
</ul>
<p><img src="/images/nlp//image-20200112230514347.png" alt="image-20200112230514347" style="zoom:50%;"></p>
<p>如上，我们输入一个长度为字典词数长度的词（可以使onehot形式），然后学习一个从输入到隐变量的一个映射，然后在学一个隐变量到输出的映射。输入时句子中出了挖空的那个单词。最后通过softmax变成概率之后，最优化交叉熵损失，得到最优的结果。</p>
<h3 id="skip-gram-model"><a href="#skip-gram-model" class="headerlink" title="skip gram model"></a>skip gram model</h3><p>这个模型的思路和CBOW相反，利用一个单词去预测其他周围位置的单词，网路结构如下：</p>
<p><img src="/images/nlp//image-20200112231300222.png" alt="image-20200112231300222" style="zoom:50%;"></p>
<p>输入为一个词的onehot，学习两个映射过程，最终得到其周围fix-window内的其他词的预测结果，其过程与CBOW类似。</p>
<p>上述的两个模型中，均是三层结构，隐藏层维度为我们希望的词向量的长度，从输入到隐藏层之间的映射参数即为词典长度x词向量长度的一个矩阵，即我们所要的wordvec。</p>
<h3 id="negative-sampling"><a href="#negative-sampling" class="headerlink" title="negative sampling"></a>negative sampling</h3><p>在训练神经网络时，每当接受一个训练样本，然后调整所有神经单元权重参数，来使神经网络预测更加准确。换句话说，每个训练样本都将会调整所有神经网络中的参数。<br> 我们词汇表的大小决定了我们skip-gram 神经网络将会有一个非常大的权重参数，并且所有的权重参数会随着数十亿训练样本不断调整。</p>
<p>negative sampling  每次让一个训练样本仅仅更新一小部分的权重参数，从而降低梯度下降过程中的计算量。<br> 如果 vocabulary 大小为1万时， 当输入样本 ( “fox”, “quick”) 到神经网络时， “ fox” 经过 one-hot 编码，在输出层我们期望对应 “quick” 单词的那个神经元结点输出 1，其余 9999 个都应该输出 0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们为 negative word.   negative sampling 的想法也很直接 ，将随机选择一小部分的 negative words，比如选 10个 negative words 来更新对应的权重参数。（选择onehot中为0的一部分数参与模型的更新。）</p>
<p>在论文中作者指出指出对于小规模数据集，建议选择 5-20 个 negative words，对于大规模数据集选择 2-5个 negative words.</p>
<p>如果使用了 negative sampling 仅仅去更新positive word- “quick” 和选择的其他 10 个negative words 的结点对应的权重，共计 11 个输出神经元，相当于每次只更新 300 x 11 = 3300 个权重参数。对于 3百万 的权重来说，相当于只计算了千分之一的权重，这样计算效率就大幅度提高。</p>
<p><strong>如何选择负样本</strong></p>
<p>一个词是否会被选择为负样本与其出现的频率有关，出现的频次越高，越容易被当成负样本：</p>
<p><img src="/images/nlp//image-20200112233855262.png" alt="image-20200112233855262" style="zoom:50%;"></p>
<p>经过实验，当这个数为3/4的时候，模型表现最好。</p>
<h3 id="hierarchical-softmax"><a href="#hierarchical-softmax" class="headerlink" title="hierarchical softmax"></a>hierarchical softmax</h3><p>层次softmax是另一种优化模型计算的方法，他的思路借鉴了huffman树的思想，出现越是频繁的树，他出现的位置越浅。哈夫曼树每一个叶子节点表示一个类别，每个非叶子节点需要做一次二分类，走左边后走右边的概率用逻辑回归来表示：</p>
<p><img src="/images/nlp//image-20200113002246268.png" alt="image-20200113002246268" style="zoom:50%;"></p>
<p>如何降低复杂度呢，当k为词个数时,h为维度，可以将复杂度降到 $O(hlog_2(k))$。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>以上便是wordvec的表示方法，即解决了NLP的一个最基本，最重要的问题，词的表示。我们利用CBOW，skip-gram等模型，在优化模型的过程中，得到了词到向量的一个映射关系，这个关系就是wordvec的一个映射矩阵，有了这个矩阵我们就可以将词转化成向量。此时向量是根据其前后的单词而产生的，因此此位置的单词是根据前后的词而产生。下面一篇post将介绍wordvec更深层的东西，wordsence。</p>
</div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>WenHui Zhou</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2020/01/09/cs224N-word-vector-I/">https://wenhui-zhou.github.io/2020/01/09/cs224N-word-vector-I/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>| 本博客所有文章除特别声明外，均采用 <a href="&quot;http://creativecommons.org/licenses/by-nc-sa/3.0/cn/&quot;" rel="&quot;external" nofollow&quot;="" target="&quot;_blank&quot;">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！ </li></ul></div><br><div class="tags"></div><div class="post-nav"><a class="pre" href="/2020/01/26/统计学习方法概述/">统计学习方法概述</a><a class="next" href="/2020/01/01/2019！2020！/">2019！2020！</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'civq9nKD49NpRALooR9Llqmf-gzGzoHsz',
  appKey:'JggO9HaSi1Lfx17nt16oDfsI',
  placeholder:'Shall we talk',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/3D重建/">3D重建</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Mac/">Mac</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tensorflow/">Tensorflow</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/effective-cpp/">effective cpp</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/super-resolution/">super resolution</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tool/">tool</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/webSearch/">webSearch</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/xigua/">xigua</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/动画/">动画</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/建站/">建站</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/手撕系列/">手撕系列</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/推荐系统/">推荐系统</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/模型评价/">模型评价</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/比赛/">比赛</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习/">深度学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习总结/">深度学习总结</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法扫盲/">算法扫盲</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/统计学习方法/">统计学习方法</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程/">编程</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/论文复现/">论文复现</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/论文阅读/">论文阅读</a><span class="category-list-count">23</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/论文阅读/3D重建/">3D重建</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/面试准备/">面试准备</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目/">项目</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目总结/">项目总结</a><span class="category-list-count">5</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/项目总结/论文阅读/">论文阅读</a><span class="category-list-count">3</span></li></ul></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/SR/" style="font-size: 15px;">SR</a> <a href="/tags/dialog/" style="font-size: 15px;">dialog</a> <a href="/tags/c/" style="font-size: 15px;">c++</a> <a href="/tags/leetcode/" style="font-size: 15px;">leetcode</a> <a href="/tags/项目总结/" style="font-size: 15px;">项目总结</a> <a href="/tags/3D重建/" style="font-size: 15px;">3D重建</a> <a href="/tags/论文阅读/" style="font-size: 15px;">论文阅读</a> <a href="/tags/tool/" style="font-size: 15px;">tool</a> <a href="/tags/netStation/" style="font-size: 15px;">netStation</a> <a href="/tags/interview/" style="font-size: 15px;">interview</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/tags/tip/" style="font-size: 15px;">tip</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/learning-cpp/" style="font-size: 15px;">learning cpp</a> <a href="/tags/tips/" style="font-size: 15px;">tips</a> <a href="/tags/职业规划/" style="font-size: 15px;">职业规划</a> <a href="/tags/—-leetcode/" style="font-size: 15px;">— leetcode</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/超分辨率/" style="font-size: 15px;">超分辨率</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/05/21/序列化RNN系列/">序列化RNN系列</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/13/2D-animation-SVG文件/">2D animation,SVG文件</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/12/RDSNet总结文档/">RDSNet总结文档</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/11/约束项以及约束的含义/">约束项以及约束的含义</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/04/优化器总结/">优化器总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/03/深度学习知识点总结/">深度学习知识点总结1</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/14/cpp常见的考点/">cpp常见的考点</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/25/problem-summary/">problem summary</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/18/RDSNet/">RDSNet</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/14/LR-推导/">LR 推导</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/WenHui-Zhou" title="GITHUB" target="_blank">GITHUB</a><ul></ul><a href="http://www.google.com/" title="GOOGLE" target="_blank">GOOGLE</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">WenHuiZhou.</a> 访问人数:<span id="busuanzi_value_site_uv"></span> 
访问量:<span id="busuanzi_value_site_pv"></span></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script> </div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>