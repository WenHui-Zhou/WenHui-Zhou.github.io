<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="cheer up"><meta name="baidu-site-verification" content="9pSIuwCbvi"><meta name="google-site-verification" content="YzcCTjF6VoVlNAtL37_S4vFjzFwYTAFZzD51Il2IGKY"><title>深度学习知识点总结1 | WenHuiZhou</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">深度学习知识点总结1</h1><a id="logo" href="/.">WenHuiZhou</a><p class="description">perper（打起精神！）</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">深度学习知识点总结1</h1><div class="post-meta">May 3, 2020<span> | </span><span class="category"><a href="/categories/深度学习总结/">深度学习总结</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" href="/2020/05/03/深度学习知识点总结/#vcomment"><span class="valine-comment-count" data-xid="/2020/05/03/深度学习知识点总结/"></span><span> 条评论</span></a><div class="post-content"><p>[TOC]</p>
<h3 id="梯度消失与梯度爆炸问题"><a href="#梯度消失与梯度爆炸问题" class="headerlink" title="梯度消失与梯度爆炸问题"></a>梯度消失与梯度爆炸问题</h3><p>训练神经网络时，神经网络各层的参数逐层累乘，因此在训练一个深度网络的时候，每层的参数累乘结果大于1，累乘n次之后则会出现梯度爆炸，反之小于1则出现梯度消失问题。</p>
<h3 id="神经网络的初始化"><a href="#神经网络的初始化" class="headerlink" title="神经网络的初始化"></a>神经网络的初始化</h3><p>为了缓解梯度消失和梯度爆炸问题，我们想到的一个办法是在参数初始化的时候，使用ReLU激活函数，同时给出参数的方差对参数进行归一化，方差通常为：<br>$$<br>\operatorname{np} . \operatorname{sqrt}\left(\frac{2}{n^{[l-1]}}\right)<br>$$<br>这种方法不能解决梯度消失或爆炸的问题，但是通过这种方式初始化得到的参数，数值比较小，比较接近1，因此在一定程度上缓解了梯度消失或爆炸的问题，可以加快网络的收敛。</p>
<h3 id="各种优化器总结"><a href="#各种优化器总结" class="headerlink" title="各种优化器总结"></a>各种优化器总结</h3><p><a href="https://perper.site/2020/05/04/优化器总结/" target="_blank" rel="noopener">https://perper.site/2020/05/04/%E4%BC%98%E5%8C%96%E5%99%A8%E6%80%BB%E7%BB%93/</a></p>
<h3 id="batch-normalization"><a href="#batch-normalization" class="headerlink" title="batch normalization"></a>batch normalization</h3><p>BN技术使得网络对超参数的选择不那么敏感，超参数的取值不会很大程度影响网络的性能。</p>
<p>covariate shift问题，即浅层网络参数的变化，会一直影响到深层网络的参数变化，使得每一层的参数分布发生变化，这种变化传递到深层网络的时候将会出现比较大的偏差，使得网络每次迭代的时候都需要去学习这种分布，因此希望通过BN将每层的分布归一化之后，加速网络的训练，泛化能力。</p>
<p>BN技术指的是使得BN那一层的参数，归一化到指定的mean和variance上。通常现将参数归一化后，然后输入激活函数中。原因是参数经过类似sigmoid这种激活函数，函数值之间的差异变得很小，归一化的作用不如直接进行归一化来得明显，同时可以保证进入sigmoid的时候，参数克服偏移，处在一个比较合适的位置上。</p>
<p>如下图计算出每一层输出z的平均值和方差，归一化参数，在实际运用中，BN通常与mini batch一起使用，利用一个batch的样本计算均值以及标准差。因此为了使得计算出的均值和方差足够代表整个数据集，我们的batch的大小需要尽量的大。</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200506202238045.png" alt="image-20200506202238045" style="zoom:50%;"></p>
<p><strong>优点</strong></p>
<ul>
<li>BN解决了网络训练过程中，每层参数分布变化的情况，参数发生整体偏移将导致网络训练缓慢，落入饱和区，不利于网络的训练，BN后，参数取值比较小且集中，可以使用更大的学习率</li>
<li>BN能够有效防止梯度消失，BN能够使得参数保持在一个合适的区间（尤其sigmoid，tanh）</li>
<li>对参数的初始化具有更强的鲁棒性</li>
<li>防止过拟合，由于每次用batch的均值和方差代替整个数据集的均值方差，因此无形给网络训练加入了噪音，能够提升网络的泛化能力，防止过拟合。</li>
</ul>
<p>由于我们对参数进行BatchNorm操作，将参数分布整体平移到N(0,1)之间，削弱了网络激活函数的非线性部分的作用，因此在进行BN之后，同时学习两个变量，对参数整体进行一个scale 和 shit。</p>
<p><a href="https://perper.site/2019/07/24/normalization/" target="_blank" rel="noopener">https://perper.site/2019/07/24/normalization/</a></p>
<h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h3><p>softmax对应的是hardmax，hardmax表示会将向量表示成one-hot形式，而softmax则比较柔和一点，输出是在0-1之间的概率值，softmax的公式为：<br>$$<br>S_{i}=\frac{e^{i}}{\sum_{j} e^{j}}<br>$$<br>通常会将softmax作为交叉熵的输入，公式如下：<br>$$<br>\text {Loss}=-\sum_{i} t_{i} \ln y_{i}<br>$$<br>其中t为真值，即为1，y即为softmax，对softmax求导的时候，是否需要对分子求导，需要分情况讨论，详情请见：</p>
<p><a href="https://perper.site/2019/02/20/cross-entropy-交叉熵以及softmax/" target="_blank" rel="noopener">https://perper.site/2019/02/20/cross-entropy-%E4%BA%A4%E5%8F%89%E7%86%B5%E4%BB%A5%E5%8F%8Asoftmax/</a></p>
<h3 id="为什么使用卷积网络"><a href="#为什么使用卷积网络" class="headerlink" title="为什么使用卷积网络"></a>为什么使用卷积网络</h3><p>卷积核参数相比全连接层参数指数级下降，卷积核仅有很少的参数，便于训练，卷积核参数少但是能够提取数据的特征，原因如下：</p>
<ul>
<li>卷积权重共享， 即特征的移动不变性，一张图片不同位置的类似特征，可以通过同一个卷积核来提取不影响网络对特征的学习。</li>
<li>稀疏连接，图像的局部特征仅仅依赖于周围的像素特征，因此卷积核不需要太大就可以学到数据所表示的特征。</li>
</ul>
<h3 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h3><p>有些网络非常深，非常大，训练的时候存在梯度消失和梯度爆炸的问题。因此提出<strong>跳跃结构</strong>，最早在resNet这篇文章中提出。即将一层的特征，直接输出到其后的某一层作为输入，因此网络仅仅需要学习一个残差。引入残差也将不同尺度的特征进行了融合。</p>
<h3 id="1x1卷积的作用"><a href="#1x1卷积的作用" class="headerlink" title="1x1卷积的作用"></a>1x1卷积的作用</h3><ul>
<li>1x1卷积最直接的作用是，遍历图像的每一个像素，做一些乘积，累加的操作</li>
<li>用于压缩向量的通道数，减小网络的计算</li>
<li>给网络添加一个非线性变换</li>
</ul>
<h3 id="inception结构"><a href="#inception结构" class="headerlink" title="inception结构"></a>inception结构</h3><p>构建网络的时候，我们需要决定使用多大的卷积核，通常可以选择的有3x3,5x5,7x7等等，inception结构就是代替你来决定，选择出一种最佳的卷积核大小的方案。</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200517165426861.png" alt="image-20200517165426861" style="zoom:50%;"></p>
<p>第二层的特征，由许多不同大小的卷积核共同生成。inception的一个问题是网络的计算量相比使用单一的卷积核来的大。</p>
<h3 id="使用迁移学习"><a href="#使用迁移学习" class="headerlink" title="使用迁移学习"></a>使用迁移学习</h3><p>我们在训练一个网络模型的时候，通常需要重头开始训练整个模型。我们可以使用迁移学习的方法，将一些大型的公共数据集的知识迁移到自己的网络上。具体的做法是将网络的分类softmax层修改了，freeze住其他的层，或者使用别人训练好的backbone，对网络进行微调。</p>
<h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>数据增强的方式其实是对数据的一种扩充，对于绝大多数任务来说，数据量越大，网络的效果越好，例如一些镜像，拉伸，随机crop，颜色通道的变换，对比度，透明度的变换，都是数据增强中比较常用的方式。</p>
<h3 id="增强网络性能"><a href="#增强网络性能" class="headerlink" title="增强网络性能"></a>增强网络性能</h3><ul>
<li>模型集成，同时使用多个独立的模型，得出检测的结果，最后对结果取平均</li>
<li>利用别人已经开源的网络结构，而不是自己重新搭建一个</li>
<li>使用别人的预训练模型</li>
<li>设计合适的手工工程，特征工程（例如人工设计的某个公式，指标等等）</li>
</ul>
<h3 id="yolo-v1的设计"><a href="#yolo-v1的设计" class="headerlink" title="yolo v1的设计"></a>yolo v1的设计</h3><p>yolo v1将图片划分为nxn个网格大小，每一个网格对应长度为m的一个标注[是否有图，x,y,h,w,class1,class2….]，对class的选择取决于对象的中点落在哪个网格区域内。</p>
<p>因此网络的输入是一张图片，经过各种卷积操作，输出是nxnxm的向量。</p>
<p>此外，yolo确定x,y,h,w的方式是和外包围的网格进行比较的，外包围网格第左上角是（0，0），右下角是（1，1）。因此x,y,h,w都是相对值。</p>
<h3 id="交并比IoU"><a href="#交并比IoU" class="headerlink" title="交并比IoU"></a>交并比IoU</h3><p>当候选框与GT框的相交面积比上他们面积的交集的比值，在大多数的情况下，这个值大于0.5则这个框为正类，否则这个框为负类。</p>
<h3 id="非极大值抑制"><a href="#非极大值抑制" class="headerlink" title="非极大值抑制"></a>非极大值抑制</h3><p>非极大值抑制的作用是确保每个对象只检测一次，具体的做法是，同一个目标可能会被多个候选框选中，我们需要在这些候选框中找出最合适作为下一步检测的框。在所有的候选框中找出置信度最高的，然后将其他边框都忽略，这就是非极大值抑制。</p>
<h3 id="anchor-box"><a href="#anchor-box" class="headerlink" title="anchor box"></a>anchor box</h3><p>有些时候，同一个网格中可能有多个目标，这时候一个网格位置需要检测多个目标，我们的做法是使用anchor box，即为每一个网格生成k个候选目标框，k个候选框的大小，长宽比均不同，这些框可以应付不同的目标出现在同一个网格内，然后网络最终的输出是将k个候选框关联起来，共同输出。</p>
<p>通常anchor box的数量会选择8个，长宽比为2：1等。</p>
<h3 id="感受野的计算"><a href="#感受野的计算" class="headerlink" title="感受野的计算"></a>感受野的计算</h3><p>感受野的计算需要逐层往回推，由上一层的感受野来计算下一层的感受野，计算感受野的公式为：<br>$$<br>(n-1)_RF = kernel_Size + stride*(n_RF - 1)<br>$$<br>我们通过第n层的感受野来求解第n-1层的感受野。</p>
<p>详情：<a href="https://zhuanlan.zhihu.com/p/31004121" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31004121</a></p>
<h3 id="卷积feature-map的计算"><a href="#卷积feature-map的计算" class="headerlink" title="卷积feature map的计算"></a>卷积feature map的计算</h3><p>$$<br>H_{o u t}=\left[\frac{H_{i n}+2 \times padding-k e r n e l s i z e}{\operatorname{stride}}\right]+1<br>$$</p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>Dropout技术随机的删除一些神经元，在不同的批量上训练不同的神经网络架构，能够有效的控制过拟合。</p>
<p>具体的，神经元以1-keep-prob的概率被删除，在每一个batch的训练中，实际上是修改的网络的结构，每次训练，参与训练的神经元都有变化，这种正则化方法可以看成是一种集成学习的方法，可显著降低网络的过拟合。</p>
<p>被删除的神经元在梯度回传的时候，忽视这个神经元产生的梯度。</p>
<p><strong>模型测试阶段</strong></p>
<p>模型测试阶段，屏蔽dropout的作用，然后对每一个神经元的参数需要乘上概率p，进行测试。</p>
</div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>WenHui Zhou</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2020/05/03/深度学习知识点总结/">https://wenhui-zhou.github.io/2020/05/03/深度学习知识点总结/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>| 本博客所有文章除特别声明外，均采用 <a href="&quot;http://creativecommons.org/licenses/by-nc-sa/3.0/cn/&quot;" rel="&quot;external" nofollow&quot;="" target="&quot;_blank&quot;">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！ </li></ul></div><br><div class="tags"></div><div class="post-nav"><a class="pre" href="/2020/05/04/优化器总结/">优化器总结</a><a class="next" href="/2020/04/14/cpp常见的考点/">cpp常见的考点</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'civq9nKD49NpRALooR9Llqmf-gzGzoHsz',
  appKey:'JggO9HaSi1Lfx17nt16oDfsI',
  placeholder:'Shall we talk',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/3D重建/">3D重建</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Mac/">Mac</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tensorflow/">Tensorflow</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/effective-cpp/">effective cpp</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/super-resolution/">super resolution</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tool/">tool</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/webSearch/">webSearch</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/xigua/">xigua</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/动画/">动画</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/建站/">建站</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/手撕系列/">手撕系列</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/推荐系统/">推荐系统</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/模型评价/">模型评价</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/比赛/">比赛</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习/">深度学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习总结/">深度学习总结</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法扫盲/">算法扫盲</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/统计学习方法/">统计学习方法</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程/">编程</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/论文复现/">论文复现</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/论文阅读/">论文阅读</a><span class="category-list-count">23</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/论文阅读/3D重建/">3D重建</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/面试准备/">面试准备</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目/">项目</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目总结/">项目总结</a><span class="category-list-count">5</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/项目总结/论文阅读/">论文阅读</a><span class="category-list-count">3</span></li></ul></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/SR/" style="font-size: 15px;">SR</a> <a href="/tags/dialog/" style="font-size: 15px;">dialog</a> <a href="/tags/leetcode/" style="font-size: 15px;">leetcode</a> <a href="/tags/项目总结/" style="font-size: 15px;">项目总结</a> <a href="/tags/3D重建/" style="font-size: 15px;">3D重建</a> <a href="/tags/c/" style="font-size: 15px;">c++</a> <a href="/tags/论文阅读/" style="font-size: 15px;">论文阅读</a> <a href="/tags/netStation/" style="font-size: 15px;">netStation</a> <a href="/tags/tool/" style="font-size: 15px;">tool</a> <a href="/tags/interview/" style="font-size: 15px;">interview</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/tags/tip/" style="font-size: 15px;">tip</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/learning-cpp/" style="font-size: 15px;">learning cpp</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/—-leetcode/" style="font-size: 15px;">— leetcode</a> <a href="/tags/职业规划/" style="font-size: 15px;">职业规划</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/tips/" style="font-size: 15px;">tips</a> <a href="/tags/超分辨率/" style="font-size: 15px;">超分辨率</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/06/06/面试试题准备/">面试试题准备</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/04/手势识别/">手势识别</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/27/常见的目标检测网络/">常见的目标检测网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/26/常见数据结构/">常见数据结构</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/21/序列化RNN系列/">序列化RNN系列</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/13/2D-animation-SVG文件/">2D animation,SVG文件</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/12/RDSNet总结文档/">RDSNet总结文档</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/11/约束项以及约束的含义/">约束项以及约束的含义</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/04/优化器总结/">优化器总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/03/深度学习知识点总结/">深度学习知识点总结1</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/WenHui-Zhou" title="GITHUB" target="_blank">GITHUB</a><ul></ul><a href="http://www.google.com/" title="GOOGLE" target="_blank">GOOGLE</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">WenHuiZhou.</a> 访问人数:<span id="busuanzi_value_site_uv"></span> 
访问量:<span id="busuanzi_value_site_pv"></span></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script> </div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>