<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="cheer up"><meta name="baidu-site-verification" content="9pSIuwCbvi"><meta name="google-site-verification" content="YzcCTjF6VoVlNAtL37_S4vFjzFwYTAFZzD51Il2IGKY"><title>常见的目标检测网络 | WenHuiZhou</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">常见的目标检测网络</h1><a id="logo" href="/.">WenHuiZhou</a><p class="description">perper（打起精神！）</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">常见的目标检测网络</h1><div class="post-meta">May 27, 2020<span> | </span><span class="category"><a href="/categories/深度学习总结/">深度学习总结</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" href="/2020/05/27/常见的目标检测网络/#vcomment"><span class="valine-comment-count" data-xid="/2020/05/27/常见的目标检测网络/"></span><span> 条评论</span></a><div class="post-content"><p>总结常见的目标检测网络，持续更新，文章要写很长。</p>
<p>[TOC]</p>
<a id="more"></a>
<h3 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h3><p>目标检测任务可以理解为目标的识别和定位，识别指的是判断物体的类别，定位指的是确定物体的位置。</p>
<p>目前的目标检测模型分为两类，一类是two-stage，这一类的典型代表是RCNN，Fast-RCNN，Faster-RCNN家族。他们的特点是检测精度高，识别错误率低，但是速度较慢，不能满足实时场景的需求。另一类是one-stage，他们典型的网络有yolo，SSD，yolo2并且准确率能够和two-stage基本持平。</p>
<h3 id="RCNN-2014"><a href="#RCNN-2014" class="headerlink" title="RCNN(2014)"></a>RCNN(2014)</h3><p>RCNN详解：<a href="https://perper.site/2019/02/11/RCNN详解/" target="_blank" rel="noopener">https://perper.site/2019/02/11/RCNN%E8%AF%A6%E8%A7%A3/</a></p>
<p><strong>大致流程：</strong></p>
<ul>
<li>首先通过select search算法选择出图像中可能出现目标的box，最终得到2000个框<ul>
<li>将图像过分割出2k-3k个框</li>
<li>对框进行合并（颜色相近，梯度相近等）</li>
<li>输出曾经有出现过的框</li>
</ul>
</li>
<li>将输出的候选框依次输入到卷积网络中进行网络训练</li>
<li>将cf7层得到4096维特征输出到SVM中，训练N个SVM二分类器，对每个框进行二分类</li>
<li>边框回归：边框回归主要学习四个参数分别是x，y的偏移和w，h的缩放。通过优化二模和参数二次项，得到四个变换的参数，然后将参数乘以相应的边得到最终的边框位置（位移和缩放）</li>
</ul>
<p>RCNN的流程如下图：</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200528011717296.png" alt="image-20200528011717296" style="zoom:50%;"></p>
<p>该网络的主要缺陷在于：</p>
<ul>
<li>候选框由select search算法产生，需要花费很长时间</li>
<li>对2k个候选框，均需要从头做卷积，存在很多的冗余计算</li>
<li>模型确定的情况只能接受固定大小的输入</li>
<li>网络训练过程分段，过程繁琐</li>
</ul>
<h3 id="Fast-RCNN-2015"><a href="#Fast-RCNN-2015" class="headerlink" title="Fast-RCNN(2015)"></a>Fast-RCNN(2015)</h3><p>Fast-RCNN详解：<a href="https://perper.site/2019/02/14/Fast-RCNN详解/" target="_blank" rel="noopener">https://perper.site/2019/02/14/Fast-RCNN%E8%AF%A6%E8%A7%A3/</a></p>
<p><strong>大致流程：</strong></p>
<ul>
<li>select search 算法提取2k个候选框</li>
<li>将图片整体输入卷积网络中，得到整个图片的feature map</li>
<li>通过RoI projection（通过感受野的映射公式）将2k个region proposal 映射到feature map</li>
<li>通过RoI pooling变成一样大，然后经过全连接层等接入softmax分类，已经bbox边框回归</li>
</ul>
<p><strong>RoI pooling</strong></p>
<p>RoI pooling将传入不同大小的region proposal划分层h x w个网格，对每个网格做max pooling，因此得到的输出大小是一致的。</p>
<p>RoI pooling反向传播过程中，原则是，max value那个位置梯度为1，其他位置为0。由于不同的region可能存在重叠的部分，因此重叠部分的梯度是每个region 梯度的求和。</p>
<p><strong>Loss</strong></p>
<p>分类部分，Fast RCNN使用的是softmax + 交叉熵：</p>
<p>softmax：<br>$$<br>P_{i}=\frac{e^{V_{i}}}{\sum_{i}^{C} e^{V_{i}}}<br>$$<br>交叉熵：<br>$$<br>L = -\sum(plog(p))<br>$$</p>
<p>边框回归使用smooth L1 loss代替原来的L2loss：<br>$$<br>\operatorname{smooth}_{L_{1}}(x)=\left{\begin{array}{lr}0.5 x^{2} &amp; \text { if }|x|&lt;1 \ |x|-0.5 &amp; \text { otherwise }\end{array}\right.<br>$$<br><strong>网络流程：</strong></p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200528111556483.png" alt="image-20200528111556483" style="zoom:50%;"></p>
<p><strong>缺点：</strong></p>
<ul>
<li>select search 算法提取候选框效率太低了（CPU上），每张图耗时3s</li>
</ul>
<h3 id="Faster-RCNN-2016"><a href="#Faster-RCNN-2016" class="headerlink" title="Faster RCNN(2016)"></a>Faster RCNN(2016)</h3><p>Faster RCNN详解：<a href="https://perper.site/2019/02/14/Faster-RCNN详解/" target="_blank" rel="noopener">https://perper.site/2019/02/14/Faster-RCNN%E8%AF%A6%E8%A7%A3/</a></p>
<p><strong>大致流程：</strong></p>
<ul>
<li>将图片输入到卷积网络中，生成feature map</li>
<li>将feature map输入RPN层（region proposal network）中，输出proposal region</li>
<li>提取出proposal region内的feature map 输入到RoI pooling层，输出固定维度的向量</li>
<li>对该向量进行softmax分类以及边框回归</li>
</ul>
<p><strong>RPN层</strong></p>
<p>RPN即region proposal network，输入feature map，输出为候选框的位置。RPN网络有两个分支。一个分支是softmax进行前后景的分类。另一支是边框回归（同RCNN），修正region proposal的位置。</p>
<p>Faster rcnn最后产生300个proposal region进行下一步边框位置的精修以及类别的判别。</p>
<p><strong>前后景分类</strong></p>
<p>在feature map的每一个像素点上，生成9个长宽比不同的anchor作为初始的anchor，然后第一个分支用softmax对每一个边框判断是前景还是背景。对于GT来说，前背景anchor标签分配的规则是：</p>
<ul>
<li>与某个GT的IoU最大的为前景</li>
<li>与任意GT IoU大于0.7为前景</li>
<li>与任意GT的IoU小于0.3为负类</li>
</ul>
<p>Faster RCNN在速度上提升很多，但是每秒仅能处理5张图片，因此one stage算法脱颖而出。</p>
<h3 id="Yolo-V1-2016"><a href="#Yolo-V1-2016" class="headerlink" title="Yolo V1(2016)"></a>Yolo V1(2016)</h3><p>Yolo V1 详解：<a href="https://perper.site/2019/03/07/YOLO-V1-详解/" target="_blank" rel="noopener">https://perper.site/2019/03/07/YOLO-V1-%E8%AF%A6%E8%A7%A3/</a></p>
<p><strong>大致流程：</strong></p>
<ul>
<li>将图片resize之后，划分成7x7的网格，每个网格负责检测两个bounding box以及一个confidence。每个网络输出还带着一个one hot，表示类别的检测</li>
<li>将图片resize后，输入网络，最终输出7x7x2 + 2 + 7x7xclasses的向量</li>
<li>将网络预测出的含有对象的框做一个NMS非极大值抑制，剔除掉目标的重复识别</li>
</ul>
<p><strong>一些注意点</strong></p>
<ul>
<li><p>GT的anchor的confidence是否为1，判断标准是一个对象的中心是否落在该anchor上</p>
</li>
<li><p>对于每一个对象，用对象中心的落在的网格来预测这个对象的边框，该边框的confidence即为1</p>
</li>
<li>边框位置在网络中使用网格边长进行归一化，因此数值大小都比较小</li>
<li>网络的损失函数的设计目标就是让坐标（x,y,w,h），confidence，classification 这个三个方面达到很好的平衡，共包含三个部分，坐标，置信度，已经类别分类。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>yolo v1对相邻的目标检测效果不好，原因是每个网格只有两个anchor，并且只能检测一个类别，限制了密集对象的检测，以及小目标的检测</li>
<li>yolo v1难以检测异常长款比的目标</li>
<li>损失函数中大检测框与小检测框出现的误差是同等对待的，但是实际中，误差对小检测框的影响远大于大检测框。（出现在小检测框的误差通常无法容忍，比如位置偏差很大，但是大检测框影响不是那么明显）</li>
</ul>
<p>Yolo v1是第一个one stage的目标检测算法，他的速度能够达到每秒45张图片。</p>
<h3 id="Yolo-V2"><a href="#Yolo-V2" class="headerlink" title="Yolo V2"></a>Yolo V2</h3><p>Yolo V2详解：<a href="https://perper.site/2019/03/08/YOLO-V2-V3详解/" target="_blank" rel="noopener">https://perper.site/2019/03/08/YOLO-V2-V3%E8%AF%A6%E8%A7%A3/</a></p>
<p>Yolo V2和v1在网络结构上相同，使用同一个框架，他的主要贡献是在网络的优化上做了很多工作。</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200529101019105.png" alt="image-20200529101019105" style="zoom:50%;"></p>
<p>每一点都可能是一个考点，好好复习一下。</p>
<h4 id="BatchNorm"><a href="#BatchNorm" class="headerlink" title="BatchNorm"></a><strong>BatchNorm</strong></h4><ul>
<li>有助于网络反向传播过程中梯度消失和梯度爆炸问题</li>
<li>降低网络对超参数的敏感性</li>
<li>BN对batch归一化，起到一定的正则化作用，获得更好的收敛速度</li>
<li>yolo v2中，在卷积后加入batchnorm替代dropout，效果提升2.4%</li>
</ul>
<p><strong>需要 BatchNorm 原因</strong></p>
<ul>
<li>在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化。因此深层次网络需要不断的调整来适应输入数据的变化，导致了学习效率的降低。</li>
<li>当我们使用一些类似sigmoid，tanh这种激活函数时，由于参数大小变大，导致梯度容易陷入梯度饱和区，网络难以收敛。</li>
</ul>
<p><strong>BatchNorm 操作</strong></p>
<ul>
<li>对每一层的参数特征进行归一化处理，将特征变换到0，方差1</li>
<li>对归一化后的参数做一次线性变换，$Z 1=Y Z+\beta$ ,使得变换后的参数不会丢失太多信息</li>
</ul>
<h4 id="高分辨率图像微调模型"><a href="#高分辨率图像微调模型" class="headerlink" title="高分辨率图像微调模型"></a>高分辨率图像微调模型</h4><p>yolo v2由于分类的数据集比含边框标注的数据集来的大，同时分类数据集分辨率比较低，作者使用高分辨率图像对模型进行微调，先是用224图像做分类，然后扩大到448，最后使用448的含边框的样本进行训练。</p>
<h4 id="使用先验框"><a href="#使用先验框" class="headerlink" title="使用先验框"></a>使用先验框</h4><p>yolo v1在检测目标时，由于目标尺度变换范围大，因此在精确定位方面难以得到准确的定位。Yolo v2采用了Faster RCNN的RPN思路，在每个grid预先设定一组不同大小和宽高比的边框（9个），来覆盖整个图像的不同位置和多种尺度，对特征图（feature map）进行卷积来预测每个位置的边界框以及置信度（是否含有物体）。</p>
<p>v2输出feature map大小比v1要大，达到13 x 13。因此v2候选框13x13x9（v1为98）,因此 anchor的检测精度略为降低（map降低0.2），但是yolo v2的召回率大大提升，因为每张图的候选框大大增加了。（提升7%）。</p>
<h4 id="聚类提取先验框尺度"><a href="#聚类提取先验框尺度" class="headerlink" title="聚类提取先验框尺度"></a>聚类提取先验框尺度</h4><p>对grid预设一组先验框大小，之前都是手工设定。Yolo v2对测试集中的先验框进行聚类分析，以寻找最可能匹配样本的边框尺寸。</p>
<p>作者采用kmean进行聚类，选用box与聚类中心box之间的IOU值作为距离指标，IoU越大，相互距离越小，最终作者选择了五个先验框作为聚类的类别数，他的效果和人工选择9个框的效果一致。</p>
<h4 id="约束预测边框的位置"><a href="#约束预测边框的位置" class="headerlink" title="约束预测边框的位置"></a>约束预测边框的位置</h4><p>yolov2 采用不同于RPN的边框回归的方法，yolov2回归的目标是预测边界框中心点相对于对应cell左上角位置的相对偏移。</p>
<h4 id="使用darknet-19作为backbone"><a href="#使用darknet-19作为backbone" class="headerlink" title="使用darknet-19作为backbone"></a>使用darknet-19作为backbone</h4><p>作者使用了自己训练的darknet-19来提取图片特征，darknet 模型参数小，运算速度快（快了33%）</p>
<h4 id="使用数据多尺度训练"><a href="#使用数据多尺度训练" class="headerlink" title="使用数据多尺度训练"></a>使用数据多尺度训练</h4><p>由于YOLOv2模型中只有卷积层和池化层，为了增强模型的鲁棒性，YOLOv2采用了多尺度输入训练策略，具体来说就是在训练过程中每间隔一定的iterations之后改变模型的输入图片大小。YOLOv2的下采样总步长为32，输入图片大小选择一系列为32倍数的值。</p>
<h3 id="Yolo-V3"><a href="#Yolo-V3" class="headerlink" title="Yolo V3"></a>Yolo V3</h3><p>yolo v3在v2的基础上进一步对网络进行优化，主要的改进有调整了网络结构，加入了残差模块。利用多尺度检测对对象进行检测，利用logistics替代softmax进行分类。</p>
<p><strong>新的网络结构Darknet-53</strong></p>
<p>加深了backbone的深度，加入了卷积、BN、leaky Relu等组件。</p>
<p><strong>多尺度检测</strong></p>
<p>Yolo V3在三个不同尺度的特征图上对对象进行检测，能够检测到更加细粒度的特征。分别在1/32，1/16，1/8，克服网络难以检测细粒度的对象。</p>
<p><strong>对象分类由softmax改成logistics</strong></p>
<p>预测对象类别时不使用softmax，改成使用logistic的输出进行预测。这样能够支持多标签对象（比如一个人有Woman 和 Person两个标签）。</p>
<h3 id="one-stage-与-two-stage-主要区别"><a href="#one-stage-与-two-stage-主要区别" class="headerlink" title="one stage 与 two stage 主要区别"></a>one stage 与 two stage 主要区别</h3><p><strong>two stage 算法</strong>首先通过启发式方法，或RPN层，产生一系列稀疏候选框，然后对候选框进行分类与回归，速度比较慢，精度比较高。</p>
<p><strong>one stage 算法</strong>则是在图片不同位置进行密集采样，抽样时选择不同的长宽比，然后CNN提取特征后，直接进行分类与回归，速度比较快，精度比较低，正负样本不均衡。</p>
<h3 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h3><p>SSD详解：<a href="https://perper.site/2019/03/10/SSD-M2Det-详解/" target="_blank" rel="noopener">https://perper.site/2019/03/10/SSD-M2Det-%E8%AF%A6%E8%A7%A3/</a></p>
<p><strong>大致流程：</strong></p>
<ul>
<li>将图像输入到网络中</li>
<li>选择卷积的中间层，如在38x38，19x19上的每个位置，选择对应横纵比不同的anchor4个，最后共有8732个候选框</li>
<li>将候选框进行NMS之后，输入loss中，loss由位置损失和置信度损失共同组成</li>
</ul>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200530131928769.png" alt="image-20200530131928769" style="zoom:50%;"></p>
<p>SSD有两个重要的特点：</p>
<ul>
<li>提取不同尺度的特征图来做检测，大尺度的特征检测较小物体，小尺度特征检测较大物体</li>
<li>SSD采用不同尺度和长宽比的先验框，先验框的计算和feature map的大小有关</li>
</ul>
<p><strong>Loss</strong></p>
<p>SSD的loss由位置loss以及分类softmax组成。位置loss与Faster RCNN相同。</p>
<h3 id="RetinaNet"><a href="#RetinaNet" class="headerlink" title="RetinaNet"></a>RetinaNet</h3><p>retinanet详解：<a href="https://perper.site/2019/05/16/RetinaNet-原理记录/" target="_blank" rel="noopener">https://perper.site/2019/05/16/RetinaNet-%E5%8E%9F%E7%90%86%E8%AE%B0%E5%BD%95/</a></p>
<p><a href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4" target="_blank" rel="noopener">https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4</a></p>
<p><strong>大致流程：</strong></p>
<ul>
<li>retinanet使用resnet作为backbone，提取图像的feature map</li>
<li>将feature map输入三层的金字塔结构，并做特征融合（层对层）</li>
<li>随后在每一层上划分proposal anchor，anchor的大小和层数存在对应关系</li>
<li>将这些proposal输入类别分类和边框回归中，得到最终的结果</li>
</ul>
<p><strong>retinanet金字塔结构</strong></p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200601102057512.png" alt="image-20200601102057512" style="zoom:50%;"></p>
<p>即resnet最后三层C3,C4,C5生成p3,p4,p5，随后p5生成p6,p7。在每一层feature map的anchor大小的设置如下：</p>
<p>在设置anchor的时候，作者选用了一下几种设置：</p>
<p>anchor-size = [32, 64, 128, 256, 512] 对应P3～P7</p>
<p>anchor—scale = [2 x x0 ，2 x x(1/3 )，2 x x (2/3)]</p>
<p>anchor-wh = [1:2 ，1 ，2:1]</p>
<p>每一层anchor的大小为anchor-size 乘以 anchor-scale。然后使用三种长宽比，每一层，每一个位置得到九种大小的anchor。随后对这些位置的anchor进行边框回归以及类别的回归。</p>
<p><strong>focal loss</strong></p>
<p>one stage 算法在选择候选框的时候通常在每个像素位置上选择9个anchor，因此候选框的数量非常的多，大概有10万个，但是传统的two stage算法提取出的候选框的个数，通常在2k个。one stage算法选择的候选框绝大多数是一些easy sample，即可以置信度很高的认定为背景类别。这一类样本对网络训练产生影响：</p>
<ul>
<li>网络训练是easy negativate 样本对loss不起作用，网络收敛很慢</li>
<li>easy negativate样本数量过多，因此在loss收敛的过程中，真正的收敛方向会被覆盖掉，导致模型精度下降</li>
</ul>
<p>因此本文在cross entropy的基础上，提出了focal loss。</p>
<p>原始交叉熵loss：<br>$$<br>\operatorname{CE}(p, y)=\left{\begin{array}{ll}-\log (p) &amp; \text { if } y=1 \ -\log (1-p) &amp; \text { otherwise }\end{array}\right.<br>$$<br>focal loss：<br>$$<br>\mathrm{FL}\left(p_{\mathrm{t}}\right)=-\left(1-p_{\mathrm{t}}\right)^{\gamma} \log \left(p_{\mathrm{t}}\right)<br>$$<br>即对置信度高的样本分配比较小的loss权重，对难学的样本（置信度低）分配比较高的梯度。</p>
<p>实际使用是使用了一个参数a变量作为调节：<br>$$<br>\mathrm{FL}\left(p_{\mathrm{t}}\right)=-\alpha_{\mathrm{t}}\left(1-p_{\mathrm{t}}\right)^{\gamma} \log \left(p_{\mathrm{t}}\right)<br>$$<br>常见网络的anchor数量：</p>
<ul>
<li><a href="https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89" target="_blank" rel="noopener">YOLOv1</a>: 98 boxes</li>
<li><a href="https://towardsdatascience.com/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65" target="_blank" rel="noopener">YOLOv2</a>: ~1k</li>
<li><a href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" target="_blank" rel="noopener">OverFeat</a>: ~1–2k</li>
<li><a href="https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11" target="_blank" rel="noopener">SSD</a>: ~8–26k</li>
<li><strong>RetinaNet: ~100k</strong>.</li>
</ul>
<p><strong>分类以及边框回归</strong></p>
<p>分类以及回归模块网络结构相同，参数不共享，回归模块只会回归1k top score的边框，置信度threshhold设置为0.05。检测得到anchor之后，使用NMS进行过滤，NMS的阈值为0.5。</p>
<h3 id="MobileNet-2017"><a href="#MobileNet-2017" class="headerlink" title="MobileNet (2017)"></a>MobileNet (2017)</h3><p>mobilenet详解：<a href="https://perper.site/2019/03/03/MobileNets-详解/" target="_blank" rel="noopener">https://perper.site/2019/03/03/MobileNets-%E8%AF%A6%E8%A7%A3/</a></p>
<p>mobileNet的核心改进在于将传统CNN结构进行近似的转换，直接降低了网络的运算量以及参数量。</p>
<p><strong>深度可分离卷积</strong></p>
<p>深度可分离卷积即将传统的卷积分解成<strong>深度卷积</strong>与<strong>逐点卷积</strong>的组合。</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200601222900682.png" alt="image-20200601222900682" style="zoom:50%;"></p>
<p> 接下来通过计算两种卷积减小的计算量来比较两种卷积的差别：</p>
<p><strong>普通卷积</strong></p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200601223850349.png" alt="image-20200601223850349" style="zoom:50%;"></p>
<p>计算量：<br>$$<br>D_k <em> D_k </em> D_w <em> D_H </em> M<em>N<br>$$<br>参数量：<br>$$<br>D_k </em> D_k <em> N<br>$$<br><em>*深度可分离卷积</em></em></p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200601224309892.png" alt="image-20200601224309892" style="zoom:50%;"></p>
<p>计算量：<br>$$<br>D_k <em> D_k</em>D_w<em>D_H</em>M + D_w<em>D_H</em>N<em>M<br>$$<br>参数量：<br>$$<br>D_k</em>D_k<em>M + M</em>N<br>$$<br>对比两者的参数量以及计算量，深度分离卷积均下降为原来的：<br>$$<br>\frac{1}{N}+\frac{1}{D_{K}^{2}}<br>$$<br><strong>网络结构的改变</strong></p>
<p>下面以一个简单的例子说明对CNN结构的分解：</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200601225407817.png" alt="image-20200601225407817" style="zoom:50%;"></p>
<p>将conv改成了Depthwidth和1X1的conv，新增了一层BN层，特别的ReLU6指的是：</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200601225529721.png" alt="image-20200601225529721" style="zoom:50%;"><br>$$<br>\operatorname{ReLU}(6)=\min (\max (0, x), 6)<br>$$<br>作者为ReLU设置了一个最大值6，作者认为非线性激活函数在低精度计算下具有更高的鲁棒性。</p>
<p><strong>width multipler</strong></p>
<p>引入一个参数$\alpha$，用于缩放通道的数量，均匀的对网络进行减负，即参数量乘以M修改成乘以$\alpha M$，$N$ 变成 $\alpha N$。</p>
<p><strong>resolution Multiplier </strong></p>
<p>引入参数$\rho$，用来缩小图片卷积得到的分辨率、因此得到的参数量也小了$\rho$。</p>
<p><strong>损失函数</strong></p>
<p>网络的损失函数使用的是softmax + 交叉熵loss。</p>
<h3 id="mobileNet-V2-2018"><a href="#mobileNet-V2-2018" class="headerlink" title="mobileNet V2(2018)"></a>mobileNet V2(2018)</h3><p>mobileNet v2详解：<a href="https://perper.site/2019/03/04/MobileNet-V2-详解/" target="_blank" rel="noopener">https://perper.site/2019/03/04/MobileNet-V2-%E8%AF%A6%E8%A7%A3/</a></p>
<h4 id="mobileNet-v2特点"><a href="#mobileNet-v2特点" class="headerlink" title="mobileNet v2特点"></a>mobileNet v2特点</h4><ul>
<li>v1在处理低精度数据时，depthwise convolution导致特征退化，卷积核特征边为0。v2引入linear bottleneck结构，去掉部分relu结构</li>
<li>v1直筒型结构印象网络性能，因此v2引入short skip connection（inverted residual）</li>
</ul>
<h4 id="linear-bottleneck"><a href="#linear-bottleneck" class="headerlink" title="linear bottleneck"></a>linear bottleneck</h4><p><strong>原因：</strong>对低维度做ReLU运算，很容易造成信息的丢失。而在高维度进行ReLU运算的话，信息的丢失则会很少，即卷积核中很多为0的项。</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200602135934414.png" alt="image-20200602135934414" style="zoom:40%;"></p>
<p>原因为低纬度数据经过ReLU后，逆操作还原回来的数据信息丢失严重。</p>
<p>因此作者提出，将最后一个relu6换成linear结构，即linear bottleneck。</p>
<p>修改结构后有如下的变化：</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200602154310452.png" alt="image-20200602154310452" style="zoom:40%;"></p>
<p>左边是mobilenet原始的结构，右边是转换后，将最后一个relu6换成linear，stride=1时添加shortcut，stride=2不添加shortcut。</p>
<h4 id="inverted-residuals"><a href="#inverted-residuals" class="headerlink" title="inverted residuals"></a>inverted residuals</h4><p>V1是一个直筒型结构，V2希望利用数据的多尺度特征，因此引入了shortcut结构：</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200602153027233.png" alt="image-20200602153027233" style="zoom:40%;"></p>
<p>mobileNet V2 先进行了升维然后进行降维，与resnet中shortcut结构相反，因此称为inverted residuals。</p>
<p>由于深度卷积本身无法改变特征的维度，而低纬度的特征效果不好，因此先通过一个1x1的结构进行升维，V2结构中，第一个1x1的结构是一个升维结构，即增加特征图的通道数，然后DW之后，通过一个1x1的结构进行降维。</p>
<h3 id="moblieNet-V3-2019"><a href="#moblieNet-V3-2019" class="headerlink" title="moblieNet V3(2019)"></a>moblieNet V3(2019)</h3><p>mobileNet V3使用了 <strong>神经结构搜索NAS</strong>来完成V3的结构优化。</p>
<p><strong>mobileNet V3中的技术</strong></p>
<ul>
<li>神经结构搜索实现的网络框架</li>
<li>引入V1中的深度可分离卷积</li>
<li>引入V2中的linear以及倒残差</li>
<li>引入squeeze and excitation轻量级注意力机制</li>
<li>使用一种新的激活函数h-swish</li>
<li>修改v2网络最后阶段，保留更高维度数据</li>
</ul>
<p><strong>h-swish激活函数</strong><br>$$<br>\mathrm{h}-\operatorname{swish}[x]=x \frac{\operatorname{ReLU} 6(x+3)}{6}<br>$$<br><strong>网络结构搜索NAS</strong></p>
<p>资源受限NAS：用于计算和参数量受限情况下，搜索网络来优化各个块</p>
<p>netAdapt：用于每一个模块确定之后，网络层微调每一层卷积核的数量，称为层级搜索</p>
<h3 id="anchor-free系列"><a href="#anchor-free系列" class="headerlink" title="anchor free系列"></a>anchor free系列</h3><h3 id="CornerNet"><a href="#CornerNet" class="headerlink" title="CornerNet"></a>CornerNet</h3><p>CornerNet将传统的anchor-base的检测方法，过渡到anchor free的方法上，文中认为anchor有一些缺点：</p>
<ul>
<li>anchor过多，出现样本不均衡的问题</li>
<li>anchor box 带来了大量的超参数</li>
</ul>
<p>网络结构如下：</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200605221207921.png" alt="image-20200605221207921" style="zoom:50%;"></p>
<p>网络由三部分组成，分别是hourglass，top left Corner prediction，bottom right Corner prediction。</p>
<p><strong>大致步骤：</strong></p>
<ul>
<li>将图片输入 hourglass 用于提取图片特征</li>
<li>得到特征，经过Corner prediction module预测角点的位置，首先经过Corner pooling 结构，然后输出三个特征，分别是Corner的heatmap，角点的embedding，用于匹配另一个角点，offerses：卷积过程中出现的偏差</li>
<li>随后通过优化loss，得到预测效果好的网络</li>
</ul>
<p><strong>corner pooling：</strong></p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200605222258791.png" alt="image-20200605222258791" style="zoom:50%;"></p>
<p>pooling的方式如上，每一个位置的值取向右，向下的最大值作为当前的值。该pooling方式有效的解释：</p>
<ol>
<li>每个顶点只与边界框的两条边相关，所以corner 更容易提取。2</li>
<li>顶点更有效提供离散的边界空间</li>
</ol>
<p><strong>Prediction Module</strong></p>
<p>prediction module的第一个输出是<strong>heatmaps</strong>，heatmaps包含C个通道（C是目标的类别，没有背景类别），每个channel是二进制掩膜，表示相应类别的角点位置。对于每个角点，只有一个ground-truth，其他位置都是负样本。在训练过程，模型减少负样本，在每个ground-truth角点设定半径r区域内都是正样本，因为r内的角点，都可以产生合适的边框。（论文中正负边框IoU为0.7）</p>
<p>定点的损失：<br>$$<br>L_{d e t}=\frac{-1}{N} \sum_{c=1}^{C} \sum_{i=1}^{H} \sum_{j=1}^{W}\left{\begin{array}{cc}\left(1-p_{c i j}\right)^{\alpha} \log \left(p_{c i j}\right) &amp; \text { if } y_{c i j}=1 \ \left(1-y_{c i j}\right)^{\beta}\left(p_{c i j}\right)^{\alpha} \log \left(1-p_{c i j}\right) &amp; \text { otherwise }\end{array}\right.<br>$$<br>其中$p_{cij}$ 表示i,j位置上的类别为c的概率，其中$y_{cij}$ 表示GT的i,j位置上半径为r的高斯值。</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200605225958982.png" alt="image-20200605225958982" style="zoom:50%;"></p>
<p><strong>offsets</strong></p>
<p>模型在卷积过程中造成的位置偏移，一些取整等操作造成的，模型通过最优化一个smooth L1来优化：<br>$$<br>L_{o f f}=\frac{1}{N} \sum_{k=1}^{N} \operatorname{SmoothL} 1 \operatorname{Loss}\left(\boldsymbol{o}_{k}, \hat{\boldsymbol{o}}_{k}\right)<br>$$<br><strong>embedding</strong></p>
<p>模型输出的第三部分，是左上角Corner和右下角Corner相互识别的标识，损失函数通过最小化同一个框的距离，最大化不同框的距离来达到目的。<br>$$<br>\begin{array}{l}L_{p u l l}=\frac{1}{N} \sum_{k=1}^{N}\left[\left(e_{t_{k}}-e_{k}\right)^{2}+\left(e_{b_{k}}-e_{k}\right)^{2}\right] \ L_{p u s h}=\frac{1}{N(N-1)} \sum_{k=1}^{N} \sum_{j=1}^{N} \max \left(0, \Delta-\left|e_{k}-e_{j}\right|\right)\end{array}<br>$$<br>$e_k$ 表示两个点的均值，$e_{tk}$ 表示左上角，$e_{lk}$表示右下角，第一个式子将同一个框的角拉近。第二个式子将不同框的角拉远。</p>
<p>总体的loss为：<br>$$<br>L=L_{d e t}+\alpha L_{p u l l}+\beta L_{p u s h}+\gamma L_{o f f}<br>$$</p>
<h3 id="FCOS"><a href="#FCOS" class="headerlink" title="FCOS"></a>FCOS</h3><p>FCOS一种one stage的anchor free目标检测算法，类似于语义分割，通过逐像素预测的方式来解决目标检测的问题。</p>
<p>剔除了anchor，避免了anchor各种超参的计算，IoU的计算等。</p>
<p><strong>网络结构</strong></p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200606100058559.png" alt="image-20200606100058559" style="zoom:50%;"></p>
<p><strong>逐像素回归</strong></p>
<p>anchor base模型，对IoU大于阈值的边框，进行边框回归，回归的规则是预测框中点，到GT边的距离。</p>
<p>FCOS则是对目标框中，所有的点都做边框回归，每个点回归到box的上下左右四个距离：<br>$$<br>\begin{array}{l}l^{<em>}=x-x_{0}^{(i)}, \quad t^{</em>}=y-y_{0}^{(i)} \ r^{<em>}=x_{1}^{(i)}-x, \quad b^{</em>}=y_{1}^{(i)}-y\end{array}<br>$$<br><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200606104920048.png" alt="image-20200606104920048" style="zoom:50%;"></p>
<p>对于目标框重叠的情况，作者选择的是小样本的框进行回归。</p>
<p>不同feature map上的点和GT进行比较之前需要将feature map每个点映射到原图上，转换关系为：</p>
<p> <strong>(floor(s/2) + x*s, floor(s/2) + y*s)</strong> ，s为stride。</p>
<p>FCOS对每一个像素直接回归box，例如使用coco数据，最终网络的输出为4维的box，FCOS训练了C个二分类网络，最终得到4 + C维的输出。</p>
<p>损失函数如下：<br>$$<br>\begin{aligned} L\left(\left{\boldsymbol{p}_{x, y}\right},\left{\boldsymbol{t}_{x, y}\right}\right) &amp;=\frac{1}{N_{\text {pos }}} \sum_{x, y} L_{\text {cls }}\left(\boldsymbol{p}_{x, y}, c_{x, y}^{<em>}\right) \ &amp;+\frac{\lambda}{N_{\text {pos }}} \sum_{x, y} \mathbb{1}_{\left{c_{x, y}^{</em>}&gt;0\right}} L_{\text {reg }}\left(\boldsymbol{t}_{x, y}, \boldsymbol{t}_{x, y}^{*}\right) \end{aligned}<br>$$<br>第一项为focal loss损失用于像素的分类，第二项为IoU 损失，用于像素点的回归。</p>
<p>由于FCOS利用逐像素进行回归，因此有大量的正样本参与训练，相比于anchor base方法，不存在正负样本不均衡的问题。</p>
<p><strong>多尺度识别</strong></p>
<p>网络将特征输入特征金字塔中（与retinaNet相同），每一级的特征都做边框回归，特征分类等。这样在feature map比较深的特征上可以检测大目标，比较浅的feature map上可以检测比较小的目标。然后通过一定规则放大到正常图像的尺度。有效提升重叠目标的检测性能。</p>
<p>作者通过一个超参数m来限制框的回归范围，如果一个框：<br>$$<br>\max \left(l^{<em>}, t^{</em>}, r^{<em>}, b^{</em>}\right)&gt;m_{i} \text { or } \max \left(l^{<em>}, t^{</em>}, r^{<em>}, b^{</em>}\right)&lt;m_{i-1}<br>$$<br>即视为是负样本，否则是正样本。对负样本不进行回归。这样的目的是对一些偏离中心很远的box忽略掉。</p>
<p><strong>center-ness</strong></p>
<p>作者发现fpn层预测的结果比retinanet差2%，原因是FCOS在远离目标中心预测了很多低质量的框，</p>
<p>因此作者在分类中多引入一个分支，对框的距离进行约束，即center-ness：<br>$$<br>\text { centerness }^{<em>}=\sqrt{\frac{\min \left(l^{</em>}, r^{<em>}\right)}{\max \left(l^{</em>}, r^{<em>}\right)} \times \frac{\min \left(t^{</em>}, b^{<em>}\right)}{\max \left(t^{</em>}, b^{*}\right)}}<br>$$<br>在训练的过程中我们会约束中center-ness的值，使得其接近于0，使得分布在目标位置边缘的低质量框能够尽可能的靠近中心。在最终使用该网络的过程中，非极大值抑制(NMS)就可以轻松滤除这些低质量的边界框，提高检测性能。</p>
<p><strong>总结</strong></p>
<ul>
<li>把图片输入backbone中，得到feature map</li>
<li>将feature map输入FPN层，得到每一层P3-P7的feature map</li>
<li>对P3-P7的feature map进行box的位置的回归以及分类</li>
<li>box regression部分，对GT内部的点，直接回归到box，计算四个参数即点到四条边的距离，同时对每一个点预测一个类别，最终输出有4 + C个维度，对一些偏离中心的边框进行舍弃</li>
<li>classification部分，一个分支去分类，使用focal loss，另一个分支使用center mess对边框位置进行约束</li>
</ul>
<p><strong>IoU loss</strong></p>
<p>直接将网络的IoU作为loss进行网络的训练，相比于L2优化边框，优点为：</p>
<ul>
<li>IoU loss将位置信息作为一个整体进行训练，而L2损失却把它们当作互相独立的四个变量进行训练，因此IoU损失能得到更为准确的训练效果；</li>
<li>输入任意样本，IoU的值均介于[0, 1]之间，这种自然的归一化损失使模型具有更强的处理多尺度图像的能力。</li>
</ul>
<h3 id="Segmentation-部分"><a href="#Segmentation-部分" class="headerlink" title="Segmentation 部分"></a>Segmentation 部分</h3><h3 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h3><p>FCN详解： <a href="https://perper.site/2019/02/20/语义分割系列-FCN详解/" target="_blank" rel="noopener">https://perper.site/2019/02/20/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%B3%BB%E5%88%97-FCN%E8%AF%A6%E8%A7%A3/</a></p>
<p>FCN作为最早的分割网络，是分割领域的开山之作。</p>
<p><strong>大致流程</strong></p>
<ul>
<li>FCN将CNN分类网络中最后几层全连接层，改成了卷积层，即全卷积</li>
<li>将图像输入网络中，将不同pooling阶段的特征进行融合，得到网络最终的输出</li>
<li>网络最终输出C+1个channel的 feature map， 优化网络损失，得到每一个像素位置上的类别信息</li>
</ul>
<p><strong>网络结构</strong></p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200607224612659.png" alt="image-20200607224612659" style="zoom:40%;"></p>
<p>网络选择了8x，16x，32x这三种尺度的上采样输出进行网络的优化，分别对这三层上采样，然后将得到的feature map进行优化，最终最优的结构为8x。</p>
<p><strong>上采样</strong></p>
<p>讲一个2x2的feature map上采样成4x4的feature map，卷积核的大小为3x3，首先将2x2加上2个padding，变成6x6，随后做3x3的卷积，即可。这一步操作使得网络的输出与输入图像大小一致。含步长的卷积可以视为对feature map各个元素之间进行填充。</p>
<p><strong>损失函数</strong></p>
<p>网络采用交叉熵损失函数，对每个位置进行回归。</p>
<p>segmentation领域碰撞出了两个最重要的设计：U-shape Structure 和 Dilation Conv，据此形成当下语义分割领域网络设计最常见的两大派系：1）U-shape 联盟以 RefineNet、GCN、DFN 等算法为代表；2）Dilation 联盟以 PSPNet、Deeplab 系列方法为代表。</p>
<h3 id="关键点检测部分网络"><a href="#关键点检测部分网络" class="headerlink" title="关键点检测部分网络"></a>关键点检测部分网络</h3></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>WenHui Zhou</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2020/05/27/常见的目标检测网络/">https://wenhui-zhou.github.io/2020/05/27/常见的目标检测网络/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>| 本博客所有文章除特别声明外，均采用 <a href="&quot;http://creativecommons.org/licenses/by-nc-sa/3.0/cn/&quot;" rel="&quot;external" nofollow&quot;="" target="&quot;_blank&quot;">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！ </li></ul></div><br><div class="tags"></div><div class="post-nav"><a class="pre" href="/2020/06/04/手势识别/">手势识别</a><a class="next" href="/2020/05/26/常见数据结构/">常见数据结构</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'civq9nKD49NpRALooR9Llqmf-gzGzoHsz',
  appKey:'JggO9HaSi1Lfx17nt16oDfsI',
  placeholder:'Shall we talk',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/3D重建/">3D重建</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Mac/">Mac</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tensorflow/">Tensorflow</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/effective-cpp/">effective cpp</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/super-resolution/">super resolution</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tool/">tool</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/webSearch/">webSearch</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/xigua/">xigua</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/动画/">动画</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/建站/">建站</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/手撕系列/">手撕系列</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/推荐系统/">推荐系统</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/模型评价/">模型评价</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/比赛/">比赛</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习/">深度学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习总结/">深度学习总结</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法扫盲/">算法扫盲</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/统计学习方法/">统计学习方法</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程/">编程</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/论文复现/">论文复现</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/论文阅读/">论文阅读</a><span class="category-list-count">23</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/论文阅读/3D重建/">3D重建</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/面试准备/">面试准备</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目/">项目</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目总结/">项目总结</a><span class="category-list-count">6</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/项目总结/论文阅读/">论文阅读</a><span class="category-list-count">3</span></li></ul></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/SR/" style="font-size: 15px;">SR</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/tags/c/" style="font-size: 15px;">c++</a> <a href="/tags/项目总结/" style="font-size: 15px;">项目总结</a> <a href="/tags/leetcode/" style="font-size: 15px;">leetcode</a> <a href="/tags/3D重建/" style="font-size: 15px;">3D重建</a> <a href="/tags/论文阅读/" style="font-size: 15px;">论文阅读</a> <a href="/tags/netStation/" style="font-size: 15px;">netStation</a> <a href="/tags/tool/" style="font-size: 15px;">tool</a> <a href="/tags/interview/" style="font-size: 15px;">interview</a> <a href="/tags/dialog/" style="font-size: 15px;">dialog</a> <a href="/tags/tip/" style="font-size: 15px;">tip</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/learning-cpp/" style="font-size: 15px;">learning cpp</a> <a href="/tags/职业规划/" style="font-size: 15px;">职业规划</a> <a href="/tags/—-leetcode/" style="font-size: 15px;">— leetcode</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/tips/" style="font-size: 15px;">tips</a> <a href="/tags/超分辨率/" style="font-size: 15px;">超分辨率</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/12/24/协同过滤算法/">协同过滤算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/19/github指令/">github指令</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/28/C-STL-library/">C++ STL library</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/07/18/海量地震数据超分辨率恢复及三维可视化/">海量地震数据超分辨率恢复及三维可视化的细枝末节</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/20/跟着面试打补丁/">跟着面试打补丁</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/17/地震数据超分辨率实验部分/">地震数据超分辨率实验部分</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/15/RDSNet细枝末节/">RDSNet细枝末节</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/14/ms-一个结束，一个开始/">ms:一个结束，一个开始</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/06/面试试题准备/">面试试题准备</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/06/04/手势识别/">手势识别</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/WenHui-Zhou" title="GITHUB" target="_blank">GITHUB</a><ul></ul><a href="http://www.google.com/" title="GOOGLE" target="_blank">GOOGLE</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">WenHuiZhou.</a> 访问人数:<span id="busuanzi_value_site_uv"></span> 
访问量:<span id="busuanzi_value_site_pv"></span></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script> </div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>