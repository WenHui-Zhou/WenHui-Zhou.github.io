<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="cheer up"><meta name="baidu-site-verification" content="9pSIuwCbvi"><meta name="google-site-verification" content="YzcCTjF6VoVlNAtL37_S4vFjzFwYTAFZzD51Il2IGKY"><title>TrackIn:BERT five-classification on MSMARCO.md | WenHuiZhou</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">TrackIn:BERT five-classification on MSMARCO.md</h1><a id="logo" href="/.">WenHuiZhou</a><p class="description">perper（打起精神！）</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">TrackIn:BERT five-classification on MSMARCO.md</h1><div class="post-meta">Mar 12, 2020<span> | </span><span class="category"><a href="/categories/NLP/">NLP</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a class="disqus-comment-count" href="/2020/03/12/TrackIn-BERT/#vcomment"><span class="valine-comment-count" data-xid="/2020/03/12/TrackIn-BERT/"></span><span> 条评论</span></a><div class="post-content"><p>这篇post主要构建一个BERT的五分类模型，然后研究使用TrackIn来确定样本在模型训练中，对模型inference的影响。</p>
<a id="more"></a>
<h3 id="MS-MARCO"><a href="#MS-MARCO" class="headerlink" title="MS MARCO"></a>MS MARCO</h3><p>MS MARCO是微软发布的一个NLP问答数据集，里头含有100000个真实的在bing上提出的问题，以及问题的答案，可作为QnA问题的数据集，此后还陆续提出了多种问题的数据集。</p>
<p>格式的介绍在这里：<a href="https://microsoft.github.io/MSMARCO-Passage-Ranking/" target="_blank" rel="noopener">https://microsoft.github.io/MSMARCO-Passage-Ranking/</a></p>
<p>MARCO数据集中，比较重要的文件有：</p>
<ul>
<li><p><strong>triples.train.small.tsv</strong>，他的文件格式为:&lt;query,doc1,doc2&gt;，第一个doc1是positive，第二个doc2是negative。</p>
</li>
<li><p><strong>top1000.dev.tsv：</strong>&lt;query_id ,doc_id,query,doc&gt;，即query和doc对</p>
</li>
<li><strong>qrels.dev.small.tsv</strong>：&lt;query_id,0,doc_id,1&gt;，用来判断query和doc是否是相互关联的。</li>
</ul>
<p>triples.train.small.tsv中query是重复的。</p>
<h3 id="STSB数据集"><a href="#STSB数据集" class="headerlink" title="STSB数据集"></a>STSB数据集</h3><p>用1到5的分数来表征两个句子的语义相似性，本质上是一个回归问题，但依然可以用分类的方法做，因此可以归类为句子对的文本五分类任务。</p>
<p>数据集的格式为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">index	genre	filename	year	old_index	source1	source2	sentence1	sentence2	score</span><br></pre></td></tr></table></figure>
<p>其中我们用到的数据是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sentence1	sentence2	score</span><br><span class="line">A plane is taking off.	An air plane is taking off.	5.000</span><br></pre></td></tr></table></figure>
<p>数据首先经过wordembedding，每个词变成长度为768的向量，每个句子通过pad_sequence变成长度为512，因此输出为【512 x 768】随后将【512 x 768】加上position embedding输入BERT模型中。</p>
<h3 id="dataloader部分"><a href="#dataloader部分" class="headerlink" title="dataloader部分"></a>dataloader部分</h3><p>使用一个数据存储数据，格式为&lt;query,doc,label&gt;,然后将数组存储为dataloader。其中label = 0 表示负类，label = 1 表示你正类。</p>
<p>其具体的实现过程如下：</p>
<p>首先将数据query与doc分别经过tokenizer分词化，然后组成一个输入:</p>
<p>[CLS] + tokens_query + [SEP] + tokens_doc + [SEP]</p>
<p>随后将分词转换为vocab中的word id，构建一个segments_tensor，长度与输入长度相同，其中query部分为0，doc部分为1。如果我们希望返回一个batch，需要将所有的tensor通过前面填充0 的方式，将tensor维度变成一直（pad_sequence）,同时构建一个mask，mask的维度与batch的维度相同，其中用零填充的部分为0，不是0填充的部分为1。</p>
<p>最终dataloader将返回：</p>
<p>&lt;tokens_tensors, masks_tensors,segments_tensors,  labels_ids&gt; </p>
<p>tensor的维度为：tokens_tensor为：32x128</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>模型使用<code>transformers.BertForSequenceClassification</code></p>
<p>该模型的网络结构如下：</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/image-20200314021335605.png" alt="image-20200314021335605" style="zoom:50%;"></p>
<p>即BERT的基础上加上一层全连接层，全链接层输入为768，输出为label class的个数，最后通过一个交叉熵得出最后的loss。</p>
<h3 id="STSB评价指标"><a href="#STSB评价指标" class="headerlink" title="STSB评价指标"></a>STSB评价指标</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pearson_and_spearman</span><span class="params">(preds, labels)</span>:</span></span><br><span class="line">        pearson_corr = pearsonr(preds, labels)[<span class="number">0</span>]</span><br><span class="line">        spearman_corr = spearmanr(preds, labels)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">"pearson"</span>: pearson_corr,</span><br><span class="line">            <span class="string">"spearmanr"</span>: spearman_corr,</span><br><span class="line">            <span class="string">"corr"</span>: (pearson_corr + spearman_corr) / <span class="number">2</span>,</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p><strong>pearson:</strong>是衡量向量相似度的一种方法。输出范围为-1到+1, 0代表无相关性，负值为负相关，正值为正相关。<br>$$<br>\rho(X, Y)=\frac{E\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right]}{\sigma_{X} \sigma_{Y}}=\frac{E\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right]}{\sqrt{\sum_{i=1}^{n}\left(X_{i}-\mu_{X}\right)^{2}} \sqrt{\sum_{i=1}^{n}\left(Y_{i}-\mu_{Y}\right)^{2}}}<br>$$<br>相关系数 0.8-1.0 极强相关</p>
<p>0.6-0.8 强相关</p>
<p>0.4-0.6 中等程度相关</p>
<p>0.2-0.4 弱相关</p>
<p>0.0-0.2 极弱相关或无相关</p>
<p><strong>spearmanr</strong>：斯皮尔曼相关系数表明X(独立变量)和Y(依赖变量)的相关方向：<br>$$<br>\rho=\frac{\sum_{i}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sqrt{\sum_{i}\left(x_{i}-\bar{x}\right)^{2} \sum_{i}\left(y_{i}-\bar{y}\right)^{2}}}<br>$$</p>
<h3 id="trackIn-理解"><a href="#trackIn-理解" class="headerlink" title="trackIn 理解"></a>trackIn 理解</h3><p>TrackIn核心的观点在于model经过当前样本训练迭代后，更新参数得到的新模型在验证集上loss与未更新前下降的数值之差。</p>
<p><strong>样本影响力</strong>的定义如下（z为指定的样本，z’为验证集）：</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/clip_image001.png" alt="img" style="zoom:50%;"></p>
<p>理想状态：即在某次迭代，能够指定一个训练样本。</p>
<p>在理想状态下，所有训练数据对验证集样本的影响如下：</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/clip_image002.png" alt="img" style="zoom:50%;"></p>
<p><strong>上述公式，默认条件是每次迭代仅在一个训练样本上进行训练。</strong></p>
<p>作者使用一阶近似来估计迭代t中，测试示例的损失变化：</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/clip_image003.png" alt="img" style="zoom:50%;"></p>
<p>其中梯度的计算是在验证集数据上的。</p>
<p>SGD梯度下降法计算参数迭代如下：</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/clip_image004.png" alt="img" style="zoom:50%;"></p>
<p>将两个公式结合，忽略高阶项得到：</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/clip_image005.png" alt="img" style="zoom:50%;"></p>
<p>因此我们固定一个样本z，可以算出利用z的所有迭代次数的影响力之和：</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/clip_image006.png" alt="img" style="zoom:50%;"></p>
<p>当我们训练数据不是一个样本，而是一个batch的时候：</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/clip_image007.png" alt="img" style="zoom:50%;"></p>
<p>作者在实际的应用中，在每次迭代仅使用单个样本，保存学习率不变，模型每次在训练集上训练一次后保存一个检查点checkpoint，因此两个checkpoint之间样本仅训练过一次。作者利用检查点的参数，作为近似参数向量：</p>
<p><img src="/Users/zhouwenhui/blog/source/images/nlp/clip_image008.png" alt="img" style="zoom:50%;"></p>
<p>上式的含义是模型参数wt 到wt+1之间，样本z对验证集的影响。通过对这个影响是positive或negative的判断，得到样本对模型的影响力，一个应用就是用来判断是否存在样本误标注的问题。</p>
<p>作者论文中给出一个NLP例子是文本分类问题，证明这种方法也是可以用在NLP中的。 </p>
<h3 id="trackIn的一点尝试"><a href="#trackIn的一点尝试" class="headerlink" title="trackIn的一点尝试"></a>trackIn的一点尝试</h3><ol>
<li><p>使用STS-B数据集：五分类数据集，数据示例如下：</p>
<p> sentence1 sentence2 similar_score（0-4）<br> A plane is taking off.    An air plane is taking off.    4<br> A man is playing a large flute.    A man is playing a flute.    3<br> A man is smoking.    A man is skating.    0</p>
</li>
<li><p>使用huggingface中的BertForSequenceClassification类，调整网络的输出为五分类，该网络的结构为：<br> Bert + dropout + linear（768x5）+ corssentropy</p>
</li>
<li><p>原始版本训练10个epoch后，得到了检测的指标指标：<br> corr = 0.8668199467352331<br> pearson = 0.8680211975793752<br> spearmanr = 0.865618695891091<br> 这个成绩在glue排行版上top20左右，top10以上基本你上90分。</p>
</li>
<li><p>fix BERT部分参数，仅训练linear层，10个epoch之后的指标为：<br> corr = 0.46851795469199176<br> pearson = 0.4883897702565989<br> spearmanr = 0.44864613912738466</p>
</li>
<li><p>由于Bert后的分类结构过于简单，因此在原始网络后面添加linear层。fix bert部分，训练得到结果：<br> Bert + linear（768x100） + linear(100x5) + crossentropy<br> corr = 0.4874025699052884<br> pearson = 0.48976918410773407<br> spearmanr = 0.4850359557028428</p>
</li>
<li><p>bert + linear(768,300) + linear(300,50) + linear(50,5):<br> corr = 0.4901503255725512<br> pearson = 0.49249477605462705<br> spearmanr = 0.48780587509047535</p>
</li>
<li><p>Bert + linear(768,300) + linear(300,100) + linear(100,50) + linear(50,5) </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">corr = 0.5010257366795687</span><br><span class="line">pearson = 0.5034574745457592</span><br><span class="line">spearmanr = 0.49859399881337824</span><br></pre></td></tr></table></figure>
</li>
<li><p>训练bert最后一层以及linear层得到的结果：<br>bert + linear(768,300) + linear(300,100) + linear(100,50) + linear(50,5)</p>
<p> corr = 0.7648302644703243<br> pearson = 0.7663893373697331<br> spearmanr = 0.7632711915709155</p>
</li>
</ol>
<p>pearson:输出范围为-1到+1, 0代表无相关性，负值为负相关，正值为正相关。<br>相关系数 0.8-1.0 极强相关<br>0.6-0.8 强相关<br>0.4-0.6 中等程度相关<br>0.2-0.4 弱相关<br>0.0-0.2 极弱相关或无相关</p>
<h3 id="分析trackIn函数"><a href="#分析trackIn函数" class="headerlink" title="分析trackIn函数"></a>分析trackIn函数</h3><p>$$<br>\operatorname{TrackIn}\left(z, z^{\prime}\right)=\sum_{t: z_{t}=z} \eta_{t} \nabla \ell\left(w_{t}, z^{\prime}\right) \cdot \nabla \ell\left(w_{t}, z\right)<br>$$</p>
<p>如上，作者经过一些推导之后得到TrackIn的表达式，表达式表明，在第t步迭代的时候，训练样本z 对 待测试样本$z^`$ 的影响的计算方法是： 待测样本在$w_t$上的梯度与训练样本在$w_t$上的梯度的乘积。</p>
<p>这时候有一个问题，就是$w_t$是一个矩阵，而TrackIn是loss的差，是一个标量，我的想法是矩阵对应位置相乘后相加，得到最后的结果。可以把t次数调大一点，使得效果能够明显一点。</p>
<p>因此思路如下：</p>
<ol>
<li>将训练样本和测试样本输入网络，然后将Bert最后一层以及classifier层的梯度拿出来</li>
<li>计算梯度相乘，然后求和得到trackIn的值</li>
<li>记录一个表，表中的内容是target example 和 对应的一些列 train example，然后每个train example有一个trackIn的值。</li>
</ol>
<p>因此要做的事情是：</p>
<ol>
<li>重新设计数据集，dataset，dataloader</li>
<li>对训练数据和测试数据迭代t次网络，然后将每次的梯度记到txt中</li>
<li>写一个矩阵相乘的代码，得到trackIn的值</li>
</ol>
<p>明天做！</p>
<p>测试数据：</p>
<p>aa: A plane is taking off.    An air plane is taking off.    5.000  </p>
<p>trackIn训练数据（假样本对数据的影响）：</p>
<p>b1: A plane is taking off.    A man wearing a hard hat is dancing.    5.000   :11326.170552326665</p>
<p>b2: A plane is taking off.    The plane is about to take off.     5.0 done         :11326.172044273939</p>
<p>b3: A plane is taking off.    Spaceship is about to take off.     3.0 done         :11326.17223393679</p>
</div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>WenHui Zhou</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="/2020/03/12/TrackIn-BERT/">https://wenhui-zhou.github.io/2020/03/12/TrackIn-BERT/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>| 本博客所有文章除特别声明外，均采用 <a href="&quot;http://creativecommons.org/licenses/by-nc-sa/3.0/cn/&quot;" rel="&quot;external" nofollow&quot;="" target="&quot;_blank&quot;">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！ </li></ul></div><br><div class="tags"></div><div class="post-nav"><a class="pre" href="/2020/03/14/LR-推导/">LR 推导</a><a class="next" href="/2020/03/10/Bert，XLNet，UNILM，RoBERTa以及QA/">Bert，XLNet，UNILM，RoBERTa以及QA</a></div><div id="vcomment"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script><script>var notify = 'false' == true ? true : false;
var verify = 'false' == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'civq9nKD49NpRALooR9Llqmf-gzGzoHsz',
  appKey:'JggO9HaSi1Lfx17nt16oDfsI',
  placeholder:'Shall we talk',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10'
})</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/3D重建/">3D重建</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Mac/">Mac</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tensorflow/">Tensorflow</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/effective-cpp/">effective cpp</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/super-resolution/">super resolution</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tool/">tool</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/webSearch/">webSearch</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/xigua/">xigua</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/建站/">建站</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/手撕系列/">手撕系列</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/推荐系统/">推荐系统</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/模型评价/">模型评价</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/比赛/">比赛</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习/">深度学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法扫盲/">算法扫盲</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/统计学习方法/">统计学习方法</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程/">编程</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/论文复现/">论文复现</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/论文阅读/">论文阅读</a><span class="category-list-count">23</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/论文阅读/3D重建/">3D重建</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/面试准备/">面试准备</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目/">项目</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/项目总结/">项目总结</a><span class="category-list-count">5</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/项目总结/论文阅读/">论文阅读</a><span class="category-list-count">3</span></li></ul></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/interview/" style="font-size: 15px;">interview</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/tags/leetcode/" style="font-size: 15px;">leetcode</a> <a href="/tags/c/" style="font-size: 15px;">c++</a> <a href="/tags/项目总结/" style="font-size: 15px;">项目总结</a> <a href="/tags/3D重建/" style="font-size: 15px;">3D重建</a> <a href="/tags/netStation/" style="font-size: 15px;">netStation</a> <a href="/tags/论文阅读/" style="font-size: 15px;">论文阅读</a> <a href="/tags/tool/" style="font-size: 15px;">tool</a> <a href="/tags/SR/" style="font-size: 15px;">SR</a> <a href="/tags/dialog/" style="font-size: 15px;">dialog</a> <a href="/tags/tip/" style="font-size: 15px;">tip</a> <a href="/tags/Tensorflow/" style="font-size: 15px;">Tensorflow</a> <a href="/tags/learning-cpp/" style="font-size: 15px;">learning cpp</a> <a href="/tags/NLP/" style="font-size: 15px;">NLP</a> <a href="/tags/—-leetcode/" style="font-size: 15px;">— leetcode</a> <a href="/tags/职业规划/" style="font-size: 15px;">职业规划</a> <a href="/tags/tips/" style="font-size: 15px;">tips</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/超分辨率/" style="font-size: 15px;">超分辨率</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/03/25/problem-summary/">problem summary</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/18/RDSNet/">RDSNet</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/14/LR-推导/">LR 推导</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/12/TrackIn-BERT/">TrackIn:BERT five-classification on MSMARCO.md</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/10/Bert，XLNet，UNILM，RoBERTa以及QA/">Bert，XLNet，UNILM，RoBERTa以及QA</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/10/天池安全AI挑战赛细枝末节/">天池安全AI挑战赛细枝末节</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/08/精彩视频剪辑的细枝末节/">精彩视频剪辑的细枝末节</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/07/openpose细枝末节/">openpose细枝末节</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/26/支持向量机-SVM/">支持向量机-SVM</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/19/聚类/">聚类</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/WenHui-Zhou" title="GITHUB" target="_blank">GITHUB</a><ul></ul><a href="http://www.google.com/" title="GOOGLE" target="_blank">GOOGLE</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">WenHuiZhou.</a> 访问人数:<span id="busuanzi_value_site_uv"></span> 
访问量:<span id="busuanzi_value_site_pv"></span></div><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script> </div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.2/jquery.fancybox.min.css"><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>